{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run DataLoader.ipynb\n",
    "%run Visualization.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3D CNN with Residual Connection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the model\n",
    "\n",
    "import torch.nn as nn\n",
    "\n",
    "# helper function to determine dimension after convolution\n",
    "def conv_output_shape(in_dimension, kernel_size, stride):\n",
    "    output_dim = []\n",
    "    for (in_dim, kern_size, strd) in zip(in_dimension, kernel_size, stride):\n",
    "        len = int(float(in_dim - kern_size) / strd + 1.)\n",
    "        output_dim.append(len)\n",
    "    \n",
    "    return output_dim\n",
    "\n",
    "        \n",
    "class TemporalCNN(nn.Module):\n",
    "    \n",
    "    def _conv_unit(self, in_channels, out_channels, in_shape, kernel_size, stride, dropout_prob):\n",
    "        r\"\"\" Return one 3D convolution unit\n",
    "        \n",
    "        Args:\n",
    "            in_channels: Input channels of the Conv3D module\n",
    "            out_channels: Output channels of the Conv3D module\n",
    "            in_shape: Shape of the input image. i.e. The last 3 dimensions of the input tensor: D x H x W\n",
    "            kernel_size: Kernel size\n",
    "            stride: Stride\n",
    "            dropout_prob: Probability of dropout layer\n",
    "                         \n",
    "        Output:\n",
    "            (conv_module, aux_module, out_shape)\n",
    "        \"\"\"\n",
    "        \n",
    "        conv = nn.Conv3d(in_channels=in_channels, out_channels=out_channels, kernel_size=kernel_size, stride=stride)\n",
    "        dropout = nn.Dropout3d(p=dropout_prob)\n",
    "        conv_module = nn.Sequential(conv, dropout).to(self.device_)\n",
    "        out_shape = conv_output_shape(in_shape, kernel_size, stride)\n",
    "        \n",
    "        flatten = nn.Flatten(start_dim=2)\n",
    "        aux = nn.Linear(in_features=np.prod(out_shape[-2:])*out_channels, out_features=128)\n",
    "        aux_module = nn.Sequential(flatten, aux).to(self.device_)\n",
    "        \n",
    "        return conv_module, aux_module, out_shape\n",
    "    \n",
    "    def _linear_unit(self, in_features, out_features, dropout_prob):\n",
    "        linear = nn.Linear(in_features, out_features)\n",
    "        relu = nn.ReLU()\n",
    "        dropout = nn.Dropout(p=dropout_prob)\n",
    "        return nn.Sequential(linear, relu, dropout).to(self.device_), out_features\n",
    "        \n",
    "    \n",
    "    def __init__(self, in_height, in_width, seq_len, dropout_prob=0.5, aux_history=10, device=None):\n",
    "        r\"\"\" TemporalCNN: this model does 3D convolution on the H, W and temporal dimension\n",
    "             It also includes residual connection from each Conv3D output to the final output\n",
    "             \n",
    "             Args:\n",
    "                 in_height: image height\n",
    "                 in_width: image width\n",
    "                 seq_len: image sequence length\n",
    "                 dropout_prob: prob for the dropout layer\n",
    "                 aux_history: length of history to extract from seq_len\n",
    "             \n",
    "             Output:\n",
    "                 nn.Module, accepts input with shape [batch_len, seq_len, C, in_height, in_width]\n",
    "        \"\"\"\n",
    "        super(TemporalCNN, self).__init__()\n",
    "        \n",
    "        self.seq_len_ = seq_len\n",
    "        self.in_width_ = in_width\n",
    "        self.in_height_ = in_height\n",
    "        self.aux_history_ = aux_history\n",
    "        in_shape = (seq_len, in_height, in_width)\n",
    "        \n",
    "        self.device_ = device if device is not None else torch.device(\"cpu\")\n",
    "        \n",
    "        # conv1\n",
    "        self.conv0_, self.aux0_, out_shape = self._conv_unit(3, 64, in_shape, (3, 12, 12), (1, 6, 6), dropout_prob)\n",
    "        # conv2\n",
    "        self.conv1_, self.aux1_, out_shape = self._conv_unit(64, 64, out_shape, (2, 5, 5), (1, 2, 2), dropout_prob)\n",
    "        # conv3\n",
    "        self.conv2_, self.aux2_, out_shape = self._conv_unit(64, 64, out_shape, (2, 5, 5), (1, 1, 1), dropout_prob)\n",
    "        # conv4\n",
    "        self.conv3_, self.aux3_, out_shape = self._conv_unit(64, 64, out_shape, (2, 5, 5), (1, 1, 1), dropout_prob)\n",
    "        \n",
    "        # Flatten the last 3 dims\n",
    "        self.flatten_ = nn.Flatten(start_dim=2).to(self.device_)\n",
    "        \n",
    "        # FC 1024\n",
    "        self.linear0_, out_features = self._linear_unit(64*np.prod(out_shape[-2:]), 1024, dropout_prob)\n",
    "        # FC 512\n",
    "        self.linear1_, out_features = self._linear_unit(out_features, 512, dropout_prob)\n",
    "        # FC 256\n",
    "        self.linear2_, out_features = self._linear_unit(out_features, 256, dropout_prob)\n",
    "        # FC 128\n",
    "        self.linear3_ = nn.Linear(out_features, 128).to(device)\n",
    "        \n",
    "        self.elu_ = nn.ELU().to(self.device_)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = x.permute([0, 2, 1, 3, 4]) # swap channel and seq_len, 3D conv over seq_len as depth channel\n",
    "                                       # now: [batch_size, channel, seq_len, H, W]\n",
    "        \n",
    "        x = self.conv0_(x) # every conv includes dropout\n",
    "        x_permute = x.permute([0, 2, 1, 3, 4])[:,-self.aux_history_:,:,:,:]\n",
    "        x_aux0 = self.aux0_(x_permute)\n",
    "        x = self.conv1_(x)\n",
    "        x_permute = x.permute([0, 2, 1, 3, 4])[:,-self.aux_history_:,:,:,:]\n",
    "        x_aux1 = self.aux1_(x_permute)\n",
    "        x = self.conv2_(x)\n",
    "        x_permute = x.permute([0, 2, 1, 3, 4])[:,-self.aux_history_:,:,:,:]\n",
    "        x_aux2 = self.aux2_(x_permute)\n",
    "        x = self.conv3_(x)\n",
    "        x = x.permute([0, 2, 1, 3, 4])\n",
    "        x_permute = x[:,-self.aux_history_:,:,:,:]\n",
    "        x_aux3 = self.aux3_(x_permute)\n",
    "\n",
    "        x = self.flatten_(x)\n",
    "        x = self.linear0_(x)\n",
    "        x = self.linear1_(x)\n",
    "        x = self.linear2_(x)\n",
    "        x = self.linear3_(x)\n",
    "        \n",
    "        final_out = x + x_aux0 + x_aux1 + x_aux2 + x_aux3\n",
    "        final_out = self.elu_(final_out)\n",
    "        \n",
    "        return final_out\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Autoregressive LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Autoregressive LSTM Module\n",
    "\n",
    "class AutoregressiveLSTMCell(nn.Module):\n",
    "    \n",
    "    def __init__(self, target_size, visual_feature_size, hidden_size, device=None):\n",
    "        r\"\"\" AutoregressiveModule takes visual feature from 3D CNN module, pass it\n",
    "             first into an internal LSTM cell and then into a Linear network. The final\n",
    "             output of this module is of dimension output_size.\n",
    "             \n",
    "             Args:\n",
    "                 target_dim: dimsion of target value. for this application 3 (angle, speed, torque)\n",
    "                 visual_feature_dim:\n",
    "                 output_size: output size after the Linear network\n",
    "                 autoregressive_mode: wether this module work as autoregressive mode or\n",
    "                     just pass the ground truth to output\n",
    "             Output:\n",
    "                 nn.Module\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        \n",
    "        self.target_size_ = target_size\n",
    "        self.visual_feature_size_ = visual_feature_size\n",
    "        self.hidden_size_ = hidden_size\n",
    "        \n",
    "        self.device_ = device if device is not None else torch.device(\"cpu\")\n",
    "        \n",
    "        self.lstm_cell_ = nn.LSTMCell(input_size=target_size+visual_feature_size, hidden_size=hidden_size).to(self.device_)\n",
    "        self.linear_ = nn.Linear(in_features=hidden_size+visual_feature_size+target_size, out_features=target_size).to(self.device_)\n",
    "    \n",
    "    def forward(self, visual_features, prev_target, prev_states):\n",
    "        r\"\"\"\n",
    "            Output:\n",
    "                (output, target_ground_truth) for autoregressive_mode = True\n",
    "                (output, (output, ))\n",
    "        \"\"\"\n",
    "        lstm_input = torch.cat((visual_features, prev_target), dim=-1)\n",
    "        h_t, c_t = self.lstm_cell_(lstm_input, prev_states)\n",
    "        linear_input = torch.cat((visual_features, prev_target, h_t), dim=-1)\n",
    "        output = self.linear_(linear_input)\n",
    "        new_state = (h_t, c_t)\n",
    "        \n",
    "        return output, new_state\n",
    "        \n",
    "class AutoregressiveLSTM(AutoregressiveLSTMCell):\n",
    "    \n",
    "    def __init__(self, target_size, visual_feature_size, hidden_size, autoregressive_mode=True, device=None):\n",
    "        super().__init__(target_size, visual_feature_size, hidden_size, device)\n",
    "        self.autoregressive_mode_ = autoregressive_mode\n",
    "    \n",
    "    def forward(self, visual_features, init_target=None, init_states=None, target_groundtruth=None):\n",
    "        # different from LSTM in torch library, we use the second dimension for sequence!\n",
    "        assert self.autoregressive_mode_ or target_groundtruth is not None\n",
    "        \n",
    "        seq_len = visual_features.shape[1]\n",
    "        batch_len = visual_features.shape[0]\n",
    "\n",
    "        prev_target = torch.zeros(batch_len, self.target_size_, device=self.device_) if init_target is None else init_target\n",
    "        prev_states = (torch.zeros(batch_len, self.hidden_size_, device=self.device_), \n",
    "                       torch.zeros(batch_len, self.hidden_size_, device=self.device_)) if init_states is None else init_states\n",
    "        \n",
    "        outputs = []\n",
    "        states = []\n",
    "        for seq_idx in range(seq_len):\n",
    "            target, state = super().forward(visual_features[:, seq_idx, :], prev_target, prev_states)\n",
    "            prev_target = target if self.autoregressive_mode_ else target_groundtruth[:, seq_idx, :]\n",
    "            prev_states = state\n",
    "            outputs.append(target)\n",
    "            states.append(torch.stack(state))\n",
    "            \n",
    "        outputs = torch.stack(outputs)\n",
    "        outputs = outputs.permute(1, 0, 2)  # dim: [batch, seq, target_size]\n",
    "        states = torch.stack(states)\n",
    "        states = states.permute(1, 2, 0, 3) # dim: [ [batch, seq, internal_size], [batch, seq, internal_size] ]\n",
    "        \n",
    "        return outputs, states\n",
    "    \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Putting them together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SteerNet(nn.Module):\n",
    "    \n",
    "    def __init__(self, device=None, dropout_prob=0.25):\n",
    "        super(SteerNet, self).__init__()\n",
    "        \n",
    "        self.device_ = device if device is not None else torch.device(\"cpu\")\n",
    "        \n",
    "        self.model_cnn_ = TemporalCNN(in_height=480, in_width=640, seq_len=15, dropout_prob=dropout_prob, aux_history=10, device=device)\n",
    "        self.feature_dropout_ = nn.Dropout(p=dropout_prob)\n",
    "        self.model_lstm_gt_ = AutoregressiveLSTM(target_size=3, visual_feature_size=128, hidden_size=32, autoregressive_mode=False, device=device)\n",
    "        self.model_lstm_autoreg_ = AutoregressiveLSTM(target_size=3, visual_feature_size=128, hidden_size=32, autoregressive_mode=True, device=device)\n",
    "        self.lstm_state_ = None\n",
    "        \n",
    "        self.train_ = True\n",
    "    \n",
    "    def forward(self, images, target=None):\n",
    "        assert target is None or self.train_==True\n",
    "        \n",
    "        features = self.model_cnn_(images)\n",
    "        features = self.feature_dropout_(features)\n",
    "        \n",
    "        if self.lstm_state_ is None:\n",
    "            if self.train_:\n",
    "                out_gt, state_gt = self.model_lstm_gt_(features, target_groundtruth=target)\n",
    "            out_autoreg, state_autoreg = self.model_lstm_autoreg_(features)\n",
    "        else:\n",
    "            state_autoreg = self.lstm_state_\n",
    "            state_gt = (state_autoreg[0].clone().detach(),\n",
    "                        state_autoreg[1].clone().detach()) # copies the state. we don't opzimize the state of this LSTM with groundtruth input!\n",
    "            if self.train_:\n",
    "                out_gt, _ = self.model_lstm_gt_(features, init_states=state_gt, target_groundtruth=target)\n",
    "            out_autoreg, state_autoreg = self.model_lstm_autoreg_(features, init_states=state_autoreg)\n",
    "        if self.train_:\n",
    "            self.lstm_state_ = (state_autoreg[0][:,-1,:].detach(), state_autoreg[1][:,-1,:].detach()) # take the state from last seq\n",
    "        \n",
    "        return (out_autoreg, out_gt) if self.train_ else out_autoreg\n",
    "    \n",
    "    def train(self, mode=True):\n",
    "        self.train_ = mode"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "size of dataset: 33808\n",
      "stat:\n",
      "mean: {'angle': -0.008478613582381778, 'torque': -0.09063468015274655, 'speed': 15.625127605181328}\n",
      "std: {'angle': 0.27152468334113167, 'torque': 0.7874424330514188, 'speed': 5.697122502690882}\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import BatchSampler, SequentialSampler, RandomSampler\n",
    "\n",
    "# Load the data\n",
    "\n",
    "# Use the whole training dataset to calculate target mean and stddev\n",
    "udacity_dataset = UdacityDataset(csv_file='/export/jupyterlab/data/udacity-challenge-2/Ch2_002_export/interpolated.csv',\n",
    "                              root_dir='/export/jupyterlab/data/udacity-challenge-2/Ch2_002_export/',\n",
    "                              transform=transforms.Compose([transforms.ToTensor()]),\n",
    "                              select_camera='center_camera')\n",
    "print(\"size of dataset: {}\\nstat:\\nmean: {}\\nstd: {}\".format(len(udacity_dataset), udacity_dataset.mean, udacity_dataset.std))\n",
    "\n",
    "# Train / valid split with slices\n",
    "training_set = UdacityDataset(csv_file='/export/jupyterlab/data/udacity-challenge-2/Ch2_002_export/interpolated.csv',\n",
    "                              root_dir='/export/jupyterlab/data/udacity-challenge-2/Ch2_002_export/',\n",
    "                              transform=transforms.Compose([transforms.ToTensor()]),\n",
    "                              select_camera='center_camera')\n",
    "validation_set = UdacityDataset(csv_file='/export/jupyterlab/data/udacity-challenge-2/Ch2_002_export/interpolated.csv',\n",
    "                                root_dir='/export/jupyterlab/data/udacity-challenge-2/Ch2_002_export/',\n",
    "                                transform=transforms.Compose([transforms.ToTensor()]),\n",
    "                                select_camera='center_camera')\n",
    "batch_size = 15\n",
    "\n",
    "cbs_train = ConsecutiveBatchSampler(data_source=training_set, batch_size=batch_size, shuffle=True, drop_last=True, seq_len=15, use_all_frames=True)\n",
    "cbs_valid = ConsecutiveBatchSampler(data_source=validation_set, batch_size=batch_size, shuffle=True, drop_last=True, seq_len=15, use_all_frames=True)\n",
    "\n",
    "training_loader = DataLoader(training_set, sampler=cbs_train, collate_fn=(lambda x: x[0]))\n",
    "validation_loader = DataLoader(validation_set, sampler=cbs_valid, collate_fn=(lambda x: x[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda for training\n"
     ]
    }
   ],
   "source": [
    "import datetime\n",
    "import subprocess\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "writer = SummaryWriter('runs/steernet-{}'.format(datetime.datetime.now()))\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using {} for training\".format(device))\n",
    "\n",
    "steernet = SteerNet(device=device, dropout_prob=0.25)\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(steernet.parameters(), lr=1e-4)\n",
    "\n",
    "def normalize_data(sample, channel, origin_dataset):\n",
    "    result = (sample[channel] - origin_dataset.mean[channel]) / origin_dataset.std[channel]\n",
    "    return result\n",
    "\n",
    "def do_epoch(epoch_num=0):\n",
    "    \n",
    "    # training\n",
    "    steernet.train()\n",
    "    \n",
    "    running_cost = 0.0\n",
    "    total_iterations = 0\n",
    "    for i, sample in enumerate(tqdm(training_loader, total=len(training_set)/batch_size)):\n",
    "        \n",
    "        angle = sample['angle'][:, -10:]\n",
    "        torque = sample['torque'][:, -10:]\n",
    "        speed = sample['speed'][:, -10:]\n",
    "        images = sample['image'].to(device)\n",
    "        angle_normalized = normalize_data(sample, 'angle', udacity_dataset)[:, -10:]\n",
    "        torque_normalized = normalize_data(sample, 'torque', udacity_dataset)[:, -10:]\n",
    "        speed_normalized = normalize_data(sample, 'speed', udacity_dataset)[:, -10:]\n",
    "        \n",
    "        target_normalized = torch.stack([angle_normalized, torque_normalized, speed_normalized])\n",
    "        target_normalized = target_normalized.permute([1, 2, 0]).to(device) # (batch_size x seq_len x target_dim)\n",
    "        angle = angle.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        out_autoreg, out_gt = steernet(images, target_normalized)\n",
    "        out_autoreg_angle = out_autoreg[:, :, 0] * udacity_dataset.std['angle'] + udacity_dataset.mean['angle']\n",
    "        \n",
    "        loss_angle = criterion(angle, out_autoreg_angle)\n",
    "        loss_autoreg = criterion(out_autoreg, target_normalized)\n",
    "        loss_gt = criterion(out_gt, target_normalized)\n",
    "        \n",
    "        loss_total = loss_angle + do_epoch.loss_target_weight * (loss_autoreg + loss_gt)\n",
    "        running_cost += loss_angle.item()\n",
    "        total_iterations = i+1\n",
    "        loss_total.backward()\n",
    "        \n",
    "        torch.nn.utils.clip_grad_norm_(steernet.model_lstm_gt_.parameters(), 15.0) # gradient clamping\n",
    "        torch.nn.utils.clip_grad_norm_(steernet.model_lstm_autoreg_.parameters(), 15.0) # gradient clamping\n",
    "        \n",
    "        writer.add_scalar('train - loss total epoch {}'.format(epoch_num), loss_total.item(), i)\n",
    "        writer.add_scalar('train - loss angle epoch {}'.format(epoch_num), loss_angle.item(), i)\n",
    "        writer.add_scalar('train - loss autoreg epoch {}'.format(epoch_num), loss_autoreg.item(), i)\n",
    "        writer.add_scalar('train - loss gt epoch {}'.format(epoch_num), loss_gt.item(), i)\n",
    "        \n",
    "        optimizer.step()\n",
    "    \n",
    "    running_cost = running_cost / total_iterations\n",
    "    writer.add_scalar('train - angle (RMSE)', np.sqrt(running_cost), epoch_num)\n",
    "    print(\"Finished training epoch {}\".format(epoch_num))\n",
    "    \n",
    "    git_label = subprocess.check_output([\"git\", \"rev-parse\", \"--short=6\", \"HEAD\"]).strip().decode('UTF-8') # get current git label\n",
    "    torch.save(steernet.state_dict(), \"saved_models/SteerNet/git-{}-epoch-{}\".format(git_label, epoch_num))\n",
    "    \n",
    "    # validation\n",
    "    \n",
    "    with torch.no_grad(): # we don't require grad calcualation in the validation phase. it saves memory.\n",
    "        steernet.eval()\n",
    "\n",
    "        running_cost = 0.0\n",
    "        total_iterations = 0\n",
    "        for i, sample in enumerate(tqdm(validation_loader, total=len(validation_set)/batch_size)):\n",
    "            angle = sample['angle'][:, -10:]\n",
    "            torque = sample['torque'][:, -10:]\n",
    "            speed = sample['speed'][:, -10:]\n",
    "            images = sample['image'].to(device)\n",
    "            angle_normalized = normalize_data(sample, 'angle', udacity_dataset)[:, -10:]\n",
    "            torque_normalized = normalize_data(sample, 'torque', udacity_dataset)[:, -10:]\n",
    "            speed_normalized = normalize_data(sample, 'speed', udacity_dataset)[:, -10:]\n",
    "            \n",
    "            target_normalized = torch.stack([angle_normalized, torque_normalized, speed_normalized])\n",
    "            target_normalized = target_normalized.permute([2, 1, 0]).to(device) # (batch_size x seq_len x target_dim)\n",
    "            angle = angle.to(device)\n",
    "\n",
    "            out_autoreg = steernet(images)\n",
    "            out_autoreg_angle = out_autoreg[:, :, 0] * udacity_dataset.std['angle'] + udacity_dataset.mean['angle']\n",
    "\n",
    "            loss_angle = criterion(angle, out_autoreg_angle)\n",
    "            loss_autoreg = criterion(out_autoreg, target_normalized)\n",
    "            \n",
    "            writer.add_scalar('valid - loss angle epoch {}'.format(epoch_num), loss_angle.item(), i)\n",
    "            writer.add_scalar('valid - loss autoreg epoch {}'.format(epoch_num), loss_autoreg.item(), i)\n",
    "            running_cost += loss_angle.item()\n",
    "            total_iterations = i+1\n",
    "        \n",
    "        running_cost = running_cost / total_iterations\n",
    "        writer.add_scalar('valid - angle (RMSE)', np.sqrt(running_cost), epoch_num)\n",
    "                  \n",
    "    print(\"Finished validating epoch {}\".format(epoch_num))\n",
    "    \n",
    "    \n",
    "do_epoch.loss_target_weight = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6c37be6b22e844c78d311438a673fd63",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=2253), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for epoch in range(10):\n",
    "    do_epoch(epoch)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {
     "24415faa972742d796997bd4f9d7b5e5": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "2f3c3d085b494eb7baadeb3ec25189b2": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "description_width": "initial"
      }
     },
     "41e9cb5e89544f3d867cdf80206da741": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "layout": "IPY_MODEL_d63e14e70de24d94a8309837ed2e5232",
       "style": "IPY_MODEL_5c6b222af9aa4646a0a6874e756252c7",
       "value": " 5/2253.866666666667 [00:09&lt;1:12:33,  1.94s/it]"
      }
     },
     "5c6b222af9aa4646a0a6874e756252c7": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "description_width": ""
      }
     },
     "6c37be6b22e844c78d311438a673fd63": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HBoxModel",
      "state": {
       "children": [
        "IPY_MODEL_a91f4bf358a648e6b0731d410a553e0a",
        "IPY_MODEL_41e9cb5e89544f3d867cdf80206da741"
       ],
       "layout": "IPY_MODEL_ab741b2d9aef4920af9926d723e29f5a"
      }
     },
     "a91f4bf358a648e6b0731d410a553e0a": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "IntProgressModel",
      "state": {
       "description": "  0%",
       "layout": "IPY_MODEL_24415faa972742d796997bd4f9d7b5e5",
       "max": 2253,
       "style": "IPY_MODEL_2f3c3d085b494eb7baadeb3ec25189b2",
       "value": 5
      }
     },
     "ab741b2d9aef4920af9926d723e29f5a": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "d63e14e70de24d94a8309837ed2e5232": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {}
     }
    },
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
