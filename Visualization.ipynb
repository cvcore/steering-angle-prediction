{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from skimage import transform"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizing Image and Steering angle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_sample(sample,pred_angle):\n",
    "    r\"\"\" Helper function for (batch) sample visualization\n",
    "    随机显示一个batch里面的五张图\n",
    "    Args:\n",
    "        sample: Dictionary\n",
    "    \"\"\"\n",
    "    image_dims = len(sample['image'].shape)\n",
    "    assert image_dims <= 5, \"Unsupported image shape: {}\".format(sample['image'].shape)\n",
    "    if image_dims == 3:\n",
    "        fig, ax = plt.subplots()\n",
    "        ax.imshow(sample['image'].permute(1,2,0))\n",
    "    if image_dims == 4:\n",
    "        n = sample['image'].shape[0]\n",
    "        sample['image'] = torch.Tensor(resize(sample['image'], (n,3,480,640),anti_aliasing=True))\n",
    "        images = sample['image'].permute(0,2,3,1)\n",
    "        fig = plt.figure(figsize=(55, 35))\n",
    "        for i in range(n):\n",
    "            ax = fig.add_subplot(10,5,i+1)\n",
    "            ax.imshow(images[i])\n",
    "            ax.axis('off')\n",
    "            ax.set_title(\"t={}\".format(sample['timestamp'][i]))\n",
    "            ax.text(10, 30, sample['frame_id'][i], color='red')\n",
    "            ax.text(10, 430, \"man-angle {:.3}\".format(sample['angle'][i].item()), color='red')\n",
    "            ax.text(10, 470, \"pred-angle {:.3}\".format(pred_angle[i].item()), color='red')\n",
    "    else:\n",
    "        sample['image'] = sample['image'].permute(0,2,1,3,4)\n",
    "        batch_size,seq_len,channel = sample['image'].shape[0],sample['image'].shape[1],sample['image'].shape[2]\n",
    "        sample['image'] = torch.Tensor(resize(sample['image'], (batch_size,seq_len,3,480,640),anti_aliasing=True))\n",
    "        n0 = sample['image'].shape[0]\n",
    "        n1 = sample['image'].shape[1] if image_dims == 5 else 1\n",
    "        images_flattened = torch.flatten(sample['image'], end_dim=-4)\n",
    "        fig, axis = plt.subplots(n0, n1, figsize=(25, 15))\n",
    "        for i1 in range(n1):\n",
    "            for i0 in range(n0):\n",
    "                image = images_flattened[i0 * n1 + i1]\n",
    "                axis = ax[i0, i1]\n",
    "                axis.imshow(image.permute(1,2,0))\n",
    "                axis.axis('off')\n",
    "                axis.set_title(\"t={}\".format(sample['timestamp'][i0][i1]))\n",
    "                axis.text(10, 30, sample['frame_id'][i0][i1], color='red')\n",
    "                axis.text(10, 430, \"man-angle {:.3}\".format(sample['angle'][i0][i1].item()), color='red')\n",
    "                axis.text(10, 470, \"pred-angle {:.3}\".format(pred_angle[i0][i1].item()), color='red')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "rsrc = \\\n",
    " [[43.45456230828867, 118.00743250075844],\n",
    "  [104.5055617352614, 69.46865203761757],\n",
    "  [114.86050156739812, 60.83953551083698],\n",
    "  [129.74572757609468, 50.48459567870026],\n",
    "  [132.98164627363735, 46.38576532847949],\n",
    "  [301.0336906326895, 98.16046448916306],\n",
    "  [238.25686790036065, 62.56535881619311],\n",
    "  [227.2547443287154, 56.30924933427718],\n",
    "  [209.13359962247614, 46.817221154818526],\n",
    "  [203.9561297064078, 43.5813024572758]]\n",
    "rdst = \\\n",
    " [[10.822125594094452, 1.42189132706374],\n",
    "  [21.177065426231174, 1.5297552836484982],\n",
    "  [25.275895776451954, 1.42189132706374],\n",
    "  [36.062291434927694, 1.6376192402332563],\n",
    "  [40.376849698318004, 1.42189132706374],\n",
    "  [11.900765159942026, -2.1376192402332563],\n",
    "  [22.25570499207874, -2.1376192402332563],\n",
    "  [26.785991168638553, -2.029755283648498],\n",
    "  [37.033067044190524, -2.029755283648498],\n",
    "  [41.67121717733509, -2.029755283648498]]\n",
    "\n",
    "tform3_img = transform.ProjectiveTransform()\n",
    "tform3_img.estimate(np.array(rdst), np.array(rsrc))\n",
    "\n",
    "def calc_curvature(v_ego, angle_steers, angle_offset=0):\n",
    "    deg_to_rad = np.pi/180.\n",
    "    slip_fator = 0.0014 # slip factor obtained from real data\n",
    "    steer_ratio = 15.3  # from http://www.edmunds.com/acura/ilx/2016/road-test-specs/\n",
    "    wheel_base = 2.67   # from http://www.edmunds.com/acura/ilx/2016/sedan/features-specs/\n",
    "\n",
    "    angle_steers_rad = (angle_steers - angle_offset) * deg_to_rad\n",
    "    curvature = angle_steers_rad/(steer_ratio * wheel_base * (1. + slip_fator * v_ego**2))\n",
    "    return curvature\n",
    "\n",
    "def calc_lookahead_offset(v_ego, angle_steers, d_lookahead, angle_offset=0):\n",
    "    #*** this function returns the lateral offset given the steering angle, speed and the lookahead distance\n",
    "    curvature = calc_curvature(v_ego, angle_steers, angle_offset)\n",
    "\n",
    "    # clip is to avoid arcsin NaNs due to too sharp turns\n",
    "    y_actual = d_lookahead * np.tan(np.arcsin(np.clip(d_lookahead * curvature, -0.999, 0.999))/2.)\n",
    "    return y_actual, curvature\n",
    "\n",
    "def show_steering_angle_single(ax, angle, image_width, image_height, speed_ms, color=None, alpha=0.5):\n",
    "    path_x = np.arange(0., 50.1, 0.5)\n",
    "    path_y, _ = calc_lookahead_offset(speed_ms, angle, path_x)\n",
    "    \n",
    "    plot_points_x = []\n",
    "    plot_points_y = []\n",
    "    \n",
    "    for x, y in zip(path_x, path_y):\n",
    "        col, row = tform3_img((x, y))[0] # transform from car to image coordinate\n",
    "        row = row / 160 * image_height\n",
    "        col = col / 320 * image_width\n",
    "        if row >= 0 and row <= image_height and col >= 0 and col <= image_width:\n",
    "            plot_points_x.append(col)\n",
    "            plot_points_y.append(row)\n",
    "    \n",
    "    ax.scatter(plot_points_x, plot_points_y, alpha=alpha, color=color)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def show_image_single(ax, image, \n",
    "                      angle_pred=None, angle_true=None, speed=None, alpha_angle=0.5,\n",
    "                      cam_image=None, alpha_cam=0.4,\n",
    "                      timestamp=None):\n",
    "    assert len(image.shape) == 3, \"Image should be 3-dimentional\"\n",
    "    ax.imshow(image)\n",
    "    image_height, image_width, _ = image.shape\n",
    "    line_space = 10 * image_height / 120\n",
    "    if cam_image is not None:\n",
    "        ax.imshow(cam_image, alpha=alpha_cam)\n",
    "    if angle_pred is not None and speed is not None:\n",
    "        show_steering_angle_single(ax, angle_pred, image_width, image_height, speed, color='red', alpha=alpha_angle)\n",
    "        ax.text(10, image_height-line_space, \"pred-angle: {:.4}\".format(angle_pred), color='red')\n",
    "    if angle_true is not None and speed is not None:\n",
    "        show_steering_angle_single(ax, angle_true, image_width, image_height, speed, color='blue', alpha=alpha_angle)\n",
    "        ax.text(10, image_height-2*line_space, \"true-angle: {:.4}\".format(angle_true), color='blue')\n",
    "    if angle_pred is not None and angle_true is not None:\n",
    "        ax.text(10, image_height-3*line_space, \"error: {:.4}\".format(angle_pred-angle_true), color='green')\n",
    "    if timestamp is not None:\n",
    "        ax.text(10, 10, \"t={}\".format(timestamp))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizing CNN filters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_cnn(cnn, max_row=10, max_col=10, select_channel=None, tb_writer=None, save_path=None):\n",
    "    assert isinstance(cnn, nn.Conv3d) or isinstance(cnn, nn.Conv2d)\n",
    "    \n",
    "    filters = cnn.weight.cpu().detach().numpy() # output_ch x input_ch x D x H x W\n",
    "    output_ch = np.minimum(cnn.weight.shape[0], max_col)\n",
    "    input_ch = np.minimum(cnn.weight.shape[1], max_row)\n",
    "    print(filters.shape)\n",
    "    \n",
    "    if isinstance(cnn, nn.Conv3d):\n",
    "        if select_channel:\n",
    "            assert isinstance(select_channel, int)\n",
    "            filters = filters[:, :, select_channel, :, :]\n",
    "        else:\n",
    "            filters = np.mean(filters[:, :, :, :, :], axis=2)\n",
    "    \n",
    "    plt_idx = 0\n",
    "    fig = plt.figure(figsize=(output_ch, input_ch))\n",
    "    plt.xlabel(\"Output Channel\")\n",
    "    plt.ylabel(\"Input Channel\")\n",
    "    frame1 = plt.gca()\n",
    "    frame1.axes.xaxis.set_ticklabels([])\n",
    "    frame1.axes.yaxis.set_ticklabels([])\n",
    "    for o in range(output_ch):\n",
    "        for i in range(input_ch):\n",
    "            image = filters[o, i, :, :]\n",
    "            plt_idx += 1\n",
    "            ax = fig.add_subplot(input_ch, output_ch, plt_idx)\n",
    "            ax.imshow(image)\n",
    "            ax.axis('off')\n",
    "#     plt.tight_layout()\n",
    "    if save_path:\n",
    "        plt.savefig(save_path)\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GradCAM (class activation mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skimage.transform import resize\n",
    "\n",
    "class CamExtractor():\n",
    "    \"\"\"\n",
    "        Extracts cam features from the model\n",
    "    \"\"\"\n",
    "    def __init__(self, model, register_hooks=True):\n",
    "        self.model = model\n",
    "        self.grad = None\n",
    "        self.conv_output = None\n",
    "        \n",
    "        if register_hooks:\n",
    "            self.register_hooks()\n",
    "        \n",
    "    def gradient_hook(self, model, grad_input, grad_output):\n",
    "        self.grad = grad_output[0].cpu().detach().numpy()\n",
    "        \n",
    "    def conv_output_hook(self, model, input, output):\n",
    "        self.conv_output = output.cpu().detach().numpy()\n",
    "        \n",
    "    def register_hooks(self):\n",
    "        raise NotImplementedError(\"You should implement this method for your own model!\")\n",
    "        \n",
    "    def forward(self, x):\n",
    "        raise NotImplementedError(\"You should implement this method for your own model!\")\n",
    "        \n",
    "    def to_image(self, height=None, width=None):\n",
    "        assert self.grad is not None and self.conv_output is not None, \"You should perform both forward pass and backward propagation first!\"\n",
    "        # both grad and conv_output should have the same dimension of: (*, channel, H, W)\n",
    "        # we produce image(s) of shape: (*, H, W)\n",
    "        channel_weight = np.mean(self.grad, axis=(-2, -1)) # *, channel\n",
    "        conv_permuted = np.moveaxis(self.conv_output, [-2, -1], [0, 1]) # H, W, *, channel\n",
    "        cam_image_permuted = channel_weight * conv_permuted # H, W, *, channel\n",
    "        cam_image_permuted = np.mean(cam_image_permuted, axis=-1) # H, W, *\n",
    "        cam_image = np.moveaxis(cam_image_permuted, [0, 1], [-2, -1]) # *, H, W\n",
    "        \n",
    "        if height is not None and width is not None:\n",
    "            image_shape = list(cam_image.shape)\n",
    "            image_shape[-2] = height\n",
    "            image_shape[-1] = width\n",
    "            cam_image = resize(cam_image, image_shape)\n",
    "        return cam_image\n",
    "        \n",
    "    \n",
    "class CamExtractorTLModel(CamExtractor):\n",
    "    \n",
    "    def register_hooks(self):\n",
    "        self.model.ResNet.layer4.register_forward_hook(self.conv_output_hook)\n",
    "        self.model.ResNet.layer4.register_backward_hook(self.gradient_hook)\n",
    "\n",
    "class CamExtractor3DCNN(CamExtractor):\n",
    "    \n",
    "    def gradient_hook(self, model, grad_input, grad_output):\n",
    "        grad = grad_output[0].cpu().detach().numpy()\n",
    "        self.grad = np.moveaxis(grad, 1, 2) # restore old dimension (batch x seq_len x channel x H x W)\n",
    "        \n",
    "    def conv_output_hook(self, model, input, output):\n",
    "        conv_output = output.cpu().detach().numpy()\n",
    "        self.conv_output = np.moveaxis(conv_output, 1, 2) # restore old dimension (batch x seq_len x channel x H x W)\n",
    "    \n",
    "    def register_hooks(self):\n",
    "        self.model.Convolution6.register_forward_hook(self.conv_output_hook)\n",
    "        self.model.Convolution6.register_backward_hook(self.gradient_hook)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_gradcam(cam_image, sample_image, save_image=None):\n",
    "    \"\"\"\n",
    "    Show CAM image as an overlay to sample image\n",
    "    \n",
    "    Args:\n",
    "        cam_image: image extracted from CamExtractor.to_image(), shape: (*, H, W)\n",
    "        sample_image: the image fed into the model, shape: (batch_size, seq_len, channel, H, W) or (batch_size, channel, H, W) if seq_len == 1\n",
    "    \"\"\"\n",
    "    \n",
    "    n_dims = len(cam_image.shape)\n",
    "    assert n_dims == len(sample_image.shape) - 1 and (n_dims == 4 or n_dims == 3), \"cam_image dim: {} but sample_image dim: {}\".format(cam_image.shape, sample_image.shape)\n",
    "    \n",
    "    batch_size = cam_image.shape[0]\n",
    "    assert batch_size == sample_image.shape[0], \"batch size of sample image and cam image must agree!\"\n",
    "    \n",
    "    if n_dims == 4:\n",
    "        seq_len = cam_image.shape[1]\n",
    "        assert seq_len == sample_image.shape[1], \"seq_len of sample image and cam image must agree!\"\n",
    "    else:\n",
    "        seq_len = 1\n",
    "    \n",
    "    fig = plt.figure(figsize=(seq_len*6, batch_size*4))\n",
    "    plt.xlabel(\"seq_idx\")\n",
    "    plt.ylabel(\"batch_idx\")\n",
    "    frame1 = plt.gca()\n",
    "    frame1.axes.xaxis.set_ticklabels([])\n",
    "    frame1.axes.yaxis.set_ticklabels([])\n",
    "    \n",
    "    plt_idx = 1\n",
    "    for b in range(batch_size):\n",
    "        for s in range(seq_len):\n",
    "            cam_image_cur = cam_image[b, s, :, :] if n_dims == 4 else cam_image[b, :, :]\n",
    "            sample_image_cur = sample_image[b, s, :, :, :] if n_dims == 4 else sample_image[b, :, :, :]\n",
    "            sample_image_cur = sample_image_cur.permute(1, 2, 0) # putting color dim in the end, as required by matplotlib\n",
    "            \n",
    "            ax = fig.add_subplot(batch_size, seq_len, plt_idx)\n",
    "            ax.imshow(sample_image_cur)\n",
    "            ax.imshow(cam_image_cur, cmap='jet', alpha=0.4)\n",
    "            ax.axis('off')\n",
    "            \n",
    "            plt_idx += 1\n",
    "            \n",
    "    plt.tight_layout()\n",
    "    if save_image:\n",
    "        plt.savefig(save_image, format='png')\n",
    "        plt.close()\n",
    "    else:\n",
    "        plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
