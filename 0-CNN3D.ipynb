{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skimage.transform import resize\n",
    "from itertools import product\n",
    "from collections import OrderedDict\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms, utils, models\n",
    "import torch.nn as nn \n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "from thirdparty.Run import RunBuilder as RB\n",
    "from thirdparty.Run import RunManager as RM\n",
    "\n",
    "%run DataLoader.ipynb\n",
    "%run Visualization.ipynb\n",
    "\n",
    "device = torch.device(\"cuda\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchvision import transforms, utils\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class Convolution3D(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Convolution3D,self).__init__()\n",
    "        self.Convolution1 = nn.Conv3d(in_channels = 3, out_channels = 64, kernel_size=(3,3,3), stride=1, padding=(1,0,0), dilation=1, groups=1, bias=True, padding_mode='zeros')\n",
    "        self.BatchN1 = nn.BatchNorm3d(num_features = 64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
    "        \n",
    "        self.MaxPooling1 = nn.MaxPool3d(kernel_size = (1,2,2), stride=(1,2,2), padding=(0,0,0), dilation=1, return_indices=False, ceil_mode=False)\n",
    "        self.MaxPooling2 = nn.MaxPool3d(kernel_size =(1,2,2), stride=(1,2,2), padding=(0,0,0), dilation=1, return_indices=False, ceil_mode=False)\n",
    "        \n",
    "        self.Convolution2 = nn.Conv3d(in_channels = 64, out_channels = 64,kernel_size=3,stride = 1, padding = (1,0,0))\n",
    "        self.BatchN2 = nn.BatchNorm3d(num_features = 64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
    "        \n",
    "        self.ResConvolution1 = nn.Conv3d(in_channels = 64, out_channels = 64, kernel_size = 3, stride = 1, padding = (1,1,1))\n",
    "        self.averagePool1 = nn.AvgPool3d(kernel_size = 1, stride =1, padding = (0,0,0))\n",
    "        self.ResBatchN1 = nn.BatchNorm3d(num_features = 64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
    "        \n",
    "        self.Convolution3 = nn.Conv3d(in_channels = 64, out_channels = 64,kernel_size=3,stride = 1, padding = (1,0,0))\n",
    "        self.BatchN3 = nn.BatchNorm3d(num_features = 64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
    "        \n",
    "        self.ResConvolution2 = nn.Conv3d(in_channels = 64, out_channels = 64, kernel_size = 3, stride = 1, padding = (1,1,1))\n",
    "        self.averagePool2 = nn.AvgPool3d(kernel_size = 1, stride =1, padding = (0,0,0) )\n",
    "        self.ResBatchN2 = nn.BatchNorm3d(num_features = 64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
    "        \n",
    "        self.Convolution4 = nn.Conv3d(in_channels = 64, out_channels = 8, kernel_size=(3,3,3), stride=1, padding=(1,0,0), dilation=1, groups=1, bias=True, padding_mode='zeros')\n",
    "        self.BatchN4 = nn.BatchNorm3d(num_features = 8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
    "        \n",
    "        self.Convolution5 = nn.Conv3d(in_channels = 8, out_channels = 8, kernel_size=(3,3,3), stride=1, padding=(1,0,0), dilation=1, groups=1, bias=True, padding_mode='zeros')\n",
    "        self.BatchN5 = nn.BatchNorm3d(num_features = 8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
    "        \n",
    "        self.Convolution6 = nn.Conv3d(in_channels = 8, out_channels = 8, kernel_size=(3,3,3), stride=1, padding=(1,0,0), dilation=1, groups=1, bias=True, padding_mode='zeros')\n",
    "        self.BatchN6 = nn.BatchNorm3d(num_features = 8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
    "        \n",
    "        self.Flatten1 = nn.Flatten(start_dim=2)\n",
    "        \n",
    "        self.LSTM1 = nn.LSTM(input_size = 10488, hidden_size = 64, num_layers = 1, batch_first=True)\n",
    "        self.LSTM2 = nn.LSTM(input_size = 64, hidden_size = 16, num_layers = 1, batch_first=True)\n",
    "        \n",
    "        self.fc1 = nn.Linear(in_features =16, out_features = 512, bias=True)\n",
    "        self.fc2 = nn.Linear(in_features = 512, out_features = 128, bias=True)\n",
    "        self.fc3 = nn.Linear(in_features = 128, out_features = 64, bias=True)\n",
    "        self.fc4 = nn.Linear(in_features = 64, out_features = 16, bias=True)\n",
    "        self.fc5 = nn.Linear(in_features = 16, out_features = 1, bias=True)\n",
    "    def forward(self, Input):\n",
    "        # 3*3*3 3D Conv 3\n",
    "        image = F.relu(self.BatchN1(self.Convolution1(Input)))\n",
    "        # input size = (1,3,15,120,320)-(Batches, Channels, Depth, Height, Width)\n",
    "        # output size = (1, 64, 13, 118, 318)-(Batches, Channels, Depth, Height, Width)\n",
    "        \n",
    "        # 3D Max Pooling\n",
    "        image = self.MaxPooling1(image)\n",
    "        # input size = (1, 64, 13, 118, 318)-(Batches, Channels, Depth, Height, Width)\n",
    "        # output size = (1, 64, 12, 117, 317)-(Batches, Channels, Depth, Height, Width)\n",
    "        \n",
    "        # 3D Max Pooling\n",
    "        image = self.MaxPooling2(image)\n",
    "        # input size = (1, 64, 12, 117, 317)-(Batches, Channels, Depth, Height, Width)\n",
    "        # output size = (1, 64, 11, 116, 316)-(Batches, Channels, Depth, Height, Width)\n",
    "        \n",
    "        # 3*3*3 3D Conv 64\n",
    "        image = F.relu(self.BatchN2(self.Convolution2(image)))\n",
    "        \n",
    "        # ResNet - 3*3*3 3D Conv 64\n",
    "        Residual = image\n",
    "        Res_Output = F.relu(self.ResBatchN1(self.ResConvolution1(image)))\n",
    "        image = F.relu(Residual + Res_Output)\n",
    "        image = self.averagePool1(image)\n",
    "        \n",
    "        # 3*3*3 3D Conv 64\n",
    "        image = F.relu(self.BatchN3(self.Convolution3(image)))\n",
    "        \n",
    "        # ResNet - 3*3*3 3D Conv 64\n",
    "        Residual = image\n",
    "        Res_Output = F.relu(self.ResBatchN2(self.ResConvolution2(image)))\n",
    "        image = F.relu(Residual + Res_Output)\n",
    "        del Residual\n",
    "        del Res_Output\n",
    "        image = self.averagePool2(image)\n",
    "        \n",
    "        # 3*3*3 3D Conv 8\n",
    "        image = F.relu(self.BatchN4(self.Convolution4(image)))\n",
    "        \n",
    "        # 3*3*3 3D Conv 8\n",
    "        image = F.relu(self.BatchN5(self.Convolution5(image)))\n",
    "        \n",
    "        # 3*3*3 3D Conv 8\n",
    "        image = F.relu(self.BatchN6(self.Convolution6(image)))\n",
    "        \n",
    "        # LSTM 64\n",
    "        image = image.permute([0,2,1,3,4])\n",
    "        image = self.Flatten1(image)\n",
    "        image = self.LSTM1(image)\n",
    "        image = image[0]\n",
    "        image = torch.tanh(image)\n",
    "        \n",
    "        # LSTM 16\n",
    "        image = torch.tanh(self.LSTM2(image)[0])\n",
    "\n",
    "        # FC 512\n",
    "        image = image.permute(1,0,2)\n",
    "        image = F.relu(self.fc1(image))\n",
    " \n",
    "        # FC 128\n",
    "        image = F.relu(self.fc2(image))\n",
    "\n",
    "        # FC 64\n",
    "        image = F.relu(self.fc3(image))\n",
    " \n",
    "        # FC 16\n",
    "        image = F.relu(self.fc4(image))\n",
    "\n",
    "        # FC 1\n",
    "        image = self.fc5(image)\n",
    "\n",
    "        return image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Training / Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7b00bc7d78d841de990c6833c7066e8d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=1081), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/opt/anaconda3/lib/python3.7/multiprocessing/queues.py\", line 242, in _feed\n",
      "    send_bytes(obj)\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/anaconda3/lib/python3.7/multiprocessing/queues.py\", line 242, in _feed\n",
      "    send_bytes(obj)\n",
      "  File \"/opt/anaconda3/lib/python3.7/multiprocessing/connection.py\", line 200, in send_bytes\n",
      "    self._send_bytes(m[offset:offset + size])\n",
      "  File \"/opt/anaconda3/lib/python3.7/multiprocessing/connection.py\", line 404, in _send_bytes\n",
      "    self._send(header + buf)\n",
      "  File \"/opt/anaconda3/lib/python3.7/multiprocessing/connection.py\", line 368, in _send\n",
      "    n = write(self._handle, buf)\n",
      "BrokenPipeError: [Errno 32] Broken pipe\n",
      "  File \"/opt/anaconda3/lib/python3.7/multiprocessing/connection.py\", line 200, in send_bytes\n",
      "    self._send_bytes(m[offset:offset + size])\n",
      "  File \"/opt/anaconda3/lib/python3.7/multiprocessing/connection.py\", line 404, in _send_bytes\n",
      "    self._send(header + buf)\n",
      "  File \"/opt/anaconda3/lib/python3.7/multiprocessing/connection.py\", line 368, in _send\n",
      "    n = write(self._handle, buf)\n",
      "BrokenPipeError: [Errno 32] Broken pipe\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-2f6e5b788db8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[0;31m# Calculation on Training Loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtraining_sample\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtraining_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtotal\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtraining_set\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mseq_len\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 45\u001b[0;31m             \u001b[0mtraining_sample\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'image'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtraining_sample\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'image'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mseq_len\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m120\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m320\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0manti_aliasing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     46\u001b[0m             \u001b[0mtraining_sample\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'image'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtraining_sample\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'image'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpermute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/skimage/transform/_warps.py\u001b[0m in \u001b[0;36mresize\u001b[0;34m(image, output_shape, order, mode, cval, clip, preserve_range, anti_aliasing, anti_aliasing_sigma)\u001b[0m\n\u001b[1;32m    191\u001b[0m         \u001b[0mndi_mode\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_to_ndimage_mode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    192\u001b[0m         out = ndi.map_coordinates(image, coord_map, order=order,\n\u001b[0;32m--> 193\u001b[0;31m                                   mode=ndi_mode, cval=cval)\n\u001b[0m\u001b[1;32m    194\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    195\u001b[0m         \u001b[0m_clip_warp_output\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcval\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclip\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/scipy/ndimage/interpolation.py\u001b[0m in \u001b[0;36mmap_coordinates\u001b[0;34m(input, coordinates, output, order, mode, cval, prefilter)\u001b[0m\n\u001b[1;32m    348\u001b[0m                                      shape=output_shape)\n\u001b[1;32m    349\u001b[0m     _nd_image.geometric_transform(filtered, None, coordinates, None, None,\n\u001b[0;32m--> 350\u001b[0;31m                                   output, order, mode, cval, None, None)\n\u001b[0m\u001b[1;32m    351\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    352\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "parameters = OrderedDict(\n",
    "    file = ['3DCNN_Paper'], # used to mark specific files in case that we want to check them on tensorboard\n",
    "    learning_rate = [0.001],\n",
    "    batch_size = [5],\n",
    "    seq_len = [5],\n",
    "    num_workers = [2],\n",
    ")\n",
    "m = RM.RunManager()\n",
    "\n",
    "for run in RB.RunBuilder.get_runs(parameters):\n",
    "    network = Convolution3D().to(device)\n",
    "\n",
    "    optimizer = optim.Adam(network.parameters(),lr = run.learning_rate,betas=(0.9, 0.999), eps=1e-08, weight_decay=0.001, amsgrad=False)\n",
    "\n",
    "    udacity_dataset = UdacityDataset(csv_file='/export/jupyterlab/data/udacity-challenge-2/Ch2_002_export/interpolated.csv',\n",
    "                                     root_dir='/export/jupyterlab/data/udacity-challenge-2/Ch2_002_export/',\n",
    "                                     transform=transforms.Compose([transforms.ToTensor()]),\n",
    "                                     select_camera='center_camera')\n",
    "\n",
    "    dataset_size = int(len(udacity_dataset))\n",
    "    del udacity_dataset\n",
    "    split_point = int(dataset_size * 0.8)\n",
    "\n",
    "    training_set = UdacityDataset(csv_file='/export/jupyterlab/data/udacity-challenge-2/Ch2_002_export/interpolated.csv',\n",
    "                                     root_dir='/export/jupyterlab/data/udacity-challenge-2/Ch2_002_export/',\n",
    "                                     transform=transforms.Compose([transforms.ToTensor()]),\n",
    "                                     select_camera='center_camera',\n",
    "                                     select_range=(0,split_point))\n",
    "    validation_set = UdacityDataset(csv_file='/export/jupyterlab/data/udacity-challenge-2/Ch2_002_export/interpolated.csv',\n",
    "                                     root_dir='/export/jupyterlab/data/udacity-challenge-2/Ch2_002_export/',\n",
    "                                     transform=transforms.Compose([transforms.ToTensor()]),\n",
    "                                     select_camera='center_camera',\n",
    "                                     select_range=(split_point,dataset_size))\n",
    "\n",
    "    training_cbs = ConsecutiveBatchSampler(data_source=training_set, batch_size=run.batch_size, shuffle=True, drop_last=False, seq_len=run.seq_len)\n",
    "    training_loader = DataLoader(training_set, sampler=training_cbs, num_workers=run.num_workers, collate_fn=(lambda x: x[0]))\n",
    "    \n",
    "    validation_cbs = ConsecutiveBatchSampler(data_source=validation_set, batch_size=run.batch_size, shuffle=True, drop_last=False, seq_len=run.seq_len)\n",
    "    validation_loader = DataLoader(validation_set, sampler=validation_cbs, num_workers=run.num_workers, collate_fn=(lambda x: x[0]))\n",
    "    \n",
    "    m.begin_run( run,network,[run.batch_size,3,run.seq_len,120,320] )\n",
    "    for epoch in range(10):\n",
    "        m.begin_epoch()\n",
    "# Calculation on Training Loss\n",
    "        for training_sample in tqdm(training_loader, total=int(len(training_set)/run.batch_size/run.seq_len)):\n",
    "            training_sample['image'] = torch.Tensor(resize(training_sample['image'], (run.batch_size,run.seq_len,3,120,320),anti_aliasing=True))\n",
    "            training_sample['image'] = training_sample['image'].permute(0,2,1,3,4)\n",
    "            \n",
    "            param_values = [v for v in training_sample.values()]\n",
    "            image,angle = param_values[0],param_values[3]\n",
    "            image = image.to(device)\n",
    "            prediction = network(image)\n",
    "            prediction = prediction.squeeze().permute(1,0).to(device)\n",
    "            labels = angle.to(device)\n",
    "            del param_values, image, angle\n",
    "            if labels.shape[0]!=prediction.shape[0]:\n",
    "                prediction = prediction[-labels.shape[0],:]\n",
    "            training_loss_angle = F.mse_loss(prediction,labels,size_average=None, reduce=None, reduction='mean')\n",
    "            optimizer.zero_grad()# zero the gradient that are being held in the Grad attribute of the weights\n",
    "            training_loss_angle.backward() # calculate the gradients\n",
    "            optimizer.step() # finishing calculation on gradient\n",
    "        print(\"Done\")\n",
    "# Calculation on Validation Loss\n",
    "        with torch.no_grad():    \n",
    "            for Validation_sample in tqdm(validation_loader, total=int(len(validation_set)/run.batch_size/run.seq_len)):\n",
    "                Validation_sample['image'] = torch.Tensor(resize(Validation_sample['image'], (run.batch_size,run.seq_len,3,120,320),anti_aliasing=True))\n",
    "                Validation_sample['image'] = Validation_sample['image'].permute(0,2,1,3,4)\n",
    "\n",
    "                param_values = [v for v in Validation_sample.values()]\n",
    "                image,angle = param_values[0],param_values[3]\n",
    "                image = image.to(device)\n",
    "                prediction = network(image)\n",
    "                prediction = prediction.squeeze().permute(1,0).to(device)\n",
    "                labels = angle.to(device)\n",
    "                del param_values, image, angle\n",
    "                if labels.shape[0]!=prediction.shape[0]:\n",
    "                    prediction = prediction[-labels.shape[0],:]\n",
    "                validation_loss_angle = F.mse_loss(prediction,labels,size_average=None, reduce=None, reduction='mean')\n",
    "                m.track_loss(validation_loss_angle)\n",
    "                m.track_num_correct(prediction,labels) \n",
    "        m.end_epoch(validation_set)\n",
    "        torch.save(network.state_dict(), \"saved_models/CNN3D/epoch-{}\".format(epoch))\n",
    "    m.end_run()\n",
    "m.save('result')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load model directly from disk\n",
    "\n",
    "cnn3d_model = Convolution3D().to(device)\n",
    "cnn3d_model.load_state_dict(torch.load('saved_models/CNN3D/3DCNN_Model-epoch-4'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(64, 3, 3, 3, 3)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x216 with 31 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "visualize_cnn(cnn3d_model.Convolution1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GradCAM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn_cam_extractor = CamExtractor3DCNN(cnn3d_model)\n",
    "\n",
    "Batch_size = 1\n",
    "Seq_len = 5 \n",
    "\n",
    "\n",
    "udacity_dataset = UdacityDataset(csv_file='/export/jupyterlab/data/udacity-challenge-2/Ch2_002_export/interpolated.csv',\n",
    "                                 root_dir='/export/jupyterlab/data/udacity-challenge-2/Ch2_002_export/',\n",
    "                                 transform=transforms.Compose([transforms.ToTensor()]),\n",
    "                                 select_camera='center_camera')\n",
    "\n",
    "# 3D CNN has different input batch size and seq_len, so we define new CBS and Loader\n",
    "cbs_3dcnn = ConsecutiveBatchSampler(data_source=udacity_dataset, batch_size=Batch_size, shuffle=False, drop_last=False, seq_len=Seq_len)\n",
    "loader_3dcnn = DataLoader(udacity_dataset, sampler=cbs_3dcnn, collate_fn=(lambda x: x[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "623d7c1f7edd4dc1b12a1dc2f1773cc8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=33808), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "mse_loss = nn.MSELoss()\n",
    "\n",
    "for i, testing_sample in enumerate(tqdm(loader_3dcnn, total=len(loader_3dcnn))):\n",
    "    testing_sample['image'] = torch.Tensor(resize(testing_sample['image'], (Batch_size,Seq_len,3,120,320),anti_aliasing=True))\n",
    "    image_input = testing_sample['image'].permute(0,2,1,3,4).to(device)\n",
    "    prediction = cnn3d_model(image_input).squeeze()\n",
    "    target = testing_sample['angle'].squeeze().to(device)\n",
    "    loss = mse_loss(prediction, target)\n",
    "    loss.backward()\n",
    "    \n",
    "    cam_image = cnn_cam_extractor.to_image(width=320, height=120) # Use this line to extract CAM image from the model!\n",
    "    visualize_gradcam(cam_image, testing_sample['image'], 'cam_images/test_cam_{}.png'.format(i))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {
     "0b47bd490afb47d1a085b8aa806fa499": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "layout": "IPY_MODEL_8c498350898345158d3fc2f3f028e7ea",
       "style": "IPY_MODEL_db8087976c7344c2a7beefc58811f316",
       "value": " 10/33808 [00:05&lt;4:57:14,  1.90it/s]"
      }
     },
     "185d9519a8ea4abc9951e94b805d8d77": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "623d7c1f7edd4dc1b12a1dc2f1773cc8": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HBoxModel",
      "state": {
       "children": [
        "IPY_MODEL_bdd2b120f9184c18aa1e16b5f4e0a7ff",
        "IPY_MODEL_0b47bd490afb47d1a085b8aa806fa499"
       ],
       "layout": "IPY_MODEL_185d9519a8ea4abc9951e94b805d8d77"
      }
     },
     "73706a4626ef4bda98dab8ac204b05da": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "description_width": "initial"
      }
     },
     "8c498350898345158d3fc2f3f028e7ea": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "a081240dda294fb58274ae75f2275607": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "bdd2b120f9184c18aa1e16b5f4e0a7ff": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "IntProgressModel",
      "state": {
       "description": "  0%",
       "layout": "IPY_MODEL_a081240dda294fb58274ae75f2275607",
       "max": 33808,
       "style": "IPY_MODEL_73706a4626ef4bda98dab8ac204b05da",
       "value": 10
      }
     },
     "db8087976c7344c2a7beefc58811f316": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "description_width": ""
      }
     }
    },
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
