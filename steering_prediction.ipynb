{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function, division\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from skimage import io, transform\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "# defining customized Dataset class for Udacity\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms, utils\n",
    "\n",
    "class UdacityDataset(Dataset):\n",
    "    def __init__(self, csv_file, root_dir, transform=None, select_camera=None, slice_frames=None, select_ratio=1.0, select_range=None):\n",
    "        \n",
    "        assert select_ratio >= -1.0 and select_ratio <= 1.0 # positive: select to ratio from beginning, negative: select to ration counting from the end\n",
    "        \n",
    "        camera_csv = pd.read_csv(csv_file)\n",
    "        csv_len = len(camera_csv)\n",
    "        assert select_camera in ['left_camera', 'right_camera', 'center_camera'], \"Invalid camera: {}\".format(select_camera)\n",
    "        \n",
    "        if slice_frames:\n",
    "            csv_selected = camera_csv[0:0] # empty dataframe\n",
    "            for start_idx in range(0, csv_len, slice_frames):\n",
    "                if select_ratio > 0:\n",
    "                    end_idx = int(start_idx + slice_frames * select_ratio)\n",
    "                else:\n",
    "                    start_idx, end_idx = int(start_idx + slice_frames * (1 + select_ratio)), start_idx + slice_frames\n",
    "\n",
    "                if end_idx > csv_len:\n",
    "                    end_idx = csv_len\n",
    "                if start_idx > csv_len:\n",
    "                    start_idx = csv_len\n",
    "                csv_selected = csv_selected.append(camera_csv[start_idx:end_idx])\n",
    "            self.camera_csv = csv_selected\n",
    "        elif select_range:\n",
    "            csv_selected = camera_csv.iloc[select_range[0]: select_range[1]]\n",
    "            self.camera_csv = csv_selected\n",
    "        else:\n",
    "            self.camera_csv = camera_csv\n",
    "            \n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "        \n",
    "        # Keep track of mean and cov value in each channel\n",
    "        self.mean = {}\n",
    "        self.std = {}\n",
    "        for key in ['angle', 'torque', 'speed']:\n",
    "            self.mean[key] = np.mean(camera_csv[key])\n",
    "            self.std[key] = np.std(camera_csv[key])\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.camera_csv)\n",
    "    \n",
    "    def read_data_single(self, idx):\n",
    "        path = os.path.join(self.root_dir, self.camera_csv['filename'].iloc[idx])\n",
    "        image = io.imread(path)\n",
    "        timestamp = self.camera_csv['timestamp'].iloc[idx]\n",
    "        frame_id = self.camera_csv['frame_id'].iloc[idx]\n",
    "        angle = self.camera_csv['angle'].iloc[idx]\n",
    "        torque = self.camera_csv['torque'].iloc[idx]\n",
    "        speed = self.camera_csv['speed'].iloc[idx]\n",
    "        \n",
    "        if self.transform:\n",
    "            image_transformed = self.transform(image)\n",
    "            del image\n",
    "            image = image_transformed\n",
    "        angle_t = torch.tensor(angle)\n",
    "        torque_t = torch.tensor(torque)\n",
    "        speed_t = torch.tensor(speed)\n",
    "        del angle, torque, speed\n",
    "            \n",
    "        return image, timestamp, frame_id, angle_t, torque_t, speed_t\n",
    "    \n",
    "    def read_data(self, idx):\n",
    "        if isinstance(idx, list):\n",
    "            data = None\n",
    "            for i in idx:\n",
    "                new_data = self.read_data(i)\n",
    "                if data is None:\n",
    "                    data = [[] for _ in range(len(new_data))]\n",
    "                for i, d in enumerate(new_data):\n",
    "                    data[i].append(new_data[i])\n",
    "                del new_data\n",
    "                \n",
    "            for stack_idx in [0, 3, 4, 5]: # we don't stack timestamp and frame_id since those are string data\n",
    "                data[stack_idx] = torch.stack(data[stack_idx])\n",
    "            \n",
    "            return data\n",
    "        \n",
    "        else:\n",
    "            return self.read_data_single(idx)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "        \n",
    "        data = self.read_data(idx)\n",
    "        \n",
    "        sample = {'image': data[0],\n",
    "                  'timestamp': data[1],\n",
    "                  'frame_id': data[2],\n",
    "                  'angle': data[3],\n",
    "                  'torque': data[4],\n",
    "                  'speed': data[5]}\n",
    "        \n",
    "        del data\n",
    "        \n",
    "        return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate Batch with consecutive frames taken from input data\n",
    "\n",
    "from torch.utils.data import Sampler\n",
    "import random\n",
    "\n",
    "class ConsecutiveBatchSampler(Sampler):\n",
    "    \n",
    "    def __init__(self, data_source, batch_size, seq_len, drop_last=False, shuffle=True, use_all_frames=False):\n",
    "        r\"\"\" Sampler to generate consecutive Batches\n",
    "        \n",
    "        Args:\n",
    "            data_source: Source of data\n",
    "            batch_size: Size of batch\n",
    "            seq_len: Number of frames in each sequence (used for context for prediction)\n",
    "            drop: Wether to drop the last incomplete batch\n",
    "            shuffle: Wether to shuffle the data\n",
    "        Return:\n",
    "            List of iterators, size: [batch_size x seq_len x n_channels x height x width]\n",
    "        \"\"\"\n",
    "        super(ConsecutiveBatchSampler, self).__init__(data_source)\n",
    "        \n",
    "        self.data_source = data_source\n",
    "        \n",
    "        assert seq_len >= 1, \"Invalid batch size: {}\".format(seq_len)\n",
    "        self.seq_len = seq_len\n",
    "        self.drop_last = drop_last\n",
    "        self.shuffle = shuffle\n",
    "        self.batch_size = batch_size\n",
    "        self.use_all_frames_ = use_all_frames\n",
    "    \n",
    "    def __iter__(self):\n",
    "        \n",
    "        data_size = len(self.data_source)\n",
    "        \n",
    "        if self.use_all_frames_:\n",
    "            start_indices = list(range(data_size))\n",
    "        else:\n",
    "            start_indices = list(range(1, data_size, self.seq_len))\n",
    "            \n",
    "        if self.shuffle:\n",
    "            random.shuffle(start_indices)\n",
    "        \n",
    "        batch = []\n",
    "        for idx, ind in enumerate(start_indices):\n",
    "            if data_size - idx < self.batch_size and self.drop_last: # if last batch\n",
    "                break\n",
    "                \n",
    "            seq = []\n",
    "            if ind + 1 < self.seq_len:\n",
    "                seq.extend([0]*(self.seq_len - ind - 1) + list(range(0, ind+1)))\n",
    "            else:\n",
    "                seq.extend(list(range(ind-self.seq_len+1, ind+1)))\n",
    "            \n",
    "            batch.append(seq)\n",
    "            \n",
    "            if len(batch) == self.batch_size or idx == data_size - 1:\n",
    "                yield batch\n",
    "                batch = []\n",
    "\n",
    "    \n",
    "    def __len__(self):\n",
    "        length = len(self.data_source)\n",
    "        batch_size = self.batch_size\n",
    "        \n",
    "        if length % batch_size == 0 or self.drop_last:\n",
    "            return length // batch_size\n",
    "        \n",
    "        return length // batch_size + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_sample(sample):\n",
    "    r\"\"\" Helper function for (batch) sample visualization\n",
    "    \n",
    "    Args:\n",
    "        sample: Dictionary\n",
    "    \"\"\"\n",
    "    image_dims = len(sample['image'].shape)\n",
    "    assert image_dims <= 5, \"Unsupported image shape: {}\".format(sample['image'].shape)\n",
    "    if image_dims == 3:\n",
    "        plt.imshow(sample['image'])\n",
    "    else:\n",
    "        n0 = sample['image'].shape[0]\n",
    "        n1 = sample['image'].shape[1] if image_dims == 5 else 1\n",
    "        images_flattened = torch.flatten(sample['image'], end_dim=-4)\n",
    "        fig, ax = plt.subplots(n0, n1, figsize=(25, 15))\n",
    "        for i1 in range(n1):\n",
    "            for i0 in range(n0):\n",
    "                image = images_flattened[i0 * n1 + i1]\n",
    "                axis = ax[i0, i1]\n",
    "                axis.imshow(image.permute(1,2,0))\n",
    "                axis.axis('off')\n",
    "                axis.set_title(\"t={}\".format(sample['timestamp'][i0][i1]))\n",
    "                axis.text(10, 30, sample['frame_id'][i0][i1], color='red')\n",
    "            \n",
    "    \n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "for i_batch, sample_batched in enumerate(dataloader):\n",
    "    \n",
    "    # Your code here...\n",
    "    \n",
    "    del sample_batched # release image to save memory space\n",
    "    \n",
    "    if i_batch == 2: # test loading 10 datapoints\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3D CNN with residual connection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the model\n",
    "\n",
    "import torch.nn as nn\n",
    "\n",
    "# helper function to determine dimension after convolution\n",
    "def conv_output_shape(in_dimension, kernel_size, stride):\n",
    "    output_dim = []\n",
    "    for (in_dim, kern_size, strd) in zip(in_dimension, kernel_size, stride):\n",
    "        len = int(float(in_dim - kern_size) / strd + 1.)\n",
    "        output_dim.append(len)\n",
    "    \n",
    "    return output_dim\n",
    "\n",
    "        \n",
    "class TemporalCNN(nn.Module):\n",
    "    \n",
    "    def _conv_unit(self, in_channels, out_channels, in_shape, kernel_size, stride, dropout_prob):\n",
    "        r\"\"\" Return one 3D convolution unit\n",
    "        \n",
    "        Args:\n",
    "            in_channels: Input channels of the Conv3D module\n",
    "            out_channels: Output channels of the Conv3D module\n",
    "            in_shape: Shape of the input image. i.e. The last 3 dimensions of the input tensor: D x H x W\n",
    "            kernel_size: Kernel size\n",
    "            stride: Stride\n",
    "            dropout_prob: Probability of dropout layer\n",
    "                         \n",
    "        Output:\n",
    "            (conv_module, aux_module, out_shape)\n",
    "        \"\"\"\n",
    "        \n",
    "        conv = nn.Conv3d(in_channels=in_channels, out_channels=out_channels, kernel_size=kernel_size, stride=stride)\n",
    "        dropout = nn.Dropout3d(p=dropout_prob)\n",
    "        conv_module = nn.Sequential(conv, dropout).to(self.device_)\n",
    "        out_shape = conv_output_shape(in_shape, kernel_size, stride)\n",
    "        \n",
    "        flatten = nn.Flatten(start_dim=2)\n",
    "        aux = nn.Linear(in_features=np.prod(out_shape[-2:])*out_channels, out_features=128)\n",
    "        aux_module = nn.Sequential(flatten, aux).to(self.device_)\n",
    "        \n",
    "        return conv_module, aux_module, out_shape\n",
    "    \n",
    "    def _linear_unit(self, in_features, out_features, dropout_prob):\n",
    "        relu = nn.ReLU()\n",
    "        linear = nn.Linear(in_features, out_features)\n",
    "        dropout = nn.Dropout(p=dropout_prob)\n",
    "        return nn.Sequential(relu, linear, dropout).to(self.device_), out_features\n",
    "        \n",
    "    \n",
    "    def __init__(self, in_height, in_width, seq_len, dropout_prob=0.5, aux_history=10, device=None):\n",
    "        r\"\"\" TemporalCNN: this model does 3D convolution on the H, W and temporal dimension\n",
    "             It also includes residual connection from each Conv3D output to the final output\n",
    "             \n",
    "             Args:\n",
    "                 in_height: image height\n",
    "                 in_width: image width\n",
    "                 seq_len: image sequence length\n",
    "                 dropout_prob: prob for the dropout layer\n",
    "                 aux_history: length of history to extract from seq_len\n",
    "             \n",
    "             Output:\n",
    "                 nn.Module, accepts input with shape [batch_len, seq_len, C, in_height, in_width]\n",
    "        \"\"\"\n",
    "        super(TemporalCNN, self).__init__()\n",
    "        \n",
    "        self.seq_len_ = seq_len\n",
    "        self.in_width_ = in_width\n",
    "        self.in_height_ = in_height\n",
    "        self.aux_history_ = aux_history\n",
    "        in_shape = (seq_len, in_height, in_width)\n",
    "        \n",
    "        self.device_ = device if device is not None else torch.device(\"cpu\")\n",
    "        \n",
    "        # conv1\n",
    "        self.conv0_, self.aux0_, out_shape = self._conv_unit(3, 64, in_shape, (3, 12, 12), (1, 6, 6), dropout_prob)\n",
    "        # conv2\n",
    "        self.conv1_, self.aux1_, out_shape = self._conv_unit(64, 64, out_shape, (2, 5, 5), (1, 2, 2), dropout_prob)\n",
    "        # conv3\n",
    "        self.conv2_, self.aux2_, out_shape = self._conv_unit(64, 64, out_shape, (2, 5, 5), (1, 1, 1), dropout_prob)\n",
    "        # conv4\n",
    "        self.conv3_, self.aux3_, out_shape = self._conv_unit(64, 64, out_shape, (2, 5, 5), (1, 1, 1), dropout_prob)\n",
    "        \n",
    "        # Flatten the last 3 dims\n",
    "        self.flatten_ = nn.Flatten(start_dim=2).to(self.device_)\n",
    "        \n",
    "        # FC 1024\n",
    "        self.linear0_, out_features = self._linear_unit(64*np.prod(out_shape[-2:]), 1024, dropout_prob)\n",
    "        # FC 512\n",
    "        self.linear1_, out_features = self._linear_unit(out_features, 512, dropout_prob)\n",
    "        # FC 256\n",
    "        self.linear2_, out_features = self._linear_unit(out_features, 256, dropout_prob)\n",
    "        # FC 128\n",
    "        self.linear3_ = nn.Linear(out_features, 128).to(device)\n",
    "        \n",
    "        self.elu_ = nn.ELU().to(self.device_)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = x.permute([0, 2, 1, 3, 4]) # swap channel and seq_len, 3D conv over seq_len as depth channel\n",
    "                                       # now: [batch_size, channel, seq_len, H, W]\n",
    "        \n",
    "        aux_outputs = []\n",
    "        for layer in [(self.conv0_, self.aux0_),\n",
    "                      (self.conv1_, self.aux1_),\n",
    "                      (self.conv2_, self.aux2_),\n",
    "                      (self.conv3_, self.aux3_)]:\n",
    "            x_out = layer[0](x)\n",
    "            x_out_permuted = x_out.permute([0, 2, 1, 3, 4]) # swap back for calculation of aux output\n",
    "            x_aux = layer[1](x_out_permuted[:,-self.aux_history_:,:,:,:])\n",
    "#             print(x_out.shape)\n",
    "#             print(x_aux.shape)\n",
    "            aux_outputs.append(x_aux)\n",
    "            x = x_out\n",
    "        \n",
    "        x = x.permute([0, 2, 1, 3, 4]) # swap back the dimensions, now: [batch_size, seq_len, channel, H, W]\n",
    "        \n",
    "        for layer in [self.flatten_, self.linear0_, self.linear1_, self.linear2_, self.linear3_]:\n",
    "            x = layer(x)\n",
    "        \n",
    "        final_out = x\n",
    "        for aux_out in aux_outputs:\n",
    "            final_out = final_out + aux_out\n",
    "        final_out = self.elu_(final_out)\n",
    "#         print(final_out.shape)\n",
    "        \n",
    "        return final_out\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Autoregressive LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Autoregressive LSTM Module\n",
    "\n",
    "class AutoregressiveLSTM(nn.Module):\n",
    "    \n",
    "    def __init__(self, target_size, visual_feature_size, hidden_size, internal_lstm, autoregressive_mode=True, device=None):\n",
    "        super(AutoregressiveLSTM, self).__init__()\n",
    "        \n",
    "        if device is None:\n",
    "            device = torch.device(\"cpu\")\n",
    "        self.device_ = device\n",
    "        \n",
    "        self.autoregressive_mode_ = autoregressive_mode\n",
    "        self.lstm_ = internal_lstm # LSTM module shared between Autoregressive and Groundtruth\n",
    "        self.linear_ = nn.Linear(visual_feature_size + target_size + hidden_size,\n",
    "                                 target_size).to(device)\n",
    "        self.target_size_ = target_size\n",
    "        self.visual_feature_size_ = visual_feature_size\n",
    "        self.hidden_size_ = hidden_size\n",
    "    \n",
    "    def forward(self, visual_features, prev_target=None, prev_states=None, target_groundtruth=None):\n",
    "        \n",
    "        assert (self.autoregressive_mode_) or (target_groundtruth is not None)\n",
    "        \n",
    "        if prev_target is None or not self.autoregressive_mode_:\n",
    "            batch_size, seq_len, _ = visual_features.shape\n",
    "            prev_target = torch.zeros(batch_size, seq_len, self.target_size_, device=self.device_)\n",
    "            \n",
    "        if self.autoregressive_mode_:\n",
    "            lstm_input = torch.cat((visual_features, prev_target), dim=-1)\n",
    "        else:\n",
    "            lstm_input = torch.cat((visual_features, target_groundtruth), dim=-1)\n",
    "    \n",
    "        lstm_output, lstm_states = self.lstm_(lstm_input, prev_states)\n",
    "        del lstm_input\n",
    "        \n",
    "        linear_input = torch.cat((visual_features, prev_target, lstm_output), dim=-1)\n",
    "        linear_output = self.linear_(linear_input)\n",
    "        del linear_input\n",
    "        \n",
    "        return (linear_output, lstm_states)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SteerNet(nn.Module):\n",
    "    \n",
    "    def __init__(self, device=None, dropout_prob=0.0):\n",
    "        super(SteerNet, self).__init__()\n",
    "        \n",
    "        self.device_ = device if device is not None else torch.device(\"cpu\")\n",
    "        \n",
    "        self.model_cnn_ = TemporalCNN(in_height=480, in_width=640, seq_len=15, dropout_prob=dropout_prob, aux_history=10, device=device)\n",
    "        \n",
    "#         # Freeze CNN\n",
    "#         for p in self.model_cnn_.parameters():\n",
    "#             p.requires_grad = False\n",
    "            \n",
    "        self.feature_dropout_ = nn.Dropout(p=dropout_prob)\n",
    "        self.lstm_ = nn.LSTM(input_size=128+3, hidden_size=64, batch_first=True).to(device)\n",
    "        self.model_lstm_gt_ = AutoregressiveLSTM(target_size=3, visual_feature_size=128, hidden_size=64, internal_lstm=self.lstm_, autoregressive_mode=False, device=device)\n",
    "        self.model_lstm_autoreg_ = AutoregressiveLSTM(target_size=3, visual_feature_size=128, hidden_size=64, internal_lstm=self.lstm_, autoregressive_mode=True, device=device)\n",
    "        \n",
    "        self.lstm_state_ = None\n",
    "        self.preserve_states_ = False\n",
    "        \n",
    "        self.train_ = True\n",
    "        \n",
    "        # for testing\n",
    "        self.test_lstm_ = nn.LSTM(input_size=128, hidden_size=64, batch_first=True, num_layers=2).to(device)\n",
    "        self.test_lstm_linear_ = nn.Linear(64, 3).to(device)\n",
    "    \n",
    "    def forward(self, images, target=None):\n",
    "        assert target is None or self.train_==True\n",
    "        \n",
    "        features = self.model_cnn_(images)\n",
    "        features = self.feature_dropout_(features)\n",
    "        \n",
    "#         if self.preserve_states_:\n",
    "            \n",
    "#             if self.lstm_state_ is None:\n",
    "#                 if self.train_:\n",
    "#                     out_gt, state_gt = self.model_lstm_gt_(features, target_groundtruth=target)\n",
    "#                 out_autoreg, state_autoreg = self.model_lstm_autoreg_(features)\n",
    "                \n",
    "#             else:\n",
    "#                 state_autoreg = self.lstm_state_[0]\n",
    "#                 if self.train_:\n",
    "#                     state_gt = self.lstm_state_[1]\n",
    "#                     out_gt, state_gt = self.model_lstm_gt_(features, prev_states=state_gt, target_groundtruth=target)\n",
    "#                 out_autoreg, state_autoreg = self.model_lstm_autoreg_(features, prev_states=state_autoreg)\n",
    "                \n",
    "#             state_autoreg_next = (state_autoreg[0].clone().detach(), state_autoreg[1].clone().detach())\n",
    "#             if self.train_:\n",
    "#                 state_gt_next = (state_gt[0].clone().detach(), state_gt[1].clone().detach())\n",
    "#             else:\n",
    "#                 state_gt_next = None\n",
    "                \n",
    "#             self.lstm_state_ = (state_autoreg_next, state_gt_next)\n",
    "        \n",
    "#         else:\n",
    "#             if self.train_:\n",
    "#                 out_gt, _ = self.model_lstm_gt_(features, target_groundtruth=target)\n",
    "#             else:\n",
    "#                 out_autoreg, _ = self.model_lstm_autoreg_(features)\n",
    "\n",
    "        # for testing\n",
    "#         out_autoreg = self.test_linear_(features)\n",
    "#         out_gt = self.test_linear_(features)\n",
    "        out, state = self.test_lstm_(features)\n",
    "        out_autoreg = self.test_lstm_linear_(out)\n",
    "        out_gt = out_autoreg\n",
    "        \n",
    "        return (out_autoreg, out_gt) if self.train_ else out_autoreg\n",
    "    \n",
    "    def train(self, mode=True):\n",
    "        self.train_ = mode\n",
    "        \n",
    "    def preserve_states(self, mode=False):\n",
    "        self.preserve_states_ = mode"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "size of training set: 8111\n",
      "stat:\n",
      "mean: {'angle': -0.008475752983277953, 'torque': -0.09077343871020742, 'speed': 15.622631789553342}\n",
      "std: {'angle': 0.27155946576081297, 'torque': 0.7871942194690468, 'speed': 5.695705986851927}\n",
      "size of validation set: 2028\n",
      "stat:\n",
      "mean: {'angle': -0.008475752983277953, 'torque': -0.09077343871020742, 'speed': 15.622631789553342}\n",
      "std: {'angle': 0.27155946576081297, 'torque': 0.7871942194690468, 'speed': 5.695705986851927}\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import BatchSampler, SequentialSampler, RandomSampler\n",
    "# train - validation split\n",
    "\n",
    "udacity_dataset = UdacityDataset(csv_file='/export/jupyterlab/data/udacity-challenge-2/Ch2_002_export/interpolated.csv',\n",
    "                              root_dir='/export/jupyterlab/data/udacity-challenge-2/Ch2_002_export/',\n",
    "                              transform=transforms.Compose([transforms.ToTensor()]),\n",
    "                              select_camera='center_camera')\n",
    "\n",
    "dataset_size = int(len(udacity_dataset) * 0.10)\n",
    "split_point = int(dataset_size * 0.8)\n",
    "training_set = UdacityDataset(csv_file='/export/jupyterlab/data/udacity-challenge-2/Ch2_002_export/interpolated.csv',\n",
    "                              root_dir='/export/jupyterlab/data/udacity-challenge-2/Ch2_002_export/',\n",
    "                              transform=transforms.Compose([transforms.ToTensor()]),\n",
    "                              select_camera='center_camera',\n",
    "                              select_range=(0, split_point))\n",
    "validation_set = UdacityDataset(csv_file='/export/jupyterlab/data/udacity-challenge-2/Ch2_002_export/interpolated.csv',\n",
    "                                root_dir='/export/jupyterlab/data/udacity-challenge-2/Ch2_002_export/',\n",
    "                                transform=transforms.Compose([transforms.ToTensor()]),\n",
    "                                select_camera='center_camera',\n",
    "                                select_range=(split_point, dataset_size))\n",
    "\n",
    "print(\"size of training set: {}\\nstat:\\nmean: {}\\nstd: {}\".format(len(training_set), training_set.mean, training_set.std))\n",
    "print(\"size of validation set: {}\\nstat:\\nmean: {}\\nstd: {}\".format(len(validation_set), validation_set.mean, validation_set.std))\n",
    "\n",
    "cbs_train = ConsecutiveBatchSampler(data_source=training_set, batch_size=10, shuffle=True, drop_last=True, seq_len=15)\n",
    "cbs_valid = ConsecutiveBatchSampler(data_source=validation_set, batch_size=10, shuffle=True, drop_last=True, seq_len=15)\n",
    "\n",
    "trainig_loader = DataLoader(training_set, sampler=cbs_train, collate_fn=(lambda x: x[0]))\n",
    "validation_loader = DataLoader(validation_set, sampler=cbs_valid, collate_fn=(lambda x: x[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda for training\n"
     ]
    }
   ],
   "source": [
    "import datetime\n",
    "import subprocess\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "from torchviz import make_dot\n",
    "\n",
    "writer = SummaryWriter('/export/jupyterlab/chengxin/runs/steernet-{}'.format(datetime.datetime.now()))\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using {} for training\".format(device))\n",
    "\n",
    "steernet = SteerNet(device=device, dropout_prob=0.75)\n",
    "steernet.preserve_states(True)\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(steernet.parameters(), lr=1e-4)\n",
    "\n",
    "def normalized_data(sample, channel, dataset_for_stat, seq_len=10):\n",
    "    result = (sample[channel][:, -seq_len:] - dataset_for_stat.mean[channel]) / dataset_for_stat.std[channel]\n",
    "    return result\n",
    "\n",
    "def do_epoch(epoch_num=0):\n",
    "    \n",
    "    # training\n",
    "    steernet.train()\n",
    "    \n",
    "    for i, sample in enumerate(trainig_loader):\n",
    "        \n",
    "        angle = sample['angle'][:, -10:]\n",
    "        torque = sample['torque'][:, -10:]\n",
    "        speed = sample['speed'][:, -10:]\n",
    "        images = sample['image'].to(device)\n",
    "#         angle = normalized_data(sample, 'angle', training_set, seq_len=10)\n",
    "#         torque = normalized_data(sample, 'torque', training_set, seq_len=10)\n",
    "#         speed = normalized_data(sample, 'speed', training_set, seq_len=10)\n",
    "        \n",
    "        target = torch.stack([angle, torque, speed])\n",
    "        target = target.permute([2, 1, 0]).to(device) # (batch_size x seq_len x target_dim)\n",
    "        angle = angle.to(device)\n",
    "        del sample, torque, speed\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        out_autoreg, out_gt = steernet(images, target)\n",
    "        out_autoreg_angle = out_autoreg[:, :, 0]\n",
    "        \n",
    "        loss_angle = criterion(angle, out_autoreg_angle)\n",
    "        loss_autoreg = criterion(out_autoreg, target)\n",
    "        loss_gt = criterion(out_gt, target)\n",
    "        \n",
    "        loss_total = loss_angle + do_epoch.loss_target_weight * (loss_autoreg + loss_gt)\n",
    "        loss_total.backward()\n",
    "        \n",
    "        writer.add_scalar('train - loss total epoch {}'.format(epoch_num), loss_total.item(), i)\n",
    "        writer.add_scalar('train - loss angle epoch {}'.format(epoch_num), loss_angle.item(), i)\n",
    "        writer.add_scalar('train - loss autoreg epoch {}'.format(epoch_num), loss_autoreg.item(), i)\n",
    "        writer.add_scalar('train - loss gt epoch {}'.format(epoch_num), loss_gt.item(), i)\n",
    "        \n",
    "#         torch.nn.utils.clip_grad_norm_(steernet.parameters(), 15.0) # gradient clamping\n",
    "        optimizer.step()\n",
    "        \n",
    "    print(\"Finished training epoch {}\".format(epoch_num))\n",
    "    \n",
    "    git_label = subprocess.check_output([\"git\", \"rev-parse\", \"--short=6\", \"HEAD\"]).strip().decode('UTF-8') # get current git label\n",
    "    torch.save(steernet.state_dict(), \"/export/jupyterlab/chengxin/models/git-{}-epoch-{}\".format(git_label, epoch_num))\n",
    "    \n",
    "    # validation\n",
    "    \n",
    "    with torch.no_grad(): # we don't require grad calcualation in the validation phase. it saves memory.\n",
    "        steernet.eval()\n",
    "\n",
    "        running_cost = 0.0\n",
    "        total_iterations = 0\n",
    "        for i, sample in enumerate(validation_loader):\n",
    "            images = sample['image'].to(device)\n",
    "            angle = sample['angle'][:, -10:]\n",
    "            torque = sample['torque'][:, -10:]\n",
    "            speed = sample['speed'][:, -10:]\n",
    "#             angle = normalized_data(sample, 'angle', training_set, seq_len=10)\n",
    "#             torque = normalized_data(sample, 'torque', training_set, seq_len=10)\n",
    "#             speed = normalized_data(sample, 'speed', training_set, seq_len=10)\n",
    "            \n",
    "            target = torch.stack([angle, torque, speed])\n",
    "            target = target.permute([2, 1, 0]).to(device) # (batch_size x seq_len x target_dim)\n",
    "            angle = angle.to(device)\n",
    "            del sample, torque, speed\n",
    "\n",
    "            out_autoreg = steernet(images)\n",
    "            out_autoreg_angle = out_autoreg[:, :, 0]\n",
    "\n",
    "            loss_angle = criterion(angle, out_autoreg_angle)\n",
    "            loss_autoreg = criterion(out_autoreg, target)\n",
    "            \n",
    "            writer.add_scalar('valid - loss angle epoch {}'.format(epoch_num), loss_angle.item(), i)\n",
    "            writer.add_scalar('valid - loss autoreg epoch {}'.format(epoch_num), loss_autoreg.item(), i)\n",
    "#             running_cost += loss_angle.item() * training_set.std['angle']**2\n",
    "            running_cost += loss_angle.item()\n",
    "            total_iterations = i+1\n",
    "        \n",
    "        running_cost = running_cost / total_iterations\n",
    "        writer.add_scalar('valid - angle (RMSE)', np.sqrt(running_cost), epoch_num)\n",
    "                  \n",
    "    print(\"Finished validating epoch {}\".format(epoch_num))\n",
    "    \n",
    "do_epoch.loss_target_weight = 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(0, 20):\n",
    "    do_epoch(epoch)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
