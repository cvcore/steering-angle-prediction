{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function, division\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from skimage import io, transform\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "# defining customized Dataset class for Udacity\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms, utils\n",
    "\n",
    "class UdacityDataset(Dataset):\n",
    "    def __init__(self, csv_file, root_dir, transform=None, select_camera=None, slice_frames=None, select_ratio=1.0, select_range=None):\n",
    "        \n",
    "        assert select_ratio >= -1.0 and select_ratio <= 1.0 # positive: select to ratio from beginning, negative: select to ration counting from the end\n",
    "        \n",
    "        camera_csv = pd.read_csv(csv_file)\n",
    "        if select_camera:\n",
    "            assert select_camera in ['left_camera', 'right_camera', 'center_camera'], \"Invalid camera: {}\".format(select_camera)\n",
    "            camera_csv = camera_csv[camera_csv['frame_id']==select_camera]\n",
    "        \n",
    "        csv_len = len(camera_csv)\n",
    "        if slice_frames:\n",
    "            csv_selected = camera_csv[0:0] # empty dataframe\n",
    "            for start_idx in range(0, csv_len, slice_frames):\n",
    "                if select_ratio > 0:\n",
    "                    end_idx = int(start_idx + slice_frames * select_ratio)\n",
    "                else:\n",
    "                    start_idx, end_idx = int(start_idx + slice_frames * (1 + select_ratio)), start_idx + slice_frames\n",
    "\n",
    "                if end_idx > csv_len:\n",
    "                    end_idx = csv_len\n",
    "                if start_idx > csv_len:\n",
    "                    start_idx = csv_len\n",
    "                csv_selected = csv_selected.append(camera_csv[start_idx:end_idx])\n",
    "            self.camera_csv = csv_selected\n",
    "        elif select_range:\n",
    "            csv_selected = camera_csv.iloc[select_range[0]: select_range[1]]\n",
    "            self.camera_csv = csv_selected\n",
    "        else:\n",
    "            self.camera_csv = camera_csv\n",
    "            \n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "        \n",
    "        # Keep track of mean and cov value in each channel\n",
    "        self.mean = {}\n",
    "        self.std = {}\n",
    "        for key in ['angle', 'torque', 'speed']:\n",
    "            self.mean[key] = np.mean(camera_csv[key])\n",
    "            self.std[key] = np.std(camera_csv[key])\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.camera_csv)\n",
    "    \n",
    "    def read_data_single(self, idx):\n",
    "        path = os.path.join(self.root_dir, self.camera_csv['filename'].iloc[idx])\n",
    "        image = io.imread(path)\n",
    "        timestamp = self.camera_csv['timestamp'].iloc[idx]\n",
    "        frame_id = self.camera_csv['frame_id'].iloc[idx]\n",
    "        angle = self.camera_csv['angle'].iloc[idx]\n",
    "        torque = self.camera_csv['torque'].iloc[idx]\n",
    "        speed = self.camera_csv['speed'].iloc[idx]\n",
    "        \n",
    "        if self.transform:\n",
    "            image_transformed = self.transform(image)\n",
    "            del image\n",
    "            image = image_transformed\n",
    "        angle_t = torch.tensor(angle)\n",
    "        torque_t = torch.tensor(torque)\n",
    "        speed_t = torch.tensor(speed)\n",
    "        del angle, torque, speed\n",
    "            \n",
    "        return image, timestamp, frame_id, angle_t, torque_t, speed_t\n",
    "    \n",
    "    def read_data(self, idx):\n",
    "        if isinstance(idx, list):\n",
    "            data = None\n",
    "            for i in idx:\n",
    "                new_data = self.read_data(i)\n",
    "                if data is None:\n",
    "                    data = [[] for _ in range(len(new_data))]\n",
    "                for i, d in enumerate(new_data):\n",
    "                    data[i].append(new_data[i])\n",
    "                del new_data\n",
    "                \n",
    "            for stack_idx in [0, 3, 4, 5]: # we don't stack timestamp and frame_id since those are string data\n",
    "                data[stack_idx] = torch.stack(data[stack_idx])\n",
    "            \n",
    "            return data\n",
    "        \n",
    "        else:\n",
    "            return self.read_data_single(idx)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "        \n",
    "        data = self.read_data(idx)\n",
    "        \n",
    "        sample = {'image': data[0],\n",
    "                  'timestamp': data[1],\n",
    "                  'frame_id': data[2],\n",
    "                  'angle': data[3],\n",
    "                  'torque': data[4],\n",
    "                  'speed': data[5]}\n",
    "        \n",
    "        del data\n",
    "        \n",
    "        return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate Batch with consecutive frames taken from input data\n",
    "\n",
    "from torch.utils.data import Sampler\n",
    "import random\n",
    "\n",
    "class ConsecutiveBatchSampler(Sampler):\n",
    "    \n",
    "    def __init__(self, data_source, batch_size, seq_len, drop_last=False, shuffle=True, use_all_frames=False):\n",
    "        r\"\"\" Sampler to generate consecutive Batches\n",
    "        \n",
    "        Args:\n",
    "            data_source: Source of data\n",
    "            batch_size: Size of batch\n",
    "            seq_len: Number of frames in each sequence (used for context for prediction)\n",
    "            drop: Wether to drop the last incomplete batch\n",
    "            shuffle: Wether to shuffle the data\n",
    "        Return:\n",
    "            List of iterators, size: [batch_size x seq_len x n_channels x height x width]\n",
    "        \"\"\"\n",
    "        super(ConsecutiveBatchSampler, self).__init__(data_source)\n",
    "        \n",
    "        self.data_source = data_source\n",
    "        \n",
    "        assert seq_len >= 1, \"Invalid batch size: {}\".format(seq_len)\n",
    "        self.seq_len = seq_len\n",
    "        self.drop_last = drop_last\n",
    "        self.shuffle = shuffle\n",
    "        self.batch_size = batch_size\n",
    "        self.use_all_frames_ = use_all_frames\n",
    "    \n",
    "    def __iter__(self):\n",
    "        \n",
    "        data_size = len(self.data_source)\n",
    "        \n",
    "        if self.use_all_frames_:\n",
    "            start_indices = list(range(data_size))\n",
    "        else:\n",
    "            start_indices = list(range(1, data_size, self.seq_len))\n",
    "            \n",
    "        if self.shuffle:\n",
    "            random.shuffle(start_indices)\n",
    "        \n",
    "        batch = []\n",
    "        for idx, ind in enumerate(start_indices):\n",
    "            if data_size - idx < self.batch_size and self.drop_last: # if last batch\n",
    "                break\n",
    "                \n",
    "            seq = []\n",
    "            if ind + 1 < self.seq_len:\n",
    "                seq.extend([0]*(self.seq_len - ind - 1) + list(range(0, ind+1)))\n",
    "            else:\n",
    "                seq.extend(list(range(ind-self.seq_len+1, ind+1)))\n",
    "            \n",
    "            batch.append(seq)\n",
    "            \n",
    "            if len(batch) == self.batch_size or idx == data_size - 1:\n",
    "                yield batch\n",
    "                batch = []\n",
    "\n",
    "    \n",
    "    def __len__(self):\n",
    "        length = len(self.data_source)\n",
    "        batch_size = self.batch_size\n",
    "        \n",
    "        if length % batch_size == 0 or self.drop_last:\n",
    "            return length // batch_size\n",
    "        \n",
    "        return length // batch_size + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_sample(sample):\n",
    "    r\"\"\" Helper function for (batch) sample visualization\n",
    "    \n",
    "    Args:\n",
    "        sample: Dictionary\n",
    "    \"\"\"\n",
    "    image_dims = len(sample['image'].shape)\n",
    "    assert image_dims <= 5, \"Unsupported image shape: {}\".format(sample['image'].shape)\n",
    "    if image_dims == 3:\n",
    "        plt.imshow(sample['image'])\n",
    "    else:\n",
    "        n0 = sample['image'].shape[0]\n",
    "        n1 = sample['image'].shape[1] if image_dims == 5 else 1\n",
    "        images_flattened = torch.flatten(sample['image'], end_dim=-4)\n",
    "        fig, ax = plt.subplots(n0, n1, figsize=(25, 15))\n",
    "        for i1 in range(n1):\n",
    "            for i0 in range(n0):\n",
    "                image = images_flattened[i0 * n1 + i1]\n",
    "                axis = ax[i0, i1]\n",
    "                axis.imshow(image.permute(1,2,0))\n",
    "                axis.axis('off')\n",
    "                axis.set_title(\"t={}\".format(sample['timestamp'][i0][i1]))\n",
    "                axis.text(10, 30, sample['frame_id'][i0][i1], color='red')\n",
    "            \n",
    "    \n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "for i_batch, sample_batched in enumerate(dataloader):\n",
    "    \n",
    "    # Your code here...\n",
    "    \n",
    "    del sample_batched # release image to save memory space\n",
    "    \n",
    "    if i_batch == 2: # test loading 10 datapoints\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3D CNN with residual connection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the model\n",
    "\n",
    "import torch.nn as nn\n",
    "\n",
    "# helper function to determine dimension after convolution\n",
    "def conv_output_shape(in_dimension, kernel_size, stride):\n",
    "    output_dim = []\n",
    "    for (in_dim, kern_size, strd) in zip(in_dimension, kernel_size, stride):\n",
    "        len = int(float(in_dim - kern_size) / strd + 1.)\n",
    "        output_dim.append(len)\n",
    "    \n",
    "    return output_dim\n",
    "\n",
    "        \n",
    "class TemporalCNN(nn.Module):\n",
    "    \n",
    "    def _conv_unit(self, in_channels, out_channels, in_shape, kernel_size, stride, dropout_prob):\n",
    "        r\"\"\" Return one 3D convolution unit\n",
    "        \n",
    "        Args:\n",
    "            in_channels: Input channels of the Conv3D module\n",
    "            out_channels: Output channels of the Conv3D module\n",
    "            in_shape: Shape of the input image. i.e. The last 3 dimensions of the input tensor: D x H x W\n",
    "            kernel_size: Kernel size\n",
    "            stride: Stride\n",
    "            dropout_prob: Probability of dropout layer\n",
    "                         \n",
    "        Output:\n",
    "            (conv_module, aux_module, out_shape)\n",
    "        \"\"\"\n",
    "        \n",
    "        conv = nn.Conv3d(in_channels=in_channels, out_channels=out_channels, kernel_size=kernel_size, stride=stride)\n",
    "        dropout = nn.Dropout3d(p=dropout_prob)\n",
    "        conv_module = nn.Sequential(conv, dropout).to(self.device_)\n",
    "        out_shape = conv_output_shape(in_shape, kernel_size, stride)\n",
    "        \n",
    "        flatten = nn.Flatten(start_dim=2)\n",
    "        aux = nn.Linear(in_features=np.prod(out_shape[-2:])*out_channels, out_features=128)\n",
    "        aux_module = nn.Sequential(flatten, aux).to(self.device_)\n",
    "        \n",
    "        return conv_module, aux_module, out_shape\n",
    "    \n",
    "    def _linear_unit(self, in_features, out_features, dropout_prob):\n",
    "        linear = nn.Linear(in_features, out_features)\n",
    "        relu = nn.ReLU()\n",
    "        dropout = nn.Dropout(p=dropout_prob)\n",
    "        return nn.Sequential(linear, relu, dropout).to(self.device_), out_features\n",
    "        \n",
    "    \n",
    "    def __init__(self, in_height, in_width, seq_len, dropout_prob=0.5, aux_history=10, device=None):\n",
    "        r\"\"\" TemporalCNN: this model does 3D convolution on the H, W and temporal dimension\n",
    "             It also includes residual connection from each Conv3D output to the final output\n",
    "             \n",
    "             Args:\n",
    "                 in_height: image height\n",
    "                 in_width: image width\n",
    "                 seq_len: image sequence length\n",
    "                 dropout_prob: prob for the dropout layer\n",
    "                 aux_history: length of history to extract from seq_len\n",
    "             \n",
    "             Output:\n",
    "                 nn.Module, accepts input with shape [batch_len, seq_len, C, in_height, in_width]\n",
    "        \"\"\"\n",
    "        super(TemporalCNN, self).__init__()\n",
    "        \n",
    "        self.seq_len_ = seq_len\n",
    "        self.in_width_ = in_width\n",
    "        self.in_height_ = in_height\n",
    "        self.aux_history_ = aux_history\n",
    "        in_shape = (seq_len, in_height, in_width)\n",
    "        \n",
    "        self.device_ = device if device is not None else torch.device(\"cpu\")\n",
    "        \n",
    "        # conv1\n",
    "        self.conv0_, self.aux0_, out_shape = self._conv_unit(3, 64, in_shape, (3, 12, 12), (1, 6, 6), dropout_prob)\n",
    "        # conv2\n",
    "        self.conv1_, self.aux1_, out_shape = self._conv_unit(64, 64, out_shape, (2, 5, 5), (1, 2, 2), dropout_prob)\n",
    "        # conv3\n",
    "        self.conv2_, self.aux2_, out_shape = self._conv_unit(64, 64, out_shape, (2, 5, 5), (1, 1, 1), dropout_prob)\n",
    "        # conv4\n",
    "        self.conv3_, self.aux3_, out_shape = self._conv_unit(64, 64, out_shape, (2, 5, 5), (1, 1, 1), dropout_prob)\n",
    "        \n",
    "        # Flatten the last 3 dims\n",
    "        self.flatten_ = nn.Flatten(start_dim=2).to(self.device_)\n",
    "        \n",
    "        # FC 1024\n",
    "        self.linear0_, out_features = self._linear_unit(64*np.prod(out_shape[-2:]), 1024, dropout_prob)\n",
    "        # FC 512\n",
    "        self.linear1_, out_features = self._linear_unit(out_features, 512, dropout_prob)\n",
    "        # FC 256\n",
    "        self.linear2_, out_features = self._linear_unit(out_features, 256, dropout_prob)\n",
    "        # FC 128\n",
    "        self.linear3_ = nn.Linear(out_features, 128).to(device)\n",
    "        \n",
    "        self.elu_ = nn.ELU().to(self.device_)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = x.permute([0, 2, 1, 3, 4]) # swap channel and seq_len, 3D conv over seq_len as depth channel\n",
    "                                       # now: [batch_size, channel, seq_len, H, W]\n",
    "        \n",
    "        x = self.conv0_(x) # every conv includes dropout\n",
    "        x_permute = x.permute([0, 2, 1, 3, 4])[:,-self.aux_history_:,:,:,:]\n",
    "        x_aux0 = self.aux0_(x_permute)\n",
    "        x = self.conv1_(x)\n",
    "        x_permute = x.permute([0, 2, 1, 3, 4])[:,-self.aux_history_:,:,:,:]\n",
    "        x_aux1 = self.aux1_(x_permute)\n",
    "        x = self.conv2_(x)\n",
    "        x_permute = x.permute([0, 2, 1, 3, 4])[:,-self.aux_history_:,:,:,:]\n",
    "        x_aux2 = self.aux2_(x_permute)\n",
    "        x = self.conv3_(x)\n",
    "        x = x.permute([0, 2, 1, 3, 4])\n",
    "        x_permute = x[:,-self.aux_history_:,:,:,:]\n",
    "        x_aux3 = self.aux3_(x_permute)\n",
    "\n",
    "        x = self.flatten_(x)\n",
    "        x = self.linear0_(x)\n",
    "        x = self.linear1_(x)\n",
    "        x = self.linear2_(x)\n",
    "        x = self.linear3_(x)\n",
    "        \n",
    "        final_out = x + x_aux0 + x_aux1 + x_aux2 + x_aux3\n",
    "        final_out = self.elu_(final_out)\n",
    "        \n",
    "        return final_out\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Autoregressive LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Autoregressive LSTM Module\n",
    "\n",
    "class AutoregressiveLSTMCell(nn.Module):\n",
    "    \n",
    "    def __init__(self, target_size, visual_feature_size, hidden_size, device=None):\n",
    "        r\"\"\" AutoregressiveModule takes visual feature from 3D CNN module, pass it\n",
    "             first into an internal LSTM cell and then into a Linear network. The final\n",
    "             output of this module is of dimension output_size.\n",
    "             \n",
    "             Args:\n",
    "                 target_dim: dimsion of target value. for this application 3 (angle, speed, torque)\n",
    "                 visual_feature_dim:\n",
    "                 output_size: output size after the Linear network\n",
    "                 autoregressive_mode: wether this module work as autoregressive mode or\n",
    "                     just pass the ground truth to output\n",
    "             Output:\n",
    "                 nn.Module\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        \n",
    "        self.target_size_ = target_size\n",
    "        self.visual_feature_size_ = visual_feature_size\n",
    "        self.hidden_size_ = hidden_size\n",
    "        \n",
    "        self.device_ = device if device is not None else torch.device(\"cpu\")\n",
    "        \n",
    "        self.lstm_cell_ = nn.LSTMCell(input_size=target_size+visual_feature_size, hidden_size=hidden_size).to(self.device_)\n",
    "        self.linear_ = nn.Linear(in_features=hidden_size+visual_feature_size+target_size, out_features=target_size).to(self.device_)\n",
    "    \n",
    "    def forward(self, visual_features, prev_target, prev_states):\n",
    "        r\"\"\"\n",
    "            Output:\n",
    "                (output, target_ground_truth) for autoregressive_mode = True\n",
    "                (output, (output, ))\n",
    "        \"\"\"\n",
    "        lstm_input = torch.cat((visual_features, prev_target), dim=-1)\n",
    "        h_t, c_t = self.lstm_cell_(lstm_input, prev_states)\n",
    "        linear_input = torch.cat((visual_features, prev_target, h_t), dim=-1)\n",
    "        output = self.linear_(linear_input)\n",
    "        new_state = (h_t, c_t)\n",
    "        \n",
    "        return output, new_state\n",
    "        \n",
    "class AutoregressiveLSTM(AutoregressiveLSTMCell):\n",
    "    \n",
    "    def __init__(self, target_size, visual_feature_size, hidden_size, autoregressive_mode=True, device=None):\n",
    "        super().__init__(target_size, visual_feature_size, hidden_size, device)\n",
    "        self.autoregressive_mode_ = autoregressive_mode\n",
    "    \n",
    "    def forward(self, visual_features, init_target=None, init_states=None, target_groundtruth=None):\n",
    "        # different from LSTM in torch library, we use the second dimension for sequence!\n",
    "        assert self.autoregressive_mode_ or target_groundtruth is not None\n",
    "        \n",
    "        seq_len = visual_features.shape[1]\n",
    "        batch_len = visual_features.shape[0]\n",
    "\n",
    "        prev_target = torch.zeros(batch_len, self.target_size_, device=self.device_) if init_target is None else init_target\n",
    "        prev_states = (torch.zeros(batch_len, self.hidden_size_, device=self.device_), \n",
    "                       torch.zeros(batch_len, self.hidden_size_, device=self.device_)) if init_states is None else init_states\n",
    "        \n",
    "        outputs = []\n",
    "        states = []\n",
    "        for seq_idx in range(seq_len):\n",
    "            target, state = super().forward(visual_features[:, seq_idx, :], prev_target, prev_states)\n",
    "            prev_target = target if self.autoregressive_mode_ else target_groundtruth[:, seq_idx, :]\n",
    "            outputs.append(target)\n",
    "            states.append(torch.stack(state))\n",
    "            \n",
    "        outputs = torch.stack(outputs)\n",
    "        outputs = outputs.permute(1, 0, 2)  # dim: [batch, seq, target_size]\n",
    "        states = torch.stack(states)\n",
    "        states = states.permute(1, 2, 0, 3) # dim: [ [batch, seq, internal_size], [batch, seq, internal_size] ]\n",
    "        \n",
    "        return outputs, states\n",
    "    \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SteerNet(nn.Module):\n",
    "    \n",
    "    def __init__(self, device=None, dropout_prob=0.25):\n",
    "        super(SteerNet, self).__init__()\n",
    "        \n",
    "        self.device_ = device if device is not None else torch.device(\"cpu\")\n",
    "        \n",
    "        self.model_cnn_ = TemporalCNN(in_height=480, in_width=640, seq_len=15, dropout_prob=dropout_prob, aux_history=10, device=device)\n",
    "        self.feature_dropout_ = nn.Dropout(p=dropout_prob)\n",
    "        self.model_lstm_gt_ = AutoregressiveLSTM(target_size=3, visual_feature_size=128, hidden_size=32, autoregressive_mode=False, device=device)\n",
    "        self.model_lstm_autoreg_ = AutoregressiveLSTM(target_size=3, visual_feature_size=128, hidden_size=32, autoregressive_mode=True, device=device)\n",
    "        self.lstm_state_ = None\n",
    "        \n",
    "        self.train_ = True\n",
    "    \n",
    "    def forward(self, images, target=None):\n",
    "        assert target is None or self.train_==True\n",
    "        \n",
    "        features = self.model_cnn_(images)\n",
    "        features = self.feature_dropout_(features)\n",
    "        \n",
    "        if self.lstm_state_ is None:\n",
    "            if self.train_:\n",
    "                out_gt, state_gt = self.model_lstm_gt_(features, target_groundtruth=target)\n",
    "            out_autoreg, state_autoreg = self.model_lstm_autoreg_(features)\n",
    "        else:\n",
    "            state_autoreg = self.lstm_state_\n",
    "            state_gt = (state_autoreg[0].clone().detach(),\n",
    "                        state_autoreg[1].clone().detach()) # copies the state. we don't opzimize the state of this LSTM with groundtruth input!\n",
    "            if self.train_:\n",
    "                out_gt, _ = self.model_lstm_gt_(features, init_states=state_gt, target_groundtruth=target)\n",
    "            out_autoreg, state_autoreg = self.model_lstm_autoreg_(features, init_states=state_autoreg)\n",
    "        if self.train_:\n",
    "            self.lstm_state_ = (state_autoreg[0][-1].detach(), state_autoreg[1][-1].detach())\n",
    "        \n",
    "        return (out_autoreg, out_gt) if self.train_ else out_autoreg\n",
    "    \n",
    "    def train(self, mode=True):\n",
    "        self.train_ = mode"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "size of training set: 33808\n",
      "stat:\n",
      "mean: {'angle': -0.008478613582381778, 'torque': -0.09063468015274655, 'speed': 15.625127605181328}\n",
      "std: {'angle': 0.27152468334113167, 'torque': 0.7874424330514188, 'speed': 5.697122502690882}\n",
      "tensor(-0.6398) tensor(0.4843)\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import BatchSampler, SequentialSampler, RandomSampler\n",
    "# train - validation split\n",
    "\n",
    "# udacity_dataset = UdacityDataset(csv_file='/export/jupyterlab/data/udacity-challenge-2/Ch2_002_export/interpolated.csv',\n",
    "#                               root_dir='/export/jupyterlab/data/udacity-challenge-2/Ch2_002_export/',\n",
    "#                               transform=transforms.Compose([transforms.ToTensor()]),\n",
    "#                               select_camera='center_camera')\n",
    "\n",
    "# dataset_size = int(len(udacity_dataset) * 0.10)\n",
    "# split_point = int(dataset_size * 0.8)\n",
    "# training_set = UdacityDataset(csv_file='/export/jupyterlab/data/udacity-challenge-2/Ch2_002_export/interpolated.csv',\n",
    "#                               root_dir='/export/jupyterlab/data/udacity-challenge-2/Ch2_002_export/',\n",
    "#                               transform=transforms.Compose([transforms.ToTensor()]),\n",
    "#                               select_camera='center_camera',\n",
    "#                               select_range=(0, split_point))\n",
    "# validation_set = UdacityDataset(csv_file='/export/jupyterlab/data/udacity-challenge-2/Ch2_002_export/interpolated.csv',\n",
    "#                                 root_dir='/export/jupyterlab/data/udacity-challenge-2/Ch2_002_export/',\n",
    "#                                 transform=transforms.Compose([transforms.ToTensor()]),\n",
    "#                                 select_camera='center_camera',\n",
    "#                                 select_range=(split_point, dataset_size))\n",
    "\n",
    "# Use the whole training dataset to calculate target mean and stddev\n",
    "udacity_dataset = UdacityDataset(csv_file='/export/jupyterlab/data/udacity-challenge-2/Ch2_002_export/interpolated.csv',\n",
    "                              root_dir='/export/jupyterlab/data/udacity-challenge-2/Ch2_002_export/',\n",
    "                              transform=transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))]),\n",
    "                              select_camera='center_camera')\n",
    "print(\"size of training set: {}\\nstat:\\nmean: {}\\nstd: {}\".format(len(udacity_dataset), udacity_dataset.mean, udacity_dataset.std))\n",
    "\n",
    "print(torch.mean(udacity_dataset[900]['image'][0, :, :]), torch.std(udacity_dataset[900]['image'][0, :, :]))\n",
    "\n",
    "# Train / valid split with slices\n",
    "training_set = UdacityDataset(csv_file='/export/jupyterlab/data/udacity-challenge-2/Ch2_002_export/interpolated.csv',\n",
    "                              root_dir='/export/jupyterlab/data/udacity-challenge-2/Ch2_002_export/',\n",
    "                              transform=transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))]),\n",
    "                              select_camera='center_camera',\n",
    "                              slice_frames=100,\n",
    "                              select_ratio=0.8)\n",
    "validation_set = UdacityDataset(csv_file='/export/jupyterlab/data/udacity-challenge-2/Ch2_002_export/interpolated.csv',\n",
    "                                root_dir='/export/jupyterlab/data/udacity-challenge-2/Ch2_002_export/',\n",
    "                                transform=transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))]),\n",
    "                                select_camera='center_camera',\n",
    "                                slice_frames=100,\n",
    "                                select_ratio=-0.2)\n",
    "batch_size = 10\n",
    "\n",
    "cbs_train = ConsecutiveBatchSampler(data_source=training_set, batch_size=batch_size, shuffle=False, drop_last=True, seq_len=15, use_all_frames=False)\n",
    "cbs_valid = ConsecutiveBatchSampler(data_source=validation_set, batch_size=batch_size, shuffle=False, drop_last=True, seq_len=15, use_all_frames=False)\n",
    "\n",
    "training_loader = DataLoader(training_set, sampler=cbs_train, collate_fn=(lambda x: x[0]))\n",
    "validation_loader = DataLoader(validation_set, sampler=cbs_valid, collate_fn=(lambda x: x[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda for training\n"
     ]
    }
   ],
   "source": [
    "import datetime\n",
    "import subprocess\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "writer = SummaryWriter('/export/jupyterlab/chengxin/runs/steernet-{}'.format(datetime.datetime.now()))\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using {} for training\".format(device))\n",
    "\n",
    "steernet = SteerNet(device=device, dropout_prob=0.25)\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(steernet.parameters())\n",
    "\n",
    "def normalize_data(sample, channel, origin_dataset):\n",
    "    result = (sample[channel] - origin_dataset.mean[channel]) / origin_dataset.std[channel]\n",
    "    return result\n",
    "\n",
    "def do_epoch(epoch_num=0):\n",
    "    \n",
    "    # training\n",
    "    steernet.train()\n",
    "    \n",
    "    running_cost = 0.0\n",
    "    total_iterations = 0\n",
    "    for i, sample in enumerate(tqdm(training_loader, total=len(training_set)/batch_size/15)):\n",
    "        \n",
    "        angle = sample['angle'][:, -10:]\n",
    "        torque = sample['torque'][:, -10:]\n",
    "        speed = sample['speed'][:, -10:]\n",
    "        images = sample['image'].to(device)\n",
    "        angle_normalized = normalize_data(sample, 'angle', udacity_dataset)[:, -10:]\n",
    "        torque_normalized = normalize_data(sample, 'torque', udacity_dataset)[:, -10:]\n",
    "        speed_normalized = normalize_data(sample, 'speed', udacity_dataset)[:, -10:]\n",
    "        \n",
    "        target_normalized = torch.stack([angle_normalized, torque_normalized, speed_normalized])\n",
    "        target_normalized = target_normalized.permute([2, 1, 0]).to(device) # (batch_size x seq_len x target_dim)\n",
    "        angle = angle.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        out_autoreg, out_gt = steernet(images, target_normalized)\n",
    "        out_autoreg_angle = out_autoreg[:, :, 0] * udacity_dataset.std['angle'] + udacity_dataset.mean['angle']\n",
    "        \n",
    "        loss_angle = criterion(angle, out_autoreg_angle)\n",
    "        loss_autoreg = criterion(out_autoreg, target_normalized)\n",
    "        loss_gt = criterion(out_gt, target_normalized)\n",
    "        \n",
    "        loss_total = loss_angle + do_epoch.loss_target_weight * (loss_autoreg + loss_gt)\n",
    "        running_cost += loss_angle.item()\n",
    "        total_iterations = i+1\n",
    "        loss_total.backward()\n",
    "        \n",
    "        torch.nn.utils.clip_grad_norm_(steernet.model_lstm_gt_.parameters(), 15.0) # gradient clamping\n",
    "        torch.nn.utils.clip_grad_norm_(steernet.model_lstm_autoreg_.parameters(), 15.0) # gradient clamping\n",
    "        \n",
    "        writer.add_scalar('train - loss total epoch {}'.format(epoch_num), loss_total.item(), i)\n",
    "        writer.add_scalar('train - loss angle epoch {}'.format(epoch_num), loss_angle.item(), i)\n",
    "        writer.add_scalar('train - loss autoreg epoch {}'.format(epoch_num), loss_autoreg.item(), i)\n",
    "        writer.add_scalar('train - loss gt epoch {}'.format(epoch_num), loss_gt.item(), i)\n",
    "        \n",
    "        optimizer.step()\n",
    "    \n",
    "    running_cost = running_cost / total_iterations\n",
    "    writer.add_scalar('train - angle (RMSE)', np.sqrt(running_cost), epoch_num)\n",
    "    print(\"Finished training epoch {}\".format(epoch_num))\n",
    "    \n",
    "    git_label = subprocess.check_output([\"git\", \"rev-parse\", \"--short=6\", \"HEAD\"]).strip().decode('UTF-8') # get current git label\n",
    "    torch.save(steernet.state_dict(), \"/export/jupyterlab/chengxin/models/git-{}-epoch-{}\".format(git_label, epoch_num))\n",
    "    \n",
    "    # validation\n",
    "    \n",
    "    with torch.no_grad(): # we don't require grad calcualation in the validation phase. it saves memory.\n",
    "        steernet.eval()\n",
    "\n",
    "        running_cost = 0.0\n",
    "        total_iterations = 0\n",
    "        for i, sample in enumerate(tqdm(validation_loader, total=len(validation_set)/batch_size/15)):\n",
    "            angle = sample['angle'][:, -10:]\n",
    "            torque = sample['torque'][:, -10:]\n",
    "            speed = sample['speed'][:, -10:]\n",
    "            images = sample['image'].to(device)\n",
    "            angle_normalized = normalize_data(sample, 'angle', udacity_dataset)[:, -10:]\n",
    "            torque_normalized = normalize_data(sample, 'torque', udacity_dataset)[:, -10:]\n",
    "            speed_normalized = normalize_data(sample, 'speed', udacity_dataset)[:, -10:]\n",
    "            \n",
    "            target_normalized = torch.stack([angle_normalized, torque_normalized, speed_normalized])\n",
    "            target_normalized = target_normalized.permute([2, 1, 0]).to(device) # (batch_size x seq_len x target_dim)\n",
    "            angle = angle.to(device)\n",
    "\n",
    "            out_autoreg = steernet(images)\n",
    "            out_autoreg_angle = out_autoreg[:, :, 0] * udacity_dataset.std['angle'] + udacity_dataset.mean['angle']\n",
    "\n",
    "            loss_angle = criterion(angle, out_autoreg_angle)\n",
    "            loss_autoreg = criterion(out_autoreg, target_normalized)\n",
    "            \n",
    "            writer.add_scalar('valid - loss angle epoch {}'.format(epoch_num), loss_angle.item(), i)\n",
    "            writer.add_scalar('valid - loss autoreg epoch {}'.format(epoch_num), loss_autoreg.item(), i)\n",
    "            running_cost += loss_angle.item()\n",
    "            total_iterations = i+1\n",
    "        \n",
    "        running_cost = running_cost / total_iterations\n",
    "        writer.add_scalar('valid - angle (RMSE)', np.sqrt(running_cost), epoch_num)\n",
    "                  \n",
    "    print(\"Finished validating epoch {}\".format(epoch_num))\n",
    "    \n",
    "do_epoch.loss_target_weight = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d7d434daaadf478cadf9a9248245a391",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=180), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Finished training epoch 0\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5583fd6ca78f44dea15b008e672747fc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=45), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Finished validating epoch 0\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(1):\n",
    "    do_epoch(epoch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualization of CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def visualize_cnn(cnn, max_row=10, max_col=10, select_channel=None, tb_writer=None):\n",
    "    assert isinstance(cnn, torch.nn.modules.conv.Conv3d)\n",
    "    \n",
    "    filters = cnn.weight.cpu().detach().numpy() # output_ch x input_ch x D x H x W\n",
    "    output_ch = np.minimum(cnn.weight.shape[0], max_col)\n",
    "    input_ch = np.minimum(cnn.weight.shape[1], max_row)\n",
    "    print(filters.shape)\n",
    "    \n",
    "    plt_idx = 0\n",
    "    fig = plt.figure(figsize=(output_ch, input_ch))\n",
    "    for o in range(output_ch):\n",
    "        for i in range(input_ch):\n",
    "            if select_channel:\n",
    "                assert isinstance(select_channel, int)\n",
    "                image = filters[o, i, select_channel, :, :]\n",
    "            else:\n",
    "                image = np.mean(filters[o, i, :, :, :], axis=0)\n",
    "            plt_idx += 1\n",
    "            ax = fig.add_subplot(input_ch, output_ch, plt_idx)\n",
    "            ax.imshow(image)\n",
    "            ax.axis('off')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(64, 3, 3, 12, 12)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x216 with 30 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "visualize_cnn(steernet.model_cnn_.conv0_[0], select_channel=0)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {
     "0aaaa19491a64b21b971f650d27e285f": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "0e550798ebc74716905d63a730f95abc": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "description_width": "initial"
      }
     },
     "2772acd581ff477e9622263a62eefd32": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "2d8cf1e562a0420cbc768fe13bb22bbc": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "layout": "IPY_MODEL_2772acd581ff477e9622263a62eefd32",
       "style": "IPY_MODEL_a05dedc8262e4577bb7245a89387bdec",
       "value": " 45/45.06666666666667 [00:51&lt;00:00,  1.14s/it]"
      }
     },
     "360857bd15614bbeb1ffa9471cf629ba": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "36d7e8ac70794d408798ef9f1d270fc1": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "38982f2fe961415c991c784424b8b0cf": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "description_width": ""
      }
     },
     "451e7d25390d440b8fe1287b8d37a03f": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "IntProgressModel",
      "state": {
       "bar_style": "danger",
       "description": "100%",
       "layout": "IPY_MODEL_52f0e635e12a463ab7083f82d3b21224",
       "max": 180,
       "style": "IPY_MODEL_46cc70eb858443a3ad904bdc0590f19e",
       "value": 180
      }
     },
     "46cc70eb858443a3ad904bdc0590f19e": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "description_width": "initial"
      }
     },
     "496badc5171d4451aa48bd342a342041": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "4989c3a787044199bc94560984bd92fd": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "4bf9e5403d374e6ab0bad7618ccf9772": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "IntProgressModel",
      "state": {
       "bar_style": "danger",
       "description": "100%",
       "layout": "IPY_MODEL_8c1a36274f6047fb862b7ec901c3b94b",
       "max": 180,
       "style": "IPY_MODEL_666692cfdd2644f7ab05c93631d9a95a",
       "value": 180
      }
     },
     "51e627811d7d44639074ca96361f8a01": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "description_width": ""
      }
     },
     "52f0e635e12a463ab7083f82d3b21224": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "53f47f3cba1d49f2b941520db91bf491": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "layout": "IPY_MODEL_36d7e8ac70794d408798ef9f1d270fc1",
       "style": "IPY_MODEL_51e627811d7d44639074ca96361f8a01",
       "value": " 45/45.06666666666667 [00:50&lt;00:00,  1.12s/it]"
      }
     },
     "5583fd6ca78f44dea15b008e672747fc": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HBoxModel",
      "state": {
       "children": [
        "IPY_MODEL_a912c0caf48343158596f389f36220e2",
        "IPY_MODEL_2d8cf1e562a0420cbc768fe13bb22bbc"
       ],
       "layout": "IPY_MODEL_dc7b0eb5fc124bc9b689ebf32245dc19"
      }
     },
     "5792d9d5d61a4378bc760c031fdcea83": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HBoxModel",
      "state": {
       "children": [
        "IPY_MODEL_451e7d25390d440b8fe1287b8d37a03f",
        "IPY_MODEL_9229b419cad04905a47f2bde367d6aa5"
       ],
       "layout": "IPY_MODEL_360857bd15614bbeb1ffa9471cf629ba"
      }
     },
     "666692cfdd2644f7ab05c93631d9a95a": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "description_width": "initial"
      }
     },
     "6fdc448bd775431d92b76519c86b9e76": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "8c1a36274f6047fb862b7ec901c3b94b": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "8f2096d5fb1e4c75a773bf3791765826": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "IntProgressModel",
      "state": {
       "bar_style": "danger",
       "description": "100%",
       "layout": "IPY_MODEL_496badc5171d4451aa48bd342a342041",
       "max": 45,
       "style": "IPY_MODEL_fbddefa3013d4af58b7098ba7e765295",
       "value": 45
      }
     },
     "9229b419cad04905a47f2bde367d6aa5": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "layout": "IPY_MODEL_6fdc448bd775431d92b76519c86b9e76",
       "style": "IPY_MODEL_ae94828f57b44e8c91609c43c11aa54a",
       "value": " 180/180.32000000000002 [05:04&lt;00:00,  1.69s/it]"
      }
     },
     "a05dedc8262e4577bb7245a89387bdec": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "description_width": ""
      }
     },
     "a912c0caf48343158596f389f36220e2": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "IntProgressModel",
      "state": {
       "bar_style": "danger",
       "description": "100%",
       "layout": "IPY_MODEL_fb5d1aadbc0642b09f8c1bbf5cc53fa5",
       "max": 45,
       "style": "IPY_MODEL_0e550798ebc74716905d63a730f95abc",
       "value": 45
      }
     },
     "ae94828f57b44e8c91609c43c11aa54a": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "description_width": ""
      }
     },
     "d23bf51d03204bda97b28407bcd7389c": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HBoxModel",
      "state": {
       "children": [
        "IPY_MODEL_8f2096d5fb1e4c75a773bf3791765826",
        "IPY_MODEL_53f47f3cba1d49f2b941520db91bf491"
       ],
       "layout": "IPY_MODEL_f339cc7b22db4d81a535a0b99ca14ef8"
      }
     },
     "d7d434daaadf478cadf9a9248245a391": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HBoxModel",
      "state": {
       "children": [
        "IPY_MODEL_4bf9e5403d374e6ab0bad7618ccf9772",
        "IPY_MODEL_fef200076b9b4c07b19abaaba171cb90"
       ],
       "layout": "IPY_MODEL_0aaaa19491a64b21b971f650d27e285f"
      }
     },
     "dc7b0eb5fc124bc9b689ebf32245dc19": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "f339cc7b22db4d81a535a0b99ca14ef8": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "fb5d1aadbc0642b09f8c1bbf5cc53fa5": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "fbddefa3013d4af58b7098ba7e765295": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "description_width": "initial"
      }
     },
     "fef200076b9b4c07b19abaaba171cb90": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "layout": "IPY_MODEL_4989c3a787044199bc94560984bd92fd",
       "style": "IPY_MODEL_38982f2fe961415c991c784424b8b0cf",
       "value": " 180/180.32000000000002 [04:10&lt;00:00,  1.39s/it]"
      }
     }
    },
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
