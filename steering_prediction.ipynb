{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function, division\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from skimage import io, transform\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "# defining customized Dataset class for Udacity\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms, utils\n",
    "\n",
    "class UdacityDataset(Dataset):\n",
    "    def __init__(self, csv_file, root_dir, transform=None, select_camera=None, select_range=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            csv_file (string): Path to the csv file with annotations.\n",
    "            root_dir (string): Directory with all the images.\n",
    "            select_camera (string): 'left_ / right_ / center_camera'\n",
    "            transform (callable, optional): Optional transform to be applied\n",
    "                on a sample.\n",
    "        \"\"\"\n",
    "        \n",
    "        camera_csv = pd.read_csv(csv_file)\n",
    "        assert select_camera in ['left_camera', 'right_camera', 'center_camera'], \"Invalid camera: {}\".format(select_camera)\n",
    "        if select_camera:\n",
    "            camera_csv = camera_csv[camera_csv['frame_id']==select_camera]\n",
    "        if select_range:\n",
    "            camera_csv = camera_csv.iloc[select_range[0]: select_range[1]]\n",
    "        self.camera_csv = camera_csv\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.camera_csv)\n",
    "    \n",
    "    def read_data_single(self, idx):\n",
    "        path = os.path.join(self.root_dir, self.camera_csv['filename'].iloc[idx])\n",
    "        image = io.imread(path)\n",
    "        timestamp = self.camera_csv['timestamp'].iloc[idx]\n",
    "        frame_id = self.camera_csv['frame_id'].iloc[idx]\n",
    "        angle = self.camera_csv['angle'].iloc[idx]\n",
    "        torque = self.camera_csv['torque'].iloc[idx]\n",
    "        speed = self.camera_csv['speed'].iloc[idx]\n",
    "        \n",
    "        if self.transform:\n",
    "            image_transformed = self.transform(image)\n",
    "            del image\n",
    "            image = image_transformed\n",
    "        angle_t = torch.tensor(angle)\n",
    "        torque_t = torch.tensor(torque)\n",
    "        speed_t = torch.tensor(speed)\n",
    "        del angle, torque, speed\n",
    "            \n",
    "        return image, timestamp, frame_id, angle_t, torque_t, speed_t\n",
    "    \n",
    "    def read_data(self, idx):\n",
    "        if isinstance(idx, list):\n",
    "            data = None\n",
    "            for i in idx:\n",
    "                new_data = self.read_data(i)\n",
    "                if data is None:\n",
    "                    data = [[] for _ in range(len(new_data))]\n",
    "                for i, d in enumerate(new_data):\n",
    "                    data[i].append(new_data[i])\n",
    "                del new_data\n",
    "                \n",
    "            for stack_idx in [0, 3, 4, 5]: # we don't stack timestamp and frame_id since those are string data\n",
    "                data[stack_idx] = torch.stack(data[stack_idx])\n",
    "            \n",
    "            return data\n",
    "        \n",
    "        else:\n",
    "            return self.read_data_single(idx)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "        \n",
    "        data = self.read_data(idx)\n",
    "        \n",
    "        sample = {'image': data[0],\n",
    "                  'timestamp': data[1],\n",
    "                  'frame_id': data[2],\n",
    "                  'angle': data[3],\n",
    "                  'torque': data[4],\n",
    "                  'speed': data[5]}\n",
    "        \n",
    "        del data\n",
    "        \n",
    "        return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate Batch with consecutive frames taken from input data\n",
    "\n",
    "from torch.utils.data import Sampler\n",
    "import random\n",
    "\n",
    "class ConsecutiveBatchSampler(Sampler):\n",
    "    \n",
    "    def __init__(self, data_source, batch_size, seq_len, drop_last=False, shuffle=True):\n",
    "        r\"\"\" Sampler to generate consecutive Batches\n",
    "        \n",
    "        Args:\n",
    "            data_source: Source of data\n",
    "            batch_size: Size of batch\n",
    "            seq_len: Number of frames in each sequence (used for context for prediction)\n",
    "            drop: Wether to drop the last incomplete batch\n",
    "            shuffle: Wether to shuffle the data\n",
    "        Return:\n",
    "            List of iterators, size: [batch_size x seq_len x n_channels x height x width]\n",
    "        \"\"\"\n",
    "        super().__init__(data_source)\n",
    "        \n",
    "        self.data_source = data_source\n",
    "        \n",
    "        assert seq_len >= 1, \"Invalid batch size: {}\".format(seq_len)\n",
    "        self.seq_len = seq_len\n",
    "        self.drop_last = drop_last\n",
    "        self.shuffle = shuffle\n",
    "        self.batch_size = batch_size\n",
    "    \n",
    "    def __iter__(self):\n",
    "        data_size = len(self.data_source)\n",
    "        start_indices = list(range(data_size))\n",
    "        if self.shuffle:\n",
    "            random.shuffle(start_indices)\n",
    "        \n",
    "        batch = []\n",
    "        for idx, ind in enumerate(start_indices):\n",
    "            if data_size - idx < self.batch_size and self.drop_last: # if last batch\n",
    "                break\n",
    "                \n",
    "            seq = []\n",
    "            if ind + 1 < self.seq_len:\n",
    "                seq.extend([0]*(self.seq_len - ind - 1) + list(range(0, ind+1)))\n",
    "            else:\n",
    "                seq.extend(list(range(ind-self.seq_len+1, ind+1)))\n",
    "            \n",
    "            batch.append(seq)\n",
    "            \n",
    "            if len(batch) == self.batch_size or idx == data_size - 1:\n",
    "                yield batch\n",
    "                batch = []\n",
    "\n",
    "    \n",
    "    def __len__(self):\n",
    "        length = len(self.data_source)\n",
    "        batch_size = self.batch_size\n",
    "        \n",
    "        if length % batch_size == 0 or self.drop_last:\n",
    "            return length // batch_size\n",
    "        \n",
    "        return length // batch_size + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_sample(sample):\n",
    "    r\"\"\" Helper function for (batch) sample visualization\n",
    "    \n",
    "    Args:\n",
    "        sample: Dictionary\n",
    "    \"\"\"\n",
    "    image_dims = len(sample['image'].shape)\n",
    "    assert image_dims <= 5, \"Unsupported image shape: {}\".format(sample['image'].shape)\n",
    "    if image_dims == 3:\n",
    "        plt.imshow(sample['image'])\n",
    "    else:\n",
    "        n0 = sample['image'].shape[0]\n",
    "        n1 = sample['image'].shape[1] if image_dims == 5 else 1\n",
    "        images_flattened = torch.flatten(sample['image'], end_dim=-4)\n",
    "        fig, ax = plt.subplots(n0, n1, figsize=(25, 15))\n",
    "        for i1 in range(n1):\n",
    "            for i0 in range(n0):\n",
    "                image = images_flattened[i0 * n1 + i1]\n",
    "                axis = ax[i0, i1]\n",
    "                axis.imshow(image.permute(1,2,0))\n",
    "                axis.axis('off')\n",
    "                axis.set_title(\"t={}\".format(sample['timestamp'][i0][i1]))\n",
    "                axis.text(10, 30, sample['frame_id'][i0][i1], color='red')\n",
    "            \n",
    "    \n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "for i_batch, sample_batched in enumerate(dataloader):\n",
    "    \n",
    "    # Your code here...\n",
    "    \n",
    "    del sample_batched # release image to save memory space\n",
    "    \n",
    "    if i_batch == 2: # test loading 10 datapoints\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3D CNN with residual connection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the model\n",
    "\n",
    "import torch.nn as nn\n",
    "\n",
    "# helper function to determine dimension after convolution\n",
    "def conv_output_shape(in_dimension, kernel_size, stride):\n",
    "    output_dim = []\n",
    "    for (in_dim, kern_size, strd) in zip(in_dimension, kernel_size, stride):\n",
    "        len = int(float(in_dim - kern_size) / strd + 1.)\n",
    "        output_dim.append(len)\n",
    "    \n",
    "    return output_dim\n",
    "\n",
    "        \n",
    "class TemporalCNN(nn.Module):\n",
    "    \n",
    "    def _conv_unit(self, in_channels, out_channels, in_shape, kernel_size, stride, dropout_prob):\n",
    "        r\"\"\" Return one 3D convolution unit\n",
    "        \n",
    "        Args:\n",
    "            in_channels: Input channels of the Conv3D module\n",
    "            out_channels: Output channels of the Conv3D module\n",
    "            in_shape: Shape of the input image. i.e. The last 3 dimensions of the input tensor: D x H x W\n",
    "            kernel_size: Kernel size\n",
    "            stride: Stride\n",
    "            dropout_prob: Probability of dropout layer\n",
    "                         \n",
    "        Output:\n",
    "            (conv_module, aux_module, out_shape)\n",
    "        \"\"\"\n",
    "        \n",
    "        conv = nn.Conv3d(in_channels=in_channels, out_channels=out_channels, kernel_size=kernel_size, stride=stride)\n",
    "        dropout = nn.Dropout3d(p=dropout_prob)\n",
    "        conv_module = nn.Sequential(conv, dropout)\n",
    "        out_shape = conv_output_shape(in_shape, kernel_size, stride)\n",
    "        \n",
    "        flatten = nn.Flatten(start_dim=2)\n",
    "        aux = nn.Linear(in_features=np.prod(out_shape[-2:])*out_channels, out_features=128)\n",
    "        aux_module = nn.Sequential(flatten, aux)\n",
    "        \n",
    "        return conv_module, aux_module, out_shape\n",
    "    \n",
    "    def _linear_unit(self, in_features, out_features, dropout_prob):\n",
    "        linear = nn.Linear(in_features, out_features)\n",
    "        dropout = nn.Dropout(p=dropout_prob)\n",
    "        return nn.Sequential(linear, dropout), out_features\n",
    "        \n",
    "    \n",
    "    def __init__(self, in_height, in_width, seq_len, dropout_prob=0.5, aux_history=10):\n",
    "        r\"\"\" TemporalCNN: this model does 3D convolution on the H, W and temporal dimension\n",
    "             It also includes residual connection from each Conv3D output to the final output\n",
    "             \n",
    "             Args:\n",
    "                 in_height: image height\n",
    "                 in_width: image width\n",
    "                 seq_len: image sequence length\n",
    "                 dropout_prob: prob for the dropout layer\n",
    "                 aux_history: length of history to extract from seq_len\n",
    "             \n",
    "             Output:\n",
    "                 nn.Module, accepts input with shape [batch_len, seq_len, C, in_height, in_width]\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        \n",
    "        self.seq_len_ = seq_len\n",
    "        self.in_width_ = in_width\n",
    "        self.in_height_ = in_height\n",
    "        self.aux_history_ = aux_history\n",
    "        in_shape = (seq_len, in_height, in_width)\n",
    "        \n",
    "        conv_layers = []\n",
    "        # conv1\n",
    "        conv, aux, out_shape = self._conv_unit(3, 64, in_shape, (3, 12, 12), (1, 6, 6), dropout_prob)\n",
    "        conv_layers.append((conv, aux))\n",
    "        # conv2\n",
    "        conv, aux, out_shape = self._conv_unit(64, 64, out_shape, (2, 5, 5), (1, 2, 2), dropout_prob)\n",
    "        conv_layers.append((conv, aux))\n",
    "        # conv3\n",
    "        conv, aux, out_shape = self._conv_unit(64, 64, out_shape, (2, 5, 5), (1, 1, 1), dropout_prob)\n",
    "        conv_layers.append((conv, aux))\n",
    "        # conv4\n",
    "        conv, aux, out_shape = self._conv_unit(64, 64, out_shape, (2, 5, 5), (1, 1, 1), dropout_prob)\n",
    "        conv_layers.append((conv, aux))\n",
    "        \n",
    "        linear_layers = []\n",
    "        # Flatten the last 3 dims\n",
    "        flatten = nn.Flatten(start_dim=2)\n",
    "        linear_layers.append(flatten)\n",
    "        # FC 1024\n",
    "        linear, out_features = self._linear_unit(64*np.prod(out_shape[-2:]), 1024, dropout_prob)\n",
    "        linear_layers.append(linear)\n",
    "        # FC 512\n",
    "        linear, out_features = self._linear_unit(out_features, 512, dropout_prob)\n",
    "        linear_layers.append(linear)\n",
    "        # FC 256\n",
    "        linear, out_features = self._linear_unit(out_features, 256, dropout_prob)\n",
    "        linear_layers.append(linear)\n",
    "        # FC 128\n",
    "        linear, out_features = self._linear_unit(out_features, 128, dropout_prob)\n",
    "        linear_layers.append(linear)\n",
    "        \n",
    "        self.conv_layers_ = conv_layers\n",
    "        self.linear_layers_ = linear_layers\n",
    "        self.final_elu_ = nn.ELU()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = x.permute([0, 2, 1, 3, 4]) # swap channel and seq_len, 3D conv over seq_len as depth channel\n",
    "                                       # now: [batch_size, channel, seq_len, H, W]\n",
    "        \n",
    "        aux_outputs = []\n",
    "        for layer in self.conv_layers_:\n",
    "            x_out = layer[0](x)\n",
    "            x_out_permuted = x_out.permute([0, 2, 1, 3, 4]) # swap back for calculation of aux output\n",
    "            x_aux = layer[1](x_out_permuted[:,-self.aux_history_:,:,:,:])\n",
    "#             print(x_out.shape)\n",
    "#             print(x_aux.shape)\n",
    "            aux_outputs.append(x_aux)\n",
    "            x = x_out\n",
    "        \n",
    "        x = x.permute([0, 2, 1, 3, 4]) # swap back the dimensions, now: [batch_size, seq_len, channel, H, W]\n",
    "        for layer in self.linear_layers_:\n",
    "            x = layer(x)\n",
    "        \n",
    "        final_out = x\n",
    "        for aux_out in aux_outputs:\n",
    "            final_out = final_out + aux_out\n",
    "        final_out = self.final_elu_(final_out)\n",
    "#         print(final_out.shape)\n",
    "        \n",
    "        return final_out\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Autoregressive LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Autoregressive LSTM Module\n",
    "\n",
    "class AutoregressiveLSTMCell(nn.Module):\n",
    "    \n",
    "    def __init__(self, target_size, visual_feature_size, hidden_size):\n",
    "        r\"\"\" AutoregressiveModule takes visual feature from 3D CNN module, pass it\n",
    "             first into an internal LSTM cell and then into a Linear network. The final\n",
    "             output of this module is of dimension output_size.\n",
    "             \n",
    "             Args:\n",
    "                 target_dim: dimsion of target value. for this application 3 (angle, speed, torque)\n",
    "                 visual_feature_dim:\n",
    "                 output_size: output size after the Linear network\n",
    "                 autoregressive_mode: wether this module work as autoregressive mode or\n",
    "                     just pass the ground truth to output\n",
    "             Output:\n",
    "                 nn.Module\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        \n",
    "        self.target_size_ = target_size\n",
    "        self.visual_feature_size_ = visual_feature_size\n",
    "        self.hidden_size_ = hidden_size\n",
    "        \n",
    "        self.lstm_cell_ = nn.LSTMCell(input_size=target_size+visual_feature_size, hidden_size=hidden_size)\n",
    "        self.linear_ = nn.Linear(in_features=hidden_size+visual_feature_size+target_size, out_features=target_size)\n",
    "    \n",
    "    def forward(self, visual_features, prev_target, prev_states):\n",
    "        r\"\"\"\n",
    "            Output:\n",
    "                (output, target_ground_truth) for autoregressive_mode = True\n",
    "                (output, (output, ))\n",
    "        \"\"\"\n",
    "        lstm_input = torch.cat((visual_features, prev_target), dim=-1)\n",
    "        h_t, c_t = self.lstm_cell_(lstm_input, prev_states)\n",
    "        linear_input = torch.cat((visual_features, prev_target, h_t), dim=-1)\n",
    "        output = self.linear_(linear_input)\n",
    "        new_state = (h_t, c_t)\n",
    "        \n",
    "        return output, new_state\n",
    "        \n",
    "class AutoregressiveLSTM(AutoregressiveLSTMCell):\n",
    "    \n",
    "    def __init__(self, target_size, visual_feature_size, hidden_size, autoregressive_mode=True):\n",
    "        super().__init__(target_size, visual_feature_size, hidden_size)\n",
    "        self.autoregressive_mode_ = autoregressive_mode\n",
    "    \n",
    "    def forward(self, visual_features, init_target=None, init_states=None, target_groundtruth=None):\n",
    "        # different from LSTM in torch library, we use the second dimension for sequence!\n",
    "        assert self.autoregressive_mode_ or target_groundtruth is not None\n",
    "        \n",
    "        seq_len = visual_features.shape[1]\n",
    "        batch_len = visual_features.shape[0]\n",
    "\n",
    "        prev_target = torch.zeros(batch_len, self.target_size_) if init_target is None else init_target\n",
    "        prev_states = (torch.zeros(batch_len, self.hidden_size_), torch.zeros(batch_len, self.hidden_size_)) if init_states is None else init_states\n",
    "        \n",
    "        outputs = []\n",
    "        states = []\n",
    "        for seq_idx in range(seq_len):\n",
    "            target, state = super().forward(visual_features[:, seq_idx, :], prev_target, prev_states)\n",
    "            prev_target = target if self.autoregressive_mode_ else target_groundtruth[:, seq_idx, :]\n",
    "            outputs.append(target)\n",
    "            states.append(torch.stack(state))\n",
    "            \n",
    "        outputs = torch.stack(outputs)\n",
    "        outputs = outputs.permute(1, 0, 2)  # dim: [batch, seq, target_size]\n",
    "        states = torch.stack(states)\n",
    "        states = states.permute(1, 2, 0, 3) # dim: [ [batch, seq, internal_size], [batch, seq, internal_size] ]\n",
    "        \n",
    "        return outputs, states\n",
    "    \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SteerNet(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.model_cnn_ = TemporalCNN(in_height=480, in_width=640, seq_len=15, dropout_prob=1.0, aux_history=10)\n",
    "        self.model_lstm_gt_ = AutoregressiveLSTM(target_size=3, visual_feature_size=128, hidden_size=64, autoregressive_mode=False)\n",
    "        self.model_lstm_autoreg_ = AutoregressiveLSTM(target_size=3, visual_feature_size=128, hidden_size=64, autoregressive_mode=True)\n",
    "        self.lstm_state_ = None\n",
    "        \n",
    "        self.train = True\n",
    "    \n",
    "    def forward(self, images, target=None):\n",
    "        assert target is None or self.train==True\n",
    "        \n",
    "        features = self.model_cnn_(images)\n",
    "        \n",
    "        if self.lstm_state_ is None:\n",
    "            if self.train:\n",
    "                out_gt, state_gt = self.model_lstm_gt_(features, target_groundtruth=target)\n",
    "            out_autoreg, state_autoreg = self.model_lstm_autoreg_(features)\n",
    "        else:\n",
    "            state_autoreg = self.lstm_state_\n",
    "            state_gt = (state_autoreg[0].clone().detach(),\n",
    "                        state_autoreg[1].clone().detach()) # copies the state. we don't opzimize the state of this LSTM with groundtruth input!\n",
    "            if self.train:\n",
    "                out_gt, _ = self.model_lstm_gt_(features, init_states=state_gt, target_groundtruth=target)\n",
    "            out_autoreg, state_autoreg = self.model_lstm_autoreg_(features, init_states=state_autoreg)\n",
    "        if self.train:\n",
    "            self.lstm_state_ = (state_autoreg[0][-1].detach(), state_autoreg[1][-1].detach())\n",
    "        \n",
    "        return (out_autoreg, out_gt) if self.train else out_autoreg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initializing DataLoader\n",
    "# Warning: this only need to be done once to reduce system overhead (leaking memory)\n",
    "\n",
    "from torch.utils.data import BatchSampler, SequentialSampler, RandomSampler\n",
    "\n",
    "udacity_dataset = UdacityDataset(csv_file='/export/jupyterlab/data/udacity-challenge-2/Ch2_002_export/interpolated.csv',\n",
    "                                 root_dir='/export/jupyterlab/data/udacity-challenge-2/Ch2_002_export/',\n",
    "                                 transform=transforms.Compose([transforms.ToTensor()]),\n",
    "                                 select_camera='center_camera')\n",
    "\n",
    "cbs = ConsecutiveBatchSampler(data_source=udacity_dataset, batch_size=20, shuffle=True, drop_last=False, seq_len=15)\n",
    "dataloader = DataLoader(udacity_dataset, sampler=cbs, collate_fn=(lambda x: x[0]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "size of training set: 270\n",
      "size of validation set: 68\n"
     ]
    }
   ],
   "source": [
    "# train - validation split\n",
    "\n",
    "dataset_size = int(len(udacity_dataset) * 0.01)\n",
    "split_point = int(dataset_size * 0.8)\n",
    "training_set = UdacityDataset(csv_file='/export/jupyterlab/data/udacity-challenge-2/Ch2_002_export/interpolated.csv',\n",
    "                              root_dir='/export/jupyterlab/data/udacity-challenge-2/Ch2_002_export/',\n",
    "                              transform=transforms.Compose([transforms.ToTensor()]),\n",
    "                              select_camera='center_camera',\n",
    "                              select_range=(0, split_point))\n",
    "validation_set = UdacityDataset(csv_file='/export/jupyterlab/data/udacity-challenge-2/Ch2_002_export/interpolated.csv',\n",
    "                                root_dir='/export/jupyterlab/data/udacity-challenge-2/Ch2_002_export/',\n",
    "                                transform=transforms.Compose([transforms.ToTensor()]),\n",
    "                                select_camera='center_camera',\n",
    "                                select_range=(split_point, dataset_size))\n",
    "\n",
    "print(\"size of training set: {}\".format(len(training_set)))\n",
    "print(\"size of validation set: {}\".format(len(validation_set)))\n",
    "\n",
    "cbs_train = ConsecutiveBatchSampler(data_source=training_set, batch_size=10, shuffle=True, drop_last=True, seq_len=15)\n",
    "cbs_valid = ConsecutiveBatchSampler(data_source=validation_set, batch_size=10, shuffle=True, drop_last=True, seq_len=15)\n",
    "\n",
    "trainig_loader = DataLoader(training_set, sampler=cbs_train, collate_fn=(lambda x: x[0]))\n",
    "validation_loader = DataLoader(validation_set, sampler=cbs_valid, collate_fn=(lambda x: x[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "steernet = SteerNet()\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(steernet.parameters())\n",
    "\n",
    "def do_epoch():\n",
    "    \n",
    "    # training\n",
    "    steernet.train = True\n",
    "    \n",
    "    running_loss = 0.0\n",
    "    for i, sample in enumerate(trainig_loader):\n",
    "        images = sample['image']\n",
    "        angle = sample['angle'][:, -10:]\n",
    "        torque = sample['torque'][:, -10:]\n",
    "        speed = sample['speed'][:, -10:]\n",
    "        target = torch.stack([angle, torque, speed])\n",
    "        target = target.permute([2, 1, 0]) # (batch_size x seq_len x target_dim)\n",
    "        del sample, torque, speed\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        out_autoreg, out_gt = steernet(images, target)\n",
    "        out_autoreg_angle = out_autoreg[:, :, 0]\n",
    "        \n",
    "        loss_angle = criterion(angle, out_autoreg_angle)\n",
    "        loss_autoreg = criterion(out_autoreg, target)\n",
    "        loss_gt = criterion(out_gt, target)\n",
    "        \n",
    "        loss_total = loss_angle + do_epoch.loss_target_weight * (loss_autoreg + loss_gt)\n",
    "        loss_total.backward()\n",
    "        \n",
    "        print(\"it: {} loss_total: {} loss_angle: {} loss_autoreg: {} loss_gt: {}\".format(i, loss_total.item(), loss_angle.item(), loss_autoreg.item(), loss_gt.item()))\n",
    "        optimizer.step()\n",
    "        running_loss += loss_total.item()\n",
    "\n",
    "    print(\"Finished training.\")\n",
    "    \n",
    "    # validation\n",
    "    \n",
    "    with torch.no_grad(): # we don't require grad calcualation in the validation phase. it saves memory.\n",
    "        steernet.train = False\n",
    "\n",
    "        for i, sample in enumerate(validation_loader):\n",
    "            images = sample['image']\n",
    "            angle = sample['angle'][:, -10:]\n",
    "            torque = sample['torque'][:, -10:]\n",
    "            speed = sample['speed'][:, -10:]\n",
    "            target = torch.stack([angle, torque, speed])\n",
    "            target = target.permute([2, 1, 0]) # (batch_size x seq_len x target_dim)\n",
    "            del sample, torque, speed\n",
    "\n",
    "            out_autoreg = steernet(images)\n",
    "            out_autoreg_angle = out_autoreg[:, :, 0]\n",
    "\n",
    "            loss_angle = criterion(angle, out_autoreg_angle)\n",
    "            loss_autoreg = criterion(out_autoreg, target)\n",
    "\n",
    "            print(\"it: {} loss_angle: {} loss_autoreg: {}\".format(i, loss_angle.item(), loss_autoreg.item()))\n",
    "                  \n",
    "    print(\"Finished validation\")\n",
    "    \n",
    "do_epoch.loss_target_weight = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "it: 0 loss_total: 33.19867706298828 loss_angle: 0.008617046289145947 loss_autoreg: 174.0677947998047 loss_gt: 157.832763671875\n",
      "it: 1 loss_total: 32.897132873535156 loss_angle: 0.010810041800141335 loss_autoreg: 172.77401733398438 loss_gt: 156.0891876220703\n",
      "it: 2 loss_total: 33.01936721801758 loss_angle: 0.007988323457539082 loss_autoreg: 173.67132568359375 loss_gt: 156.4424285888672\n",
      "it: 3 loss_total: 32.9239616394043 loss_angle: 0.009369692765176296 loss_autoreg: 173.47828674316406 loss_gt: 155.66766357421875\n",
      "it: 4 loss_total: 32.86370849609375 loss_angle: 0.0054670716635882854 loss_autoreg: 173.3343963623047 loss_gt: 155.2480010986328\n",
      "it: 5 loss_total: 32.86917495727539 loss_angle: 0.005387292709201574 loss_autoreg: 173.64503479003906 loss_gt: 154.99285888671875\n",
      "it: 6 loss_total: 32.786033630371094 loss_angle: 0.0036986633203923702 loss_autoreg: 173.49441528320312 loss_gt: 154.32891845703125\n",
      "it: 7 loss_total: 33.14338302612305 loss_angle: 0.002446686616167426 loss_autoreg: 175.55491638183594 loss_gt: 155.8544464111328\n",
      "it: 8 loss_total: 33.56192398071289 loss_angle: 0.0007394293206743896 loss_autoreg: 178.03099060058594 loss_gt: 157.58082580566406\n",
      "it: 9 loss_total: 32.97202682495117 loss_angle: 0.0012166679371148348 loss_autoreg: 175.0749969482422 loss_gt: 154.6331024169922\n",
      "it: 10 loss_total: 32.025482177734375 loss_angle: 0.004411710891872644 loss_autoreg: 170.42796325683594 loss_gt: 149.78274536132812\n",
      "it: 11 loss_total: 32.69598388671875 loss_angle: 0.0021473949309438467 loss_autoreg: 174.23020935058594 loss_gt: 152.7081298828125\n",
      "it: 12 loss_total: 32.36343765258789 loss_angle: 0.002632425632327795 loss_autoreg: 172.69154357910156 loss_gt: 150.91650390625\n",
      "it: 13 loss_total: 31.5260066986084 loss_angle: 0.006822689902037382 loss_autoreg: 168.38967895507812 loss_gt: 146.80215454101562\n",
      "it: 14 loss_total: 31.845033645629883 loss_angle: 0.003541897516697645 loss_autoreg: 170.34739685058594 loss_gt: 148.06753540039062\n",
      "it: 15 loss_total: 32.14221954345703 loss_angle: 0.001451718038879335 loss_autoreg: 172.12240600585938 loss_gt: 149.28526306152344\n",
      "it: 16 loss_total: 32.21440887451172 loss_angle: 0.0028486924711614847 loss_autoreg: 172.7732696533203 loss_gt: 149.34231567382812\n",
      "it: 17 loss_total: 32.477813720703125 loss_angle: 0.0014209140790626407 loss_autoreg: 174.38552856445312 loss_gt: 150.37841796875\n",
      "it: 18 loss_total: 31.458662033081055 loss_angle: 0.0017386011313647032 loss_autoreg: 169.29612731933594 loss_gt: 145.27308654785156\n",
      "it: 19 loss_total: 32.32938766479492 loss_angle: 0.0012584355426952243 loss_autoreg: 173.8618621826172 loss_gt: 149.41941833496094\n",
      "it: 20 loss_total: 31.987422943115234 loss_angle: 0.000878328166436404 loss_autoreg: 172.3947296142578 loss_gt: 147.47071838378906\n",
      "it: 21 loss_total: 31.818683624267578 loss_angle: 0.0014591068029403687 loss_autoreg: 171.599365234375 loss_gt: 146.57286071777344\n",
      "it: 22 loss_total: 31.678754806518555 loss_angle: 0.0015507264761254191 loss_autoreg: 171.1319122314453 loss_gt: 145.64012145996094\n",
      "it: 23 loss_total: 31.45058822631836 loss_angle: 0.0022558108903467655 loss_autoreg: 170.16615295410156 loss_gt: 144.31715393066406\n",
      "it: 24 loss_total: 31.831684112548828 loss_angle: 0.0016605627024546266 loss_autoreg: 172.29275512695312 loss_gt: 146.00746154785156\n",
      "it: 25 loss_total: 31.364816665649414 loss_angle: 0.001644271775148809 loss_autoreg: 169.8328399658203 loss_gt: 143.79885864257812\n",
      "Finished training.\n",
      "it: 0 loss_angle: 0.01131243072450161 loss_autoreg: 150.99880981445312\n",
      "it: 1 loss_angle: 0.012384171597659588 loss_autoreg: 153.3126983642578\n",
      "it: 2 loss_angle: 0.012359405867755413 loss_autoreg: 152.43264770507812\n",
      "it: 3 loss_angle: 0.011398817412555218 loss_autoreg: 148.91802978515625\n",
      "it: 4 loss_angle: 0.010600886307656765 loss_autoreg: 149.7559051513672\n",
      "Finished validation\n",
      "it: 0 loss_total: 30.90532684326172 loss_angle: 0.0043992516584694386 loss_autoreg: 167.40696716308594 loss_gt: 141.60231018066406\n",
      "it: 1 loss_total: 30.389175415039062 loss_angle: 0.006616842932999134 loss_autoreg: 164.88258361816406 loss_gt: 138.94300842285156\n",
      "it: 2 loss_total: 30.70668601989746 loss_angle: 0.005205256398767233 loss_autoreg: 166.5933380126953 loss_gt: 140.4214630126953\n",
      "it: 3 loss_total: 30.807085037231445 loss_angle: 0.0023007707204669714 loss_autoreg: 167.03697204589844 loss_gt: 141.01089477539062\n"
     ]
    }
   ],
   "source": [
    "for batch in range(10):\n",
    "    do_epoch()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
