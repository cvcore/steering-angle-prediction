{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function, division\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from skimage import io, transform\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "# defining customized Dataset class for Udacity\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms, utils\n",
    "\n",
    "class UdacityDataset(Dataset):\n",
    "    def __init__(self, csv_file, root_dir, transform=None, select_camera=None, select_range=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            csv_file (string): Path to the csv file with annotations.\n",
    "            root_dir (string): Directory with all the images.\n",
    "            select_camera (string): 'left_ / right_ / center_camera'\n",
    "            transform (callable, optional): Optional transform to be applied\n",
    "                on a sample.\n",
    "        \"\"\"\n",
    "        \n",
    "        camera_csv = pd.read_csv(csv_file)\n",
    "        assert select_camera in ['left_camera', 'right_camera', 'center_camera'], \"Invalid camera: {}\".format(select_camera)\n",
    "        if select_camera:\n",
    "            camera_csv = camera_csv[camera_csv['frame_id']==select_camera]\n",
    "        if select_range:\n",
    "            camera_csv = camera_csv.iloc[select_range[0]: select_range[1]]\n",
    "        self.camera_csv = camera_csv\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.camera_csv)\n",
    "    \n",
    "    def read_data_single(self, idx):\n",
    "        path = os.path.join(self.root_dir, self.camera_csv['filename'].iloc[idx])\n",
    "        image = io.imread(path)\n",
    "        timestamp = self.camera_csv['timestamp'].iloc[idx]\n",
    "        frame_id = self.camera_csv['frame_id'].iloc[idx]\n",
    "        angle = self.camera_csv['angle'].iloc[idx]\n",
    "        torque = self.camera_csv['torque'].iloc[idx]\n",
    "        speed = self.camera_csv['speed'].iloc[idx]\n",
    "        \n",
    "        if self.transform:\n",
    "            image_transformed = self.transform(image)\n",
    "            del image\n",
    "            image = image_transformed\n",
    "        angle_t = torch.tensor(angle)\n",
    "        torque_t = torch.tensor(torque)\n",
    "        speed_t = torch.tensor(speed)\n",
    "        del angle, torque, speed\n",
    "            \n",
    "        return image, timestamp, frame_id, angle_t, torque_t, speed_t\n",
    "    \n",
    "    def read_data(self, idx):\n",
    "        if isinstance(idx, list):\n",
    "            data = None\n",
    "            for i in idx:\n",
    "                new_data = self.read_data(i)\n",
    "                if data is None:\n",
    "                    data = [[] for _ in range(len(new_data))]\n",
    "                for i, d in enumerate(new_data):\n",
    "                    data[i].append(new_data[i])\n",
    "                del new_data\n",
    "                \n",
    "            for stack_idx in [0, 3, 4, 5]: # we don't stack timestamp and frame_id since those are string data\n",
    "                data[stack_idx] = torch.stack(data[stack_idx])\n",
    "            \n",
    "            return data\n",
    "        \n",
    "        else:\n",
    "            return self.read_data_single(idx)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "        \n",
    "        data = self.read_data(idx)\n",
    "        \n",
    "        sample = {'image': data[0],\n",
    "                  'timestamp': data[1],\n",
    "                  'frame_id': data[2],\n",
    "                  'angle': data[3],\n",
    "                  'torque': data[4],\n",
    "                  'speed': data[5]}\n",
    "        \n",
    "        del data\n",
    "        \n",
    "        return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate Batch with consecutive frames taken from input data\n",
    "\n",
    "from torch.utils.data import Sampler\n",
    "import random\n",
    "\n",
    "class ConsecutiveBatchSampler(Sampler):\n",
    "    \n",
    "    def __init__(self, data_source, batch_size, seq_len, drop_last=False, shuffle=True):\n",
    "        r\"\"\" Sampler to generate consecutive Batches\n",
    "        \n",
    "        Args:\n",
    "            data_source: Source of data\n",
    "            batch_size: Size of batch\n",
    "            seq_len: Number of frames in each sequence (used for context for prediction)\n",
    "            drop: Wether to drop the last incomplete batch\n",
    "            shuffle: Wether to shuffle the data\n",
    "        Return:\n",
    "            List of iterators, size: [batch_size x seq_len x n_channels x height x width]\n",
    "        \"\"\"\n",
    "        super().__init__(data_source)\n",
    "        \n",
    "        self.data_source = data_source\n",
    "        \n",
    "        assert seq_len >= 1, \"Invalid batch size: {}\".format(seq_len)\n",
    "        self.seq_len = seq_len\n",
    "        self.drop_last = drop_last\n",
    "        self.shuffle = shuffle\n",
    "        self.batch_size = batch_size\n",
    "    \n",
    "    def __iter__(self):\n",
    "        data_size = len(self.data_source)\n",
    "        start_indices = list(range(data_size))\n",
    "        if self.shuffle:\n",
    "            random.shuffle(start_indices)\n",
    "        \n",
    "        batch = []\n",
    "        for idx, ind in enumerate(start_indices):\n",
    "            if data_size - idx < self.batch_size and self.drop_last: # if last batch\n",
    "                break\n",
    "                \n",
    "            seq = []\n",
    "            if ind + 1 < self.seq_len:\n",
    "                seq.extend([0]*(self.seq_len - ind - 1) + list(range(0, ind+1)))\n",
    "            else:\n",
    "                seq.extend(list(range(ind-self.seq_len+1, ind+1)))\n",
    "            \n",
    "            batch.append(seq)\n",
    "            \n",
    "            if len(batch) == self.batch_size or idx == data_size - 1:\n",
    "                yield batch\n",
    "                batch = []\n",
    "\n",
    "    \n",
    "    def __len__(self):\n",
    "        length = len(self.data_source)\n",
    "        batch_size = self.batch_size\n",
    "        \n",
    "        if length % batch_size == 0 or self.drop_last:\n",
    "            return length // batch_size\n",
    "        \n",
    "        return length // batch_size + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_sample(sample):\n",
    "    r\"\"\" Helper function for (batch) sample visualization\n",
    "    \n",
    "    Args:\n",
    "        sample: Dictionary\n",
    "    \"\"\"\n",
    "    image_dims = len(sample['image'].shape)\n",
    "    assert image_dims <= 5, \"Unsupported image shape: {}\".format(sample['image'].shape)\n",
    "    if image_dims == 3:\n",
    "        plt.imshow(sample['image'])\n",
    "    else:\n",
    "        n0 = sample['image'].shape[0]\n",
    "        n1 = sample['image'].shape[1] if image_dims == 5 else 1\n",
    "        images_flattened = torch.flatten(sample['image'], end_dim=-4)\n",
    "        fig, ax = plt.subplots(n0, n1, figsize=(25, 15))\n",
    "        for i1 in range(n1):\n",
    "            for i0 in range(n0):\n",
    "                image = images_flattened[i0 * n1 + i1]\n",
    "                axis = ax[i0, i1]\n",
    "                axis.imshow(image.permute(1,2,0))\n",
    "                axis.axis('off')\n",
    "                axis.set_title(\"t={}\".format(sample['timestamp'][i0][i1]))\n",
    "                axis.text(10, 30, sample['frame_id'][i0][i1], color='red')\n",
    "            \n",
    "    \n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "for i_batch, sample_batched in enumerate(dataloader):\n",
    "    \n",
    "    # Your code here...\n",
    "    \n",
    "    del sample_batched # release image to save memory space\n",
    "    \n",
    "    if i_batch == 2: # test loading 10 datapoints\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3D CNN with residual connection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the model\n",
    "\n",
    "import torch.nn as nn\n",
    "\n",
    "# helper function to determine dimension after convolution\n",
    "def conv_output_shape(in_dimension, kernel_size, stride):\n",
    "    output_dim = []\n",
    "    for (in_dim, kern_size, strd) in zip(in_dimension, kernel_size, stride):\n",
    "        len = int(float(in_dim - kern_size) / strd + 1.)\n",
    "        output_dim.append(len)\n",
    "    \n",
    "    return output_dim\n",
    "\n",
    "        \n",
    "class TemporalCNN(nn.Module):\n",
    "    \n",
    "    def _conv_unit(self, in_channels, out_channels, in_shape, kernel_size, stride, dropout_prob):\n",
    "        r\"\"\" Return one 3D convolution unit\n",
    "        \n",
    "        Args:\n",
    "            in_channels: Input channels of the Conv3D module\n",
    "            out_channels: Output channels of the Conv3D module\n",
    "            in_shape: Shape of the input image. i.e. The last 3 dimensions of the input tensor: D x H x W\n",
    "            kernel_size: Kernel size\n",
    "            stride: Stride\n",
    "            dropout_prob: Probability of dropout layer\n",
    "                         \n",
    "        Output:\n",
    "            (conv_module, aux_module, out_shape)\n",
    "        \"\"\"\n",
    "        \n",
    "        conv = nn.Conv3d(in_channels=in_channels, out_channels=out_channels, kernel_size=kernel_size, stride=stride)\n",
    "        dropout = nn.Dropout3d(p=dropout_prob)\n",
    "        conv_module = nn.Sequential(conv, dropout)\n",
    "        out_shape = conv_output_shape(in_shape, kernel_size, stride)\n",
    "        \n",
    "        flatten = nn.Flatten(start_dim=2)\n",
    "        aux = nn.Linear(in_features=np.prod(out_shape[-2:])*out_channels, out_features=128)\n",
    "        aux_module = nn.Sequential(flatten, aux)\n",
    "        \n",
    "        return conv_module, aux_module, out_shape\n",
    "    \n",
    "    def _linear_unit(self, in_features, out_features, dropout_prob):\n",
    "        linear = nn.Linear(in_features, out_features)\n",
    "        dropout = nn.Dropout(p=dropout_prob)\n",
    "        return nn.Sequential(linear, dropout), out_features\n",
    "        \n",
    "    \n",
    "    def __init__(self, in_height, in_width, seq_len, dropout_prob=0.5, aux_history=10):\n",
    "        r\"\"\" TemporalCNN: this model does 3D convolution on the H, W and temporal dimension\n",
    "             It also includes residual connection from each Conv3D output to the final output\n",
    "             \n",
    "             Args:\n",
    "                 in_height: image height\n",
    "                 in_width: image width\n",
    "                 seq_len: image sequence length\n",
    "                 dropout_prob: prob for the dropout layer\n",
    "                 aux_history: length of history to extract from seq_len\n",
    "             \n",
    "             Output:\n",
    "                 nn.Module, accepts input with shape [batch_len, seq_len, C, in_height, in_width]\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        \n",
    "        self.seq_len_ = seq_len\n",
    "        self.in_width_ = in_width\n",
    "        self.in_height_ = in_height\n",
    "        self.aux_history_ = aux_history\n",
    "        in_shape = (seq_len, in_height, in_width)\n",
    "        \n",
    "        conv_layers = []\n",
    "        # conv1\n",
    "        conv, aux, out_shape = self._conv_unit(3, 64, in_shape, (3, 12, 12), (1, 6, 6), dropout_prob)\n",
    "        conv_layers.append((conv, aux))\n",
    "        # conv2\n",
    "        conv, aux, out_shape = self._conv_unit(64, 64, out_shape, (2, 5, 5), (1, 2, 2), dropout_prob)\n",
    "        conv_layers.append((conv, aux))\n",
    "        # conv3\n",
    "        conv, aux, out_shape = self._conv_unit(64, 64, out_shape, (2, 5, 5), (1, 1, 1), dropout_prob)\n",
    "        conv_layers.append((conv, aux))\n",
    "        # conv4\n",
    "        conv, aux, out_shape = self._conv_unit(64, 64, out_shape, (2, 5, 5), (1, 1, 1), dropout_prob)\n",
    "        conv_layers.append((conv, aux))\n",
    "        \n",
    "        linear_layers = []\n",
    "        # Flatten the last 3 dims\n",
    "        flatten = nn.Flatten(start_dim=2)\n",
    "        linear_layers.append(flatten)\n",
    "        # FC 1024\n",
    "        linear, out_features = self._linear_unit(64*np.prod(out_shape[-2:]), 1024, dropout_prob)\n",
    "        linear_layers.append(linear)\n",
    "        # FC 512\n",
    "        linear, out_features = self._linear_unit(out_features, 512, dropout_prob)\n",
    "        linear_layers.append(linear)\n",
    "        # FC 256\n",
    "        linear, out_features = self._linear_unit(out_features, 256, dropout_prob)\n",
    "        linear_layers.append(linear)\n",
    "        # FC 128\n",
    "        linear, out_features = self._linear_unit(out_features, 128, dropout_prob)\n",
    "        linear_layers.append(linear)\n",
    "        \n",
    "        self.conv_layers_ = conv_layers\n",
    "        self.linear_layers_ = linear_layers\n",
    "        self.final_elu_ = nn.ELU()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = x.permute([0, 2, 1, 3, 4]) # swap channel and seq_len, 3D conv over seq_len as depth channel\n",
    "                                       # now: [batch_size, channel, seq_len, H, W]\n",
    "        \n",
    "        aux_outputs = []\n",
    "        for layer in self.conv_layers_:\n",
    "            x_out = layer[0](x)\n",
    "            x_out_permuted = x_out.permute([0, 2, 1, 3, 4]) # swap back for calculation of aux output\n",
    "            x_aux = layer[1](x_out_permuted[:,-self.aux_history_:,:,:,:])\n",
    "#             print(x_out.shape)\n",
    "#             print(x_aux.shape)\n",
    "            aux_outputs.append(x_aux)\n",
    "            x = x_out\n",
    "        \n",
    "        x = x.permute([0, 2, 1, 3, 4]) # swap back the dimensions, now: [batch_size, seq_len, channel, H, W]\n",
    "        for layer in self.linear_layers_:\n",
    "            x = layer(x)\n",
    "        \n",
    "        final_out = x\n",
    "        for aux_out in aux_outputs:\n",
    "            final_out = final_out + aux_out\n",
    "        final_out = self.final_elu_(final_out)\n",
    "#         print(final_out.shape)\n",
    "        \n",
    "        return final_out\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Autoregressive LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Autoregressive LSTM Module\n",
    "\n",
    "class AutoregressiveLSTMCell(nn.Module):\n",
    "    \n",
    "    def __init__(self, target_size, visual_feature_size, hidden_size):\n",
    "        r\"\"\" AutoregressiveModule takes visual feature from 3D CNN module, pass it\n",
    "             first into an internal LSTM cell and then into a Linear network. The final\n",
    "             output of this module is of dimension output_size.\n",
    "             \n",
    "             Args:\n",
    "                 target_dim: dimsion of target value. for this application 3 (angle, speed, torque)\n",
    "                 visual_feature_dim:\n",
    "                 output_size: output size after the Linear network\n",
    "                 autoregressive_mode: wether this module work as autoregressive mode or\n",
    "                     just pass the ground truth to output\n",
    "             Output:\n",
    "                 nn.Module\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        \n",
    "        self.target_size_ = target_size\n",
    "        self.visual_feature_size_ = visual_feature_size\n",
    "        self.hidden_size_ = hidden_size\n",
    "        \n",
    "        self.lstm_cell_ = nn.LSTMCell(input_size=target_size+visual_feature_size, hidden_size=hidden_size)\n",
    "        self.linear_ = nn.Linear(in_features=hidden_size+visual_feature_size+target_size, out_features=target_size)\n",
    "    \n",
    "    def forward(self, visual_features, prev_target, prev_states):\n",
    "        r\"\"\"\n",
    "            Output:\n",
    "                (output, target_ground_truth) for autoregressive_mode = True\n",
    "                (output, (output, ))\n",
    "        \"\"\"\n",
    "        lstm_input = torch.cat((visual_features, prev_target), dim=-1)\n",
    "        h_t, c_t = self.lstm_cell_(lstm_input, prev_states)\n",
    "        linear_input = torch.cat((visual_features, prev_target, h_t), dim=-1)\n",
    "        output = self.linear_(linear_input)\n",
    "        new_state = (h_t, c_t)\n",
    "        \n",
    "        return output, new_state\n",
    "        \n",
    "class AutoregressiveLSTM(AutoregressiveLSTMCell):\n",
    "    \n",
    "    def __init__(self, target_size, visual_feature_size, hidden_size, autoregressive_mode=True):\n",
    "        super().__init__(target_size, visual_feature_size, hidden_size)\n",
    "        self.autoregressive_mode_ = autoregressive_mode\n",
    "    \n",
    "    def forward(self, visual_features, init_target=None, init_states=None, target_groundtruth=None):\n",
    "        # different from LSTM in torch library, we use the second dimension for sequence!\n",
    "        assert self.autoregressive_mode_ or target_groundtruth is not None\n",
    "        \n",
    "        seq_len = visual_features.shape[1]\n",
    "        batch_len = visual_features.shape[0]\n",
    "\n",
    "        prev_target = torch.zeros(batch_len, self.target_size_) if init_target is None else init_target\n",
    "        prev_states = (torch.zeros(batch_len, self.hidden_size_), torch.zeros(batch_len, self.hidden_size_)) if init_states is None else init_states\n",
    "        \n",
    "        outputs = []\n",
    "        states = []\n",
    "        for seq_idx in range(seq_len):\n",
    "            target, state = super().forward(visual_features[:, seq_idx, :], prev_target, prev_states)\n",
    "            prev_target = target if self.autoregressive_mode_ else target_groundtruth[:, seq_idx, :]\n",
    "            outputs.append(target)\n",
    "            states.append(torch.stack(state))\n",
    "            \n",
    "        outputs = torch.stack(outputs)\n",
    "        outputs = outputs.permute(1, 0, 2)  # dim: [batch, seq, target_size]\n",
    "        states = torch.stack(states)\n",
    "        states = states.permute(1, 2, 0, 3) # dim: [ [batch, seq, internal_size], [batch, seq, internal_size] ]\n",
    "        \n",
    "        return outputs, states\n",
    "    \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SteerNet(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.model_cnn_ = TemporalCNN(in_height=480, in_width=640, seq_len=15, dropout_prob=1.0, aux_history=10)\n",
    "        self.model_lstm_gt_ = AutoregressiveLSTM(target_size=3, visual_feature_size=128, hidden_size=64, autoregressive_mode=False)\n",
    "        self.model_lstm_autoreg_ = AutoregressiveLSTM(target_size=3, visual_feature_size=128, hidden_size=64, autoregressive_mode=True)\n",
    "        self.lstm_state_ = None\n",
    "        \n",
    "        self.train = True\n",
    "    \n",
    "    def forward(self, images, target):\n",
    "        \n",
    "        \n",
    "#         optimizer.zero_grad()\n",
    "        \n",
    "        features = self.model_cnn_(images)\n",
    "#         print(\"features dim: {}\".format(features.shape))\n",
    "        \n",
    "        if self.lstm_state_ is None:\n",
    "            if self.train:\n",
    "                out_gt, state_gt = self.model_lstm_gt_(features, target_groundtruth=target)\n",
    "            out_autoreg, state_autoreg = self.model_lstm_autoreg_(features)\n",
    "        else:\n",
    "            state_autoreg = self.lstm_state_\n",
    "            state_gt = (state_autoreg[0].clone().detach(),\n",
    "                        state_autoreg[1].clone().detach()) # copies the state. we don't opzimize the state of this LSTM with groundtruth input!\n",
    "            if self.train:\n",
    "                out_gt, _ = self.model_lstm_gt_(features, init_states=state_gt, target_groundtruth=target)\n",
    "            out_autoreg, state_autoreg = self.model_lstm_autoreg_(features, init_states=state_autoreg)\n",
    "        self.lstm_state_ = (state_autoreg[0][-1].detach(), state_autoreg[1][-1].detach())\n",
    "        \n",
    "        return (out_autoreg, out_gt) if self.train else out_autoreg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initializing DataLoader\n",
    "# Warning: this only need to be done once to reduce system overhead (leaking memory)\n",
    "\n",
    "from torch.utils.data import BatchSampler, SequentialSampler, RandomSampler\n",
    "\n",
    "udacity_dataset = UdacityDataset(csv_file='/export/jupyterlab/data/udacity-challenge-2/Ch2_002_export/interpolated.csv',\n",
    "                                 root_dir='/export/jupyterlab/data/udacity-challenge-2/Ch2_002_export/',\n",
    "                                 transform=transforms.Compose([transforms.ToTensor()]),\n",
    "                                 select_camera='center_camera')\n",
    "\n",
    "cbs = ConsecutiveBatchSampler(data_source=udacity_dataset, batch_size=20, shuffle=True, drop_last=False, seq_len=15)\n",
    "dataloader = DataLoader(udacity_dataset, sampler=cbs, collate_fn=(lambda x: x[0]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "size of training set: 27046\n",
      "size of validation set: 6762\n"
     ]
    }
   ],
   "source": [
    "# train - validation split\n",
    "\n",
    "dataset_size = len(udacity_dataset)\n",
    "split_point = int(dataset_size * 0.8)\n",
    "training_set = UdacityDataset(csv_file='/export/jupyterlab/data/udacity-challenge-2/Ch2_002_export/interpolated.csv',\n",
    "                              root_dir='/export/jupyterlab/data/udacity-challenge-2/Ch2_002_export/',\n",
    "                              transform=transforms.Compose([transforms.ToTensor()]),\n",
    "                              select_camera='center_camera',\n",
    "                              select_range=(0, split_point))\n",
    "validation_set = UdacityDataset(csv_file='/export/jupyterlab/data/udacity-challenge-2/Ch2_002_export/interpolated.csv',\n",
    "                                root_dir='/export/jupyterlab/data/udacity-challenge-2/Ch2_002_export/',\n",
    "                                transform=transforms.Compose([transforms.ToTensor()]),\n",
    "                                select_camera='center_camera',\n",
    "                                select_range=(split_point, dataset_size))\n",
    "\n",
    "print(\"size of training set: {}\".format(len(training_set)))\n",
    "print(\"size of validation set: {}\".format(len(validation_set)))\n",
    "\n",
    "cbs_train = ConsecutiveBatchSampler(data_source=training_set, batch_size=10, shuffle=True, drop_last=False, seq_len=15)\n",
    "cbs_valid = ConsecutiveBatchSampler(data_source=validation_set, batch_size=10, shuffle=True, drop_last=False, seq_len=15)\n",
    "\n",
    "trainig_loader = DataLoader(training_set, sampler=cbs_train, collate_fn=(lambda x: x[0]))\n",
    "validation_loader = DataLoader(validation_set, sampler=cbs_valid, collate_fn=(lambda x: x[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "steernet = SteerNet()\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(steernet.parameters())\n",
    "\n",
    "def do_epoch():\n",
    "    \n",
    "    # training\n",
    "    \n",
    "    running_loss = 0.0\n",
    "    for i, sample in enumerate(trainig_loader):\n",
    "        images = sample['image']\n",
    "        angle = sample['angle'][:, -10:]\n",
    "        torque = sample['torque'][:, -10:]\n",
    "        speed = sample['speed'][:, -10:]\n",
    "        target = torch.stack([angle, torque, speed])\n",
    "        target = target.permute([2, 1, 0]) # (batch_size x seq_len x target_dim)\n",
    "        del sample, torque, speed\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        out_autoreg, out_gt = steernet(images, target)\n",
    "        out_autoreg_angle = out_autoreg[:, :, 0]\n",
    "        \n",
    "        loss_angle = criterion(angle, out_autoreg_angle)\n",
    "        loss_autoreg = criterion(out_autoreg, target)\n",
    "        loss_gt = criterion(out_gt, target)\n",
    "        \n",
    "        loss_total = loss_angle + do_epoch.loss_target_weight * (loss_autoreg + loss_gt)\n",
    "        loss_total.backward()\n",
    "        \n",
    "        print(\"it: {} loss_total: {} loss_angle: {} loss_autoreg: {} loss_gt: {}\".format(i, loss_total.item(), loss_angle.item(), loss_autoreg.item(), loss_gt.item()))\n",
    "        optimizer.step()\n",
    "        running_loss += loss_total.item()\n",
    "#         if i % 1000 == 999:    # print every 1000 mini-batches\n",
    "#             print('[%d, %5d] loss: %.3f' %\n",
    "#                   (epoch + 1, i + 1, running_loss / 2000))\n",
    "#             running_loss = 0.0\n",
    "#     print('Finished Training')\n",
    "\n",
    "do_epoch.loss_target_weight = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "it: 0 loss_total: 113.51592254638672 loss_angle: 0.310525506734848 loss_autoreg: 117.16898345947266 loss_gt: 109.2418212890625\n",
      "it: 1 loss_total: 72.7643051147461 loss_angle: 0.02961003966629505 loss_autoreg: 75.20736694335938 loss_gt: 70.26202392578125\n",
      "it: 2 loss_total: 106.75252532958984 loss_angle: 0.23691199719905853 loss_autoreg: 110.36042022705078 loss_gt: 102.6707992553711\n",
      "it: 3 loss_total: 108.10198211669922 loss_angle: 0.02939455583691597 loss_autoreg: 112.58586120605469 loss_gt: 103.55931854248047\n",
      "it: 4 loss_total: 107.8693618774414 loss_angle: 0.07340401411056519 loss_autoreg: 112.42992401123047 loss_gt: 103.16199493408203\n",
      "it: 5 loss_total: 79.7800521850586 loss_angle: 0.3062925636768341 loss_autoreg: 82.93258666992188 loss_gt: 76.01493835449219\n",
      "it: 6 loss_total: 109.61125946044922 loss_angle: 0.016431711614131927 loss_autoreg: 114.26378631591797 loss_gt: 104.92586517333984\n",
      "it: 7 loss_total: 121.49832916259766 loss_angle: 0.04432440176606178 loss_autoreg: 127.06889343261719 loss_gt: 115.839111328125\n",
      "it: 8 loss_total: 85.69319915771484 loss_angle: 0.04698865860700607 loss_autoreg: 89.99939727783203 loss_gt: 81.29301452636719\n",
      "it: 9 loss_total: 110.70535278320312 loss_angle: 0.13637861609458923 loss_autoreg: 116.00499725341797 loss_gt: 105.1329574584961\n",
      "it: 10 loss_total: 108.34068298339844 loss_angle: 0.02524922601878643 loss_autoreg: 113.95354461669922 loss_gt: 102.67733001708984\n",
      "it: 11 loss_total: 92.52887725830078 loss_angle: 0.03498726338148117 loss_autoreg: 97.44043731689453 loss_gt: 87.5473403930664\n",
      "it: 12 loss_total: 101.20213317871094 loss_angle: 0.09412787109613419 loss_autoreg: 106.53231048583984 loss_gt: 95.6836929321289\n",
      "it: 13 loss_total: 66.10791778564453 loss_angle: 0.02808511070907116 loss_autoreg: 69.89954376220703 loss_gt: 62.26011657714844\n",
      "it: 14 loss_total: 122.57282257080078 loss_angle: 0.1549244076013565 loss_autoreg: 129.35223388671875 loss_gt: 115.48356628417969\n",
      "it: 15 loss_total: 116.39775085449219 loss_angle: 0.06201739236712456 loss_autoreg: 122.57698059082031 loss_gt: 110.094482421875\n",
      "it: 16 loss_total: 102.52194213867188 loss_angle: 0.18579423427581787 loss_autoreg: 108.82787322998047 loss_gt: 95.84442901611328\n",
      "it: 17 loss_total: 84.91896057128906 loss_angle: 0.02827312983572483 loss_autoreg: 90.16432189941406 loss_gt: 79.61705780029297\n",
      "it: 18 loss_total: 76.32174682617188 loss_angle: 0.00902620330452919 loss_autoreg: 81.08102416992188 loss_gt: 71.54441833496094\n",
      "it: 19 loss_total: 94.86289978027344 loss_angle: 0.13972538709640503 loss_autoreg: 100.5444564819336 loss_gt: 88.90188598632812\n",
      "it: 20 loss_total: 73.92414093017578 loss_angle: 0.14488635957241058 loss_autoreg: 78.31685638427734 loss_gt: 69.24163818359375\n",
      "it: 21 loss_total: 78.13414001464844 loss_angle: 0.17902904748916626 loss_autoreg: 83.12683868408203 loss_gt: 72.78337860107422\n",
      "it: 22 loss_total: 111.79307556152344 loss_angle: 0.07168910652399063 loss_autoreg: 119.59942626953125 loss_gt: 103.8433609008789\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-aee98890e67b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdo_epoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-9-a5b76d59b6d2>\u001b[0m in \u001b[0;36mdo_epoch\u001b[0;34m()\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m         \u001b[0mout_autoreg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout_gt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msteernet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m         \u001b[0mout_autoreg_angle\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mout_autoreg\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    530\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    531\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 532\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    533\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    534\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-6-5eb05f1a8b9d>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, images, target)\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;31m#         optimizer.zero_grad()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m         \u001b[0mfeatures\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_cnn_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m \u001b[0;31m#         print(\"features dim: {}\".format(features.shape))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    530\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    531\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 532\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    533\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    534\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-4-8885dcc1aff3>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    110\u001b[0m         \u001b[0maux_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    111\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mlayer\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv_layers_\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 112\u001b[0;31m             \u001b[0mx_out\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlayer\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    113\u001b[0m             \u001b[0mx_out_permuted\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx_out\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpermute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# swap back for calculation of aux output\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    114\u001b[0m             \u001b[0mx_aux\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlayer\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_out_permuted\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maux_history_\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    530\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    531\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 532\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    533\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    534\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     98\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 100\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    101\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    530\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    531\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 532\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    533\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    534\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    478\u001b[0m                             self.dilation, self.groups)\n\u001b[1;32m    479\u001b[0m         return F.conv3d(input, self.weight, self.bias, self.stride,\n\u001b[0;32m--> 480\u001b[0;31m                         self.padding, self.dilation, self.groups)\n\u001b[0m\u001b[1;32m    481\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    482\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "do_epoch()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
