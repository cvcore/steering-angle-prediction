{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function, division\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from skimage import io, transform\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "# defining customized Dataset class for Udacity\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms, utils\n",
    "\n",
    "class UdacityDataset(Dataset):\n",
    "    def __init__(self, csv_file, root_dir, transform=None, select_camera=None, slice_frames=None, select_ratio=1.0, select_range=None):\n",
    "        \n",
    "        assert select_ratio >= -1.0 and select_ratio <= 1.0 # positive: select to ratio from beginning, negative: select to ration counting from the end\n",
    "        \n",
    "        camera_csv = pd.read_csv(csv_file)\n",
    "        csv_len = len(camera_csv)\n",
    "        assert select_camera in ['left_camera', 'right_camera', 'center_camera'], \"Invalid camera: {}\".format(select_camera)\n",
    "        \n",
    "        if slice_frames:\n",
    "            csv_selected = camera_csv[0:0] # empty dataframe\n",
    "            for start_idx in range(0, csv_len, slice_frames):\n",
    "                if select_ratio > 0:\n",
    "                    end_idx = int(start_idx + slice_frames * select_ratio)\n",
    "                else:\n",
    "                    start_idx, end_idx = int(start_idx + slice_frames * (1 + select_ratio)), start_idx + slice_frames\n",
    "\n",
    "                if end_idx > csv_len:\n",
    "                    end_idx = csv_len\n",
    "                if start_idx > csv_len:\n",
    "                    start_idx = csv_len\n",
    "                csv_selected = csv_selected.append(camera_csv[start_idx:end_idx])\n",
    "            self.camera_csv = csv_selected\n",
    "        elif select_range:\n",
    "            csv_selected = camera_csv.iloc[select_range[0]: select_range[1]]\n",
    "            self.camera_csv = csv_selected\n",
    "        else:\n",
    "            self.camera_csv = camera_csv\n",
    "            \n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "        \n",
    "        # Keep track of mean and cov value in each channel\n",
    "        self.mean = {}\n",
    "        self.std = {}\n",
    "        for key in ['angle', 'torque', 'speed']:\n",
    "            self.mean[key] = np.mean(camera_csv[key])\n",
    "            self.std[key] = np.std(camera_csv[key])\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.camera_csv)\n",
    "    \n",
    "    def read_data_single(self, idx):\n",
    "        path = os.path.join(self.root_dir, self.camera_csv['filename'].iloc[idx])\n",
    "        image = io.imread(path)\n",
    "        timestamp = self.camera_csv['timestamp'].iloc[idx]\n",
    "        frame_id = self.camera_csv['frame_id'].iloc[idx]\n",
    "        angle = self.camera_csv['angle'].iloc[idx]\n",
    "        torque = self.camera_csv['torque'].iloc[idx]\n",
    "        speed = self.camera_csv['speed'].iloc[idx]\n",
    "        \n",
    "        if self.transform:\n",
    "            image_transformed = self.transform(image)\n",
    "            del image\n",
    "            image = image_transformed\n",
    "        angle_t = torch.tensor(angle)\n",
    "        torque_t = torch.tensor(torque)\n",
    "        speed_t = torch.tensor(speed)\n",
    "        del angle, torque, speed\n",
    "            \n",
    "        return image, timestamp, frame_id, angle_t, torque_t, speed_t\n",
    "    \n",
    "    def read_data(self, idx):\n",
    "        if isinstance(idx, list):\n",
    "            data = None\n",
    "            for i in idx:\n",
    "                new_data = self.read_data(i)\n",
    "                if data is None:\n",
    "                    data = [[] for _ in range(len(new_data))]\n",
    "                for i, d in enumerate(new_data):\n",
    "                    data[i].append(new_data[i])\n",
    "                del new_data\n",
    "                \n",
    "            for stack_idx in [0, 3, 4, 5]: # we don't stack timestamp and frame_id since those are string data\n",
    "                data[stack_idx] = torch.stack(data[stack_idx])\n",
    "            \n",
    "            return data\n",
    "        \n",
    "        else:\n",
    "            return self.read_data_single(idx)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "        \n",
    "        data = self.read_data(idx)\n",
    "        \n",
    "        sample = {'image': data[0],\n",
    "                  'timestamp': data[1],\n",
    "                  'frame_id': data[2],\n",
    "                  'angle': data[3],\n",
    "                  'torque': data[4],\n",
    "                  'speed': data[5]}\n",
    "        \n",
    "        del data\n",
    "        \n",
    "        return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate Batch with consecutive frames taken from input data\n",
    "\n",
    "from torch.utils.data import Sampler\n",
    "import random\n",
    "\n",
    "class ConsecutiveBatchSampler(Sampler):\n",
    "    \n",
    "    def __init__(self, data_source, batch_size, seq_len, drop_last=False, shuffle=True, use_all_frames=False):\n",
    "        r\"\"\" Sampler to generate consecutive Batches\n",
    "        \n",
    "        Args:\n",
    "            data_source: Source of data\n",
    "            batch_size: Size of batch\n",
    "            seq_len: Number of frames in each sequence (used for context for prediction)\n",
    "            drop: Wether to drop the last incomplete batch\n",
    "            shuffle: Wether to shuffle the data\n",
    "        Return:\n",
    "            List of iterators, size: [batch_size x seq_len x n_channels x height x width]\n",
    "        \"\"\"\n",
    "        super(ConsecutiveBatchSampler, self).__init__(data_source)\n",
    "        \n",
    "        self.data_source = data_source\n",
    "        \n",
    "        assert seq_len >= 1, \"Invalid batch size: {}\".format(seq_len)\n",
    "        self.seq_len = seq_len\n",
    "        self.drop_last = drop_last\n",
    "        self.shuffle = shuffle\n",
    "        self.batch_size = batch_size\n",
    "        self.use_all_frames_ = use_all_frames\n",
    "    \n",
    "    def __iter__(self):\n",
    "        \n",
    "        data_size = len(self.data_source)\n",
    "        \n",
    "        if self.use_all_frames_:\n",
    "            start_indices = list(range(data_size))\n",
    "        else:\n",
    "            start_indices = list(range(1, data_size, self.seq_len))\n",
    "            \n",
    "        if self.shuffle:\n",
    "            random.shuffle(start_indices)\n",
    "        \n",
    "        batch = []\n",
    "        for idx, ind in enumerate(start_indices):\n",
    "            if data_size - idx < self.batch_size and self.drop_last: # if last batch\n",
    "                break\n",
    "                \n",
    "            seq = []\n",
    "            if ind + 1 < self.seq_len:\n",
    "                seq.extend([0]*(self.seq_len - ind - 1) + list(range(0, ind+1)))\n",
    "            else:\n",
    "                seq.extend(list(range(ind-self.seq_len+1, ind+1)))\n",
    "            \n",
    "            batch.append(seq)\n",
    "            \n",
    "            if len(batch) == self.batch_size or idx == data_size - 1:\n",
    "                yield batch\n",
    "                batch = []\n",
    "\n",
    "    \n",
    "    def __len__(self):\n",
    "        length = len(self.data_source)\n",
    "        batch_size = self.batch_size\n",
    "        \n",
    "        if length % batch_size == 0 or self.drop_last:\n",
    "            return length // batch_size\n",
    "        \n",
    "        return length // batch_size + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_sample(sample):\n",
    "    r\"\"\" Helper function for (batch) sample visualization\n",
    "    \n",
    "    Args:\n",
    "        sample: Dictionary\n",
    "    \"\"\"\n",
    "    image_dims = len(sample['image'].shape)\n",
    "    assert image_dims <= 5, \"Unsupported image shape: {}\".format(sample['image'].shape)\n",
    "    if image_dims == 3:\n",
    "        plt.imshow(sample['image'])\n",
    "    else:\n",
    "        n0 = sample['image'].shape[0]\n",
    "        n1 = sample['image'].shape[1] if image_dims == 5 else 1\n",
    "        images_flattened = torch.flatten(sample['image'], end_dim=-4)\n",
    "        fig, ax = plt.subplots(n0, n1, figsize=(25, 15))\n",
    "        for i1 in range(n1):\n",
    "            for i0 in range(n0):\n",
    "                image = images_flattened[i0 * n1 + i1]\n",
    "                axis = ax[i0, i1]\n",
    "                axis.imshow(image.permute(1,2,0))\n",
    "                axis.axis('off')\n",
    "                axis.set_title(\"t={}\".format(sample['timestamp'][i0][i1]))\n",
    "                axis.text(10, 30, sample['frame_id'][i0][i1], color='red')\n",
    "            \n",
    "    \n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "for i_batch, sample_batched in enumerate(dataloader):\n",
    "    \n",
    "    # Your code here...\n",
    "    \n",
    "    del sample_batched # release image to save memory space\n",
    "    \n",
    "    if i_batch == 2: # test loading 10 datapoints\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3D CNN with residual connection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the model\n",
    "\n",
    "import torch.nn as nn\n",
    "\n",
    "# helper function to determine dimension after convolution\n",
    "def conv_output_shape(in_dimension, kernel_size, stride):\n",
    "    output_dim = []\n",
    "    for (in_dim, kern_size, strd) in zip(in_dimension, kernel_size, stride):\n",
    "        len = int(float(in_dim - kern_size) / strd + 1.)\n",
    "        output_dim.append(len)\n",
    "    \n",
    "    return output_dim\n",
    "\n",
    "        \n",
    "class TemporalCNN(nn.Module):\n",
    "    \n",
    "    def _conv_unit(self, in_channels, out_channels, in_shape, kernel_size, stride, dropout_prob):\n",
    "        r\"\"\" Return one 3D convolution unit\n",
    "        \n",
    "        Args:\n",
    "            in_channels: Input channels of the Conv3D module\n",
    "            out_channels: Output channels of the Conv3D module\n",
    "            in_shape: Shape of the input image. i.e. The last 3 dimensions of the input tensor: D x H x W\n",
    "            kernel_size: Kernel size\n",
    "            stride: Stride\n",
    "            dropout_prob: Probability of dropout layer\n",
    "                         \n",
    "        Output:\n",
    "            (conv_module, aux_module, out_shape)\n",
    "        \"\"\"\n",
    "        \n",
    "        conv = nn.Conv3d(in_channels=in_channels, out_channels=out_channels, kernel_size=kernel_size, stride=stride)\n",
    "        dropout = nn.Dropout3d(p=dropout_prob)\n",
    "        conv_module = nn.Sequential(conv, dropout).to(self.device_)\n",
    "        out_shape = conv_output_shape(in_shape, kernel_size, stride)\n",
    "        \n",
    "        flatten = nn.Flatten(start_dim=2)\n",
    "        aux = nn.Linear(in_features=np.prod(out_shape[-2:])*out_channels, out_features=128)\n",
    "        aux_module = nn.Sequential(flatten, aux).to(self.device_)\n",
    "        \n",
    "        return conv_module, aux_module, out_shape\n",
    "    \n",
    "    def _linear_unit(self, in_features, out_features, dropout_prob):\n",
    "        linear = nn.Linear(in_features, out_features)\n",
    "        dropout = nn.Dropout(p=dropout_prob)\n",
    "        return nn.Sequential(linear, dropout).to(self.device_), out_features\n",
    "        \n",
    "    \n",
    "    def __init__(self, in_height, in_width, seq_len, dropout_prob=0.5, aux_history=10, device=None):\n",
    "        r\"\"\" TemporalCNN: this model does 3D convolution on the H, W and temporal dimension\n",
    "             It also includes residual connection from each Conv3D output to the final output\n",
    "             \n",
    "             Args:\n",
    "                 in_height: image height\n",
    "                 in_width: image width\n",
    "                 seq_len: image sequence length\n",
    "                 dropout_prob: prob for the dropout layer\n",
    "                 aux_history: length of history to extract from seq_len\n",
    "             \n",
    "             Output:\n",
    "                 nn.Module, accepts input with shape [batch_len, seq_len, C, in_height, in_width]\n",
    "        \"\"\"\n",
    "        super(TemporalCNN, self).__init__()\n",
    "        \n",
    "        self.seq_len_ = seq_len\n",
    "        self.in_width_ = in_width\n",
    "        self.in_height_ = in_height\n",
    "        self.aux_history_ = aux_history\n",
    "        in_shape = (seq_len, in_height, in_width)\n",
    "        \n",
    "        self.device_ = device if device is not None else torch.device(\"cpu\")\n",
    "        \n",
    "        conv_layers = []\n",
    "        # conv1\n",
    "        conv, aux, out_shape = self._conv_unit(3, 64, in_shape, (3, 12, 12), (1, 6, 6), dropout_prob)\n",
    "        conv_layers.append((conv, aux))\n",
    "        # conv2\n",
    "        conv, aux, out_shape = self._conv_unit(64, 64, out_shape, (2, 5, 5), (1, 2, 2), dropout_prob)\n",
    "        conv_layers.append((conv, aux))\n",
    "        # conv3\n",
    "        conv, aux, out_shape = self._conv_unit(64, 64, out_shape, (2, 5, 5), (1, 1, 1), dropout_prob)\n",
    "        conv_layers.append((conv, aux))\n",
    "        # conv4\n",
    "        conv, aux, out_shape = self._conv_unit(64, 64, out_shape, (2, 5, 5), (1, 1, 1), dropout_prob)\n",
    "        conv_layers.append((conv, aux))\n",
    "        \n",
    "        linear_layers = []\n",
    "        # Flatten the last 3 dims\n",
    "        flatten = nn.Flatten(start_dim=2).to(self.device_)\n",
    "        linear_layers.append(flatten)\n",
    "        # FC 1024\n",
    "        linear, out_features = self._linear_unit(64*np.prod(out_shape[-2:]), 1024, dropout_prob)\n",
    "        linear_layers.append(linear)\n",
    "        # FC 512\n",
    "        linear, out_features = self._linear_unit(out_features, 512, dropout_prob)\n",
    "        linear_layers.append(linear)\n",
    "        # FC 256\n",
    "        linear, out_features = self._linear_unit(out_features, 256, dropout_prob)\n",
    "        linear_layers.append(linear)\n",
    "        # FC 128\n",
    "        linear, out_features = self._linear_unit(out_features, 128, dropout_prob)\n",
    "        linear_layers.append(linear)\n",
    "        \n",
    "        self.conv_layers_ = conv_layers\n",
    "        self.linear_layers_ = linear_layers\n",
    "        self.final_elu_ = nn.ELU().to(self.device_)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = x.permute([0, 2, 1, 3, 4]) # swap channel and seq_len, 3D conv over seq_len as depth channel\n",
    "                                       # now: [batch_size, channel, seq_len, H, W]\n",
    "        \n",
    "        aux_outputs = []\n",
    "        for layer in self.conv_layers_:\n",
    "            x_out = layer[0](x)\n",
    "            x_out_permuted = x_out.permute([0, 2, 1, 3, 4]) # swap back for calculation of aux output\n",
    "            x_aux = layer[1](x_out_permuted[:,-self.aux_history_:,:,:,:])\n",
    "#             print(x_out.shape)\n",
    "#             print(x_aux.shape)\n",
    "            aux_outputs.append(x_aux)\n",
    "            x = x_out\n",
    "        \n",
    "        x = x.permute([0, 2, 1, 3, 4]) # swap back the dimensions, now: [batch_size, seq_len, channel, H, W]\n",
    "        \n",
    "        for layer in [self.flatten_, self.linear0_, self.linear1_, self.linear2_, self.linear3_]:\n",
    "            x = layer(x)\n",
    "        \n",
    "        final_out = x\n",
    "        for aux_out in aux_outputs:\n",
    "            final_out = final_out + aux_out\n",
    "        final_out = self.elu_(final_out)\n",
    "#         print(final_out.shape)\n",
    "        \n",
    "        return final_out\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Autoregressive LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Autoregressive LSTM Module\n",
    "\n",
    "class AutoregressiveLSTMCell(nn.Module):\n",
    "    \n",
    "    def __init__(self, target_size, visual_feature_size, hidden_size, device=None):\n",
    "        r\"\"\" AutoregressiveModule takes visual feature from 3D CNN module, pass it\n",
    "             first into an internal LSTM cell and then into a Linear network. The final\n",
    "             output of this module is of dimension output_size.\n",
    "             \n",
    "             Args:\n",
    "                 target_dim: dimsion of target value. for this application 3 (angle, speed, torque)\n",
    "                 visual_feature_dim:\n",
    "                 output_size: output size after the Linear network\n",
    "                 autoregressive_mode: wether this module work as autoregressive mode or\n",
    "                     just pass the ground truth to output\n",
    "             Output:\n",
    "                 nn.Module\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        \n",
    "        self.target_size_ = target_size\n",
    "        self.visual_feature_size_ = visual_feature_size\n",
    "        self.hidden_size_ = hidden_size\n",
    "        \n",
    "        self.device_ = device if device is not None else torch.device(\"cpu\")\n",
    "        \n",
    "        self.lstm_cell_ = nn.LSTMCell(input_size=target_size+visual_feature_size, hidden_size=hidden_size).to(self.device_)\n",
    "        self.linear_ = nn.Linear(in_features=hidden_size+visual_feature_size+target_size, out_features=target_size).to(self.device_)\n",
    "    \n",
    "    def forward(self, visual_features, prev_target, prev_states):\n",
    "        r\"\"\"\n",
    "            Output:\n",
    "                (output, target_ground_truth) for autoregressive_mode = True\n",
    "                (output, (output, ))\n",
    "        \"\"\"\n",
    "        lstm_input = torch.cat((visual_features, prev_target), dim=-1)\n",
    "        h_t, c_t = self.lstm_cell_(lstm_input, prev_states)\n",
    "        linear_input = torch.cat((visual_features, prev_target, h_t), dim=-1)\n",
    "        output = self.linear_(linear_input)\n",
    "        new_state = (h_t, c_t)\n",
    "        \n",
    "        return output, new_state\n",
    "        \n",
    "class AutoregressiveLSTM(AutoregressiveLSTMCell):\n",
    "    \n",
    "    def __init__(self, target_size, visual_feature_size, hidden_size, autoregressive_mode=True, device=None):\n",
    "        super().__init__(target_size, visual_feature_size, hidden_size, device)\n",
    "        self.autoregressive_mode_ = autoregressive_mode\n",
    "    \n",
    "    def forward(self, visual_features, init_target=None, init_states=None, target_groundtruth=None):\n",
    "        # different from LSTM in torch library, we use the second dimension for sequence!\n",
    "        assert self.autoregressive_mode_ or target_groundtruth is not None\n",
    "        \n",
    "        seq_len = visual_features.shape[1]\n",
    "        batch_len = visual_features.shape[0]\n",
    "\n",
    "        prev_target = torch.zeros(batch_len, self.target_size_, device=self.device_) if init_target is None else init_target\n",
    "        prev_states = (torch.zeros(batch_len, self.hidden_size_, device=self.device_), \n",
    "                       torch.zeros(batch_len, self.hidden_size_, device=self.device_)) if init_states is None else init_states\n",
    "        \n",
    "        outputs = []\n",
    "        states = []\n",
    "        for seq_idx in range(seq_len):\n",
    "            target, state = super().forward(visual_features[:, seq_idx, :], prev_target, prev_states)\n",
    "            prev_target = target if self.autoregressive_mode_ else target_groundtruth[:, seq_idx, :]\n",
    "            outputs.append(target)\n",
    "            states.append(torch.stack(state))\n",
    "            \n",
    "        outputs = torch.stack(outputs)\n",
    "        outputs = outputs.permute(1, 0, 2)  # dim: [batch, seq, target_size]\n",
    "        states = torch.stack(states)\n",
    "        states = states.permute(1, 2, 0, 3) # dim: [ [batch, seq, internal_size], [batch, seq, internal_size] ]\n",
    "        \n",
    "        return outputs, states\n",
    "    \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SteerNet(nn.Module):\n",
    "    \n",
    "    def __init__(self, device=None, dropout_prob=1.0):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.device_ = device if device is not None else torch.device(\"cpu\")\n",
    "        \n",
    "        self.model_cnn_ = TemporalCNN(in_height=480, in_width=640, seq_len=15, dropout_prob=dropout_prob, aux_history=10, device=device)\n",
    "        self.feature_dropout_ = nn.Dropout(p=dropout_prob)\n",
    "        self.model_lstm_gt_ = AutoregressiveLSTM(target_size=3, visual_feature_size=128, hidden_size=64, autoregressive_mode=False, device=device)\n",
    "        self.model_lstm_autoreg_ = AutoregressiveLSTM(target_size=3, visual_feature_size=128, hidden_size=64, autoregressive_mode=True, device=device)\n",
    "        self.lstm_state_ = None\n",
    "        \n",
    "        self.train_ = True\n",
    "    \n",
    "    def forward(self, images, target=None):\n",
    "        assert target is None or self.train_==True\n",
    "        \n",
    "        features = self.model_cnn_(images)\n",
    "        features = self.feature_dropout_(features)\n",
    "        \n",
    "        if self.lstm_state_ is None:\n",
    "            if self.train_:\n",
    "                out_gt, state_gt = self.model_lstm_gt_(features, target_groundtruth=target)\n",
    "            out_autoreg, state_autoreg = self.model_lstm_autoreg_(features)\n",
    "        else:\n",
    "            state_autoreg = self.lstm_state_\n",
    "            state_gt = (state_autoreg[0].clone().detach(),\n",
    "                        state_autoreg[1].clone().detach()) # copies the state. we don't opzimize the state of this LSTM with groundtruth input!\n",
    "            if self.train_:\n",
    "                out_gt, _ = self.model_lstm_gt_(features, init_states=state_gt, target_groundtruth=target)\n",
    "            out_autoreg, state_autoreg = self.model_lstm_autoreg_(features, init_states=state_autoreg)\n",
    "        if self.train_:\n",
    "            self.lstm_state_ = (state_autoreg[0][-1].detach(), state_autoreg[1][-1].detach())\n",
    "        \n",
    "        return (out_autoreg, out_gt) if self.train_ else out_autoreg\n",
    "    \n",
    "    def train(self, mode=True):\n",
    "        self.train_ = mode"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "size of training set: 81120\n",
      "stat:\n",
      "mean: {'angle': -0.008475752983277953, 'torque': -0.09077343871020742, 'speed': 15.622631789553342}\n",
      "std: {'angle': 0.27155946576081297, 'torque': 0.7871942194690468, 'speed': 5.695705986851927}\n",
      "size of validation set: 20276\n",
      "stat:\n",
      "mean: {'angle': -0.008475752983277953, 'torque': -0.09077343871020742, 'speed': 15.622631789553342}\n",
      "std: {'angle': 0.27155946576081297, 'torque': 0.7871942194690468, 'speed': 5.695705986851927}\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import BatchSampler, SequentialSampler, RandomSampler\n",
    "# train - validation split\n",
    "\n",
    "# udacity_dataset = UdacityDataset(csv_file='/export/jupyterlab/data/udacity-challenge-2/Ch2_002_export/interpolated.csv',\n",
    "#                               root_dir='/export/jupyterlab/data/udacity-challenge-2/Ch2_002_export/',\n",
    "#                               transform=transforms.Compose([transforms.ToTensor()]),\n",
    "#                               select_camera='center_camera')\n",
    "\n",
    "# dataset_size = int(len(udacity_dataset) * 0.10)\n",
    "# split_point = int(dataset_size * 0.8)\n",
    "# training_set = UdacityDataset(csv_file='/export/jupyterlab/data/udacity-challenge-2/Ch2_002_export/interpolated.csv',\n",
    "#                               root_dir='/export/jupyterlab/data/udacity-challenge-2/Ch2_002_export/',\n",
    "#                               transform=transforms.Compose([transforms.ToTensor()]),\n",
    "#                               select_camera='center_camera',\n",
    "#                               select_range=(0, split_point))\n",
    "# validation_set = UdacityDataset(csv_file='/export/jupyterlab/data/udacity-challenge-2/Ch2_002_export/interpolated.csv',\n",
    "#                                 root_dir='/export/jupyterlab/data/udacity-challenge-2/Ch2_002_export/',\n",
    "#                                 transform=transforms.Compose([transforms.ToTensor()]),\n",
    "#                                 select_camera='center_camera',\n",
    "#                                 select_range=(split_point, dataset_size))\n",
    "\n",
    "# Train / valid split with slices\n",
    "training_set = UdacityDataset(csv_file='/export/jupyterlab/data/udacity-challenge-2/Ch2_002_export/interpolated.csv',\n",
    "                              root_dir='/export/jupyterlab/data/udacity-challenge-2/Ch2_002_export/',\n",
    "                              transform=transforms.Compose([transforms.ToTensor()]),\n",
    "                              select_camera='center_camera',\n",
    "                              slice_frames=100,\n",
    "                              select_ratio=0.8)\n",
    "validation_set = UdacityDataset(csv_file='/export/jupyterlab/data/udacity-challenge-2/Ch2_002_export/interpolated.csv',\n",
    "                                root_dir='/export/jupyterlab/data/udacity-challenge-2/Ch2_002_export/',\n",
    "                                transform=transforms.Compose([transforms.ToTensor()]),\n",
    "                                select_camera='center_camera',\n",
    "                                slice_frames=100,\n",
    "                                select_ratio=-0.2)\n",
    "\n",
    "print(\"size of training set: {}\\nstat:\\nmean: {}\\nstd: {}\".format(len(training_set), training_set.mean, training_set.std))\n",
    "print(\"size of validation set: {}\\nstat:\\nmean: {}\\nstd: {}\".format(len(validation_set), validation_set.mean, validation_set.std))\n",
    "\n",
    "cbs_train = ConsecutiveBatchSampler(data_source=training_set, batch_size=10, shuffle=True, drop_last=True, seq_len=15)\n",
    "cbs_valid = ConsecutiveBatchSampler(data_source=validation_set, batch_size=10, shuffle=True, drop_last=True, seq_len=15)\n",
    "\n",
    "trainig_loader = DataLoader(training_set, sampler=cbs_train, collate_fn=(lambda x: x[0]))\n",
    "validation_loader = DataLoader(validation_set, sampler=cbs_valid, collate_fn=(lambda x: x[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda for training\n"
     ]
    }
   ],
   "source": [
    "import datetime\n",
    "import subprocess\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "writer = SummaryWriter('/export/jupyterlab/chengxin/runs/steernet-{}'.format(datetime.datetime.now()))\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using {} for training\".format(device))\n",
    "\n",
    "steernet = SteerNet(device=device, dropout_prob=0.25)\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(steernet.parameters())\n",
    "\n",
    "def normalized_data(sample, channel, dataset_for_stat, seq_len=10):\n",
    "    result = (sample[channel][:, -seq_len:] - dataset_for_stat.mean[channel]) / dataset_for_stat.std[channel]\n",
    "    return result\n",
    "\n",
    "def do_epoch(epoch_num=0):\n",
    "    \n",
    "    # training\n",
    "    steernet.train()\n",
    "    \n",
    "    for i, sample in enumerate(trainig_loader):\n",
    "        \n",
    "        angle = sample['angle'][:, -10:]\n",
    "        torque = sample['torque'][:, -10:]\n",
    "        speed = sample['speed'][:, -10:]\n",
    "        images = sample['image'].to(device)\n",
    "#         angle = normalized_data(sample, 'angle', training_set, seq_len=10)\n",
    "#         torque = normalized_data(sample, 'torque', training_set, seq_len=10)\n",
    "#         speed = normalized_data(sample, 'speed', training_set, seq_len=10)\n",
    "        \n",
    "        target = torch.stack([angle, torque, speed])\n",
    "        target = target.permute([2, 1, 0]).to(device) # (batch_size x seq_len x target_dim)\n",
    "        angle = angle.to(device)\n",
    "        del sample, torque, speed\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        out_autoreg, out_gt = steernet(images, target)\n",
    "        out_autoreg_angle = out_autoreg[:, :, 0]\n",
    "        \n",
    "        loss_angle = criterion(angle, out_autoreg_angle)\n",
    "        loss_autoreg = criterion(out_autoreg, target)\n",
    "        loss_gt = criterion(out_gt, target)\n",
    "        \n",
    "        loss_total = loss_angle + do_epoch.loss_target_weight * (loss_autoreg + loss_gt)\n",
    "        loss_total.backward()\n",
    "        \n",
    "        writer.add_scalar('train - loss total epoch {}'.format(epoch_num), loss_total.item(), i)\n",
    "        writer.add_scalar('train - loss angle epoch {}'.format(epoch_num), loss_angle.item(), i)\n",
    "        writer.add_scalar('train - loss autoreg epoch {}'.format(epoch_num), loss_autoreg.item(), i)\n",
    "        writer.add_scalar('train - loss gt epoch {}'.format(epoch_num), loss_gt.item(), i)\n",
    "        \n",
    "#         torch.nn.utils.clip_grad_norm_(steernet.parameters(), 15.0) # gradient clamping\n",
    "        optimizer.step()\n",
    "        \n",
    "    print(\"Finished training epoch {}\".format(epoch_num))\n",
    "    \n",
    "    git_label = subprocess.check_output([\"git\", \"rev-parse\", \"--short=6\", \"HEAD\"]).strip().decode('UTF-8') # get current git label\n",
    "    torch.save(steernet.state_dict(), \"/export/jupyterlab/chengxin/models/git-{}-epoch-{}\".format(git_label, epoch_num))\n",
    "    \n",
    "    # validation\n",
    "    \n",
    "    with torch.no_grad(): # we don't require grad calcualation in the validation phase. it saves memory.\n",
    "        steernet.eval()\n",
    "\n",
    "        running_cost = 0.0\n",
    "        total_iterations = 0\n",
    "        for i, sample in enumerate(validation_loader):\n",
    "            images = sample['image'].to(device)\n",
    "            angle = sample['angle'][:, -10:]\n",
    "            torque = sample['torque'][:, -10:]\n",
    "            speed = sample['speed'][:, -10:]\n",
    "#             angle = normalized_data(sample, 'angle', training_set, seq_len=10)\n",
    "#             torque = normalized_data(sample, 'torque', training_set, seq_len=10)\n",
    "#             speed = normalized_data(sample, 'speed', training_set, seq_len=10)\n",
    "            \n",
    "            target = torch.stack([angle, torque, speed])\n",
    "            target = target.permute([2, 1, 0]).to(device) # (batch_size x seq_len x target_dim)\n",
    "            angle = angle.to(device)\n",
    "            del sample, torque, speed\n",
    "\n",
    "            out_autoreg = steernet(images)\n",
    "            out_autoreg_angle = out_autoreg[:, :, 0]\n",
    "\n",
    "            loss_angle = criterion(angle, out_autoreg_angle)\n",
    "            loss_autoreg = criterion(out_autoreg, target)\n",
    "            \n",
    "            writer.add_scalar('valid - loss angle epoch {}'.format(epoch_num), loss_angle.item(), i)\n",
    "            writer.add_scalar('valid - loss autoreg epoch {}'.format(epoch_num), loss_autoreg.item(), i)\n",
    "#             running_cost += loss_angle.item() * training_set.std['angle']**2\n",
    "            running_cost += loss_angle.item()\n",
    "            total_iterations = i+1\n",
    "        \n",
    "        running_cost = running_cost / total_iterations\n",
    "        writer.add_scalar('valid - angle (RMSE)', np.sqrt(running_cost), epoch_num)\n",
    "                  \n",
    "    print(\"Finished validating epoch {}\".format(epoch_num))\n",
    "    \n",
    "do_epoch.loss_target_weight = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'TemporalCNN' object has no attribute 'flatten_'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-f7bbe2e34bd9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m20\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mdo_epoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-8-3e3b82788ab9>\u001b[0m in \u001b[0;36mdo_epoch\u001b[0;34m(epoch_num)\u001b[0m\n\u001b[1;32m     38\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m         \u001b[0mout_autoreg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout_gt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msteernet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     41\u001b[0m         \u001b[0mout_autoreg_angle\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mout_autoreg\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    530\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    531\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 532\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    533\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    534\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-6-0323f005d270>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, images, target)\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0mtarget\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m         \u001b[0mfeatures\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_cnn_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m         \u001b[0mfeatures\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeature_dropout_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    530\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    531\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 532\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    533\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    534\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-4-3399c0bd2163>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    122\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpermute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# swap back the dimensions, now: [batch_size, seq_len, channel, H, W]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    123\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 124\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mlayer\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatten_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear0_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear1_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear2_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear3_\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    125\u001b[0m             \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlayer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    126\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m    574\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mmodules\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    575\u001b[0m         raise AttributeError(\"'{}' object has no attribute '{}'\".format(\n\u001b[0;32m--> 576\u001b[0;31m             type(self).__name__, name))\n\u001b[0m\u001b[1;32m    577\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    578\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__setattr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'TemporalCNN' object has no attribute 'flatten_'"
     ]
    }
   ],
   "source": [
    "for epoch in range(0, 20):\n",
    "    do_epoch(epoch)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
